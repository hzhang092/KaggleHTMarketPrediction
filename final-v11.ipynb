{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T19:36:35.709419Z",
          "iopub.status.busy": "2025-11-10T19:36:35.709059Z",
          "iopub.status.idle": "2025-11-10T19:36:51.542800Z",
          "shell.execute_reply": "2025-11-10T19:36:51.541954Z",
          "shell.execute_reply.started": "2025-11-10T19:36:35.709395Z"
        },
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import StackingRegressor, RandomForestRegressor, GradientBoostingRegressor,VotingRegressor\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from catboost import CatBoostRegressor, CatBoostClassifier\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.compose import make_column_transformer, ColumnTransformer\n",
        "from sklearn.model_selection import cross_val_score, cross_validate, KFold,TimeSeriesSplit,PredefinedSplit\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, ClassifierMixin, clone\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.neighbors import KNeighborsRegressor \n",
        "from sklearn.linear_model import ElasticNet, Lasso\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_selection import mutual_info_regression\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import FunctionTransformer, RobustScaler\n",
        "from sklearn.datasets import make_regression\n",
        "from scipy import stats\n",
        "from sklearn.metrics import r2_score \n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from scipy.optimize import curve_fit\n",
        "from scipy.signal import detrend\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from pprint import pprint\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from catboost import CatBoostRegressor, CatBoostClassifier\n",
        "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, ClassifierMixin, clone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T19:36:51.544309Z",
          "iopub.status.busy": "2025-11-10T19:36:51.543647Z",
          "iopub.status.idle": "2025-11-10T19:36:51.921352Z",
          "shell.execute_reply": "2025-11-10T19:36:51.920376Z",
          "shell.execute_reply.started": "2025-11-10T19:36:51.544282Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full train dataset shape is (9021, 98)\n"
          ]
        }
      ],
      "source": [
        "train_file_path = \"./hull-tactical-market-prediction/train.csv\"\n",
        "test_file_path = \"./hull-tactical-market-prediction/test.csv\"\n",
        "train_df = pd.read_csv(train_file_path)\n",
        "test_df = pd.read_csv(test_file_path)\n",
        "print(\"Full train dataset shape is {}\".format(train_df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 已定义严谨的三段数据分割函数: split_data_three_way\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 严谨的三段数据分割函数（用于避免过拟合）\n",
        "# ============================================================\n",
        "\n",
        "def split_data_three_way(\n",
        "    X, \n",
        "    y, \n",
        "    n_train_rows=None,\n",
        "    n_validation_rows=None,\n",
        "    n_test_rows=None,\n",
        "    train_ratio=0.6,\n",
        "    validation_ratio=0.2,\n",
        "    test_ratio=0.2,\n",
        "    verbose=1\n",
        "):\n",
        "    \"\"\"\n",
        "    严谨的三段数据分割函数，确保时间序列顺序和完整性\n",
        "    \n",
        "    参数:\n",
        "    X: 特征数据（DataFrame 或 array-like）\n",
        "    y: 目标数据（Series 或 array-like）\n",
        "    n_train_rows: 训练段样本数（如果指定，优先使用）\n",
        "    n_validation_rows: 验证段样本数（如果指定，优先使用）\n",
        "    n_test_rows: 测试段样本数（如果指定，优先使用）\n",
        "    train_ratio: 训练段比例（当未指定具体行数时使用）\n",
        "    validation_ratio: 验证段比例（当未指定具体行数时使用）\n",
        "    test_ratio: 测试段比例（当未指定具体行数时使用）\n",
        "    verbose: 详细输出级别\n",
        "    \n",
        "    返回:\n",
        "    (X_train, y_train, X_validation, y_validation, X_test, y_test, split_info)\n",
        "    \n",
        "    分割逻辑（时间顺序，从早到晚）:\n",
        "    1. 训练段（最早）: 用于训练方向模型\n",
        "    2. 验证段（中间）: 方向模型预测，Meta模型训练\n",
        "    3. 测试段（最新）: 最终评估\n",
        "    \"\"\"\n",
        "    # 转换为 DataFrame/Series 以便索引对齐\n",
        "    if not isinstance(X, pd.DataFrame):\n",
        "        X = pd.DataFrame(X)\n",
        "    if not isinstance(y, pd.Series):\n",
        "        y = pd.Series(y, index=X.index)\n",
        "    \n",
        "    total_rows = len(X)\n",
        "    \n",
        "    if verbose >= 1:\n",
        "        print(\"=\"*70)\n",
        "        print(\"三段数据分割（严谨模式 - 避免过拟合）\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"总样本数: {total_rows}\")\n",
        "    \n",
        "    # 验证输入\n",
        "    if len(X) != len(y):\n",
        "        raise ValueError(f\"X 和 y 长度不一致: X={len(X)}, y={len(y)}\")\n",
        "    \n",
        "    if not X.index.equals(y.index):\n",
        "        if verbose >= 1:\n",
        "            print(f\"警告: X 和 y 索引不完全一致，尝试对齐...\")\n",
        "        # 对齐索引\n",
        "        common_index = X.index.intersection(y.index)\n",
        "        X = X.loc[common_index]\n",
        "        y = y.loc[common_index]\n",
        "        total_rows = len(X)\n",
        "        if verbose >= 1:\n",
        "            print(f\"对齐后样本数: {total_rows}\")\n",
        "    \n",
        "    # 计算各段大小\n",
        "    if n_test_rows is not None:\n",
        "        n_test = n_test_rows\n",
        "        remaining = total_rows - n_test\n",
        "        \n",
        "        if n_validation_rows is not None:\n",
        "            n_validation = n_validation_rows\n",
        "            n_train = remaining - n_validation\n",
        "        else:\n",
        "            n_validation = int(remaining * validation_ratio / (train_ratio + validation_ratio))\n",
        "            n_train = remaining - n_validation\n",
        "    else:\n",
        "        if n_validation_rows is not None:\n",
        "            n_validation = n_validation_rows\n",
        "            remaining = total_rows - n_validation\n",
        "            \n",
        "            if n_train_rows is not None:\n",
        "                n_train = n_train_rows\n",
        "                n_test = remaining - n_train\n",
        "            else:\n",
        "                n_test = int(remaining * test_ratio / (train_ratio + test_ratio))\n",
        "                n_train = remaining - n_test\n",
        "        else:\n",
        "            if n_train_rows is not None:\n",
        "                n_train = n_train_rows\n",
        "                remaining = total_rows - n_train\n",
        "                n_validation = int(remaining * validation_ratio / (validation_ratio + test_ratio))\n",
        "                n_test = remaining - n_validation\n",
        "            else:\n",
        "                # 使用比例\n",
        "                n_train = int(total_rows * train_ratio)\n",
        "                n_validation = int(total_rows * validation_ratio)\n",
        "                n_test = total_rows - n_train - n_validation\n",
        "    \n",
        "    # 验证分割合理性\n",
        "    if n_train < 10:\n",
        "        raise ValueError(f\"训练段样本数过少: {n_train}，至少需要10个样本\")\n",
        "    if n_validation < 10:\n",
        "        raise ValueError(f\"验证段样本数过少: {n_validation}，至少需要10个样本\")\n",
        "    if n_test < 10:\n",
        "        raise ValueError(f\"测试段样本数过少: {n_test}，至少需要10个样本\")\n",
        "    \n",
        "    if n_train + n_validation + n_test != total_rows:\n",
        "        # 调整测试段大小以匹配总数\n",
        "        n_test = total_rows - n_train - n_validation\n",
        "        if verbose >= 1:\n",
        "            print(f\"调整测试段大小: {n_test}\")\n",
        "    \n",
        "    # 严谨的时间序列分割（确保顺序正确）\n",
        "    # 训练段：最早的数据 [0:n_train]\n",
        "    # 验证段：中间的数据 [n_train:n_train+n_validation]\n",
        "    # 测试段：最新的数据 [n_train+n_validation:]\n",
        "    \n",
        "    train_end = n_train\n",
        "    validation_end = n_train + n_validation\n",
        "    \n",
        "    X_train = X.iloc[:train_end].copy()\n",
        "    y_train = y.iloc[:train_end].copy()\n",
        "    \n",
        "    X_validation = X.iloc[train_end:validation_end].copy()\n",
        "    y_validation = y.iloc[train_end:validation_end].copy()\n",
        "    \n",
        "    X_test = X.iloc[validation_end:].copy()\n",
        "    y_test = y.iloc[validation_end:].copy()\n",
        "    \n",
        "    # 验证分割结果\n",
        "    assert len(X_train) == n_train, f\"训练段长度不匹配: {len(X_train)} != {n_train}\"\n",
        "    assert len(X_validation) == n_validation, f\"验证段长度不匹配: {len(X_validation)} != {n_validation}\"\n",
        "    assert len(X_test) == n_test, f\"测试段长度不匹配: {len(X_test)} != {n_test}\"\n",
        "    assert len(X_train) + len(X_validation) + len(X_test) == total_rows, \"总长度不匹配\"\n",
        "    \n",
        "    # 验证时间顺序（如果索引是时间类型）\n",
        "    if hasattr(X.index, 'is_monotonic_increasing'):\n",
        "        if not X.index.is_monotonic_increasing:\n",
        "            if verbose >= 1:\n",
        "                print(\"警告: 索引不是单调递增，请确保数据已按时间排序\")\n",
        "    \n",
        "    # 验证索引连续性\n",
        "    train_indices = set(X_train.index)\n",
        "    validation_indices = set(X_validation.index)\n",
        "    test_indices = set(X_test.index)\n",
        "    \n",
        "    if train_indices & validation_indices:\n",
        "        raise ValueError(\"训练段和验证段索引重叠！\")\n",
        "    if train_indices & test_indices:\n",
        "        raise ValueError(\"训练段和测试段索引重叠！\")\n",
        "    if validation_indices & test_indices:\n",
        "        raise ValueError(\"验证段和测试段索引重叠！\")\n",
        "    \n",
        "    # 收集分割信息\n",
        "    split_info = {\n",
        "        'total_rows': total_rows,\n",
        "        'n_train': n_train,\n",
        "        'n_validation': n_validation,\n",
        "        'n_test': n_test,\n",
        "        'train_ratio': n_train / total_rows,\n",
        "        'validation_ratio': n_validation / total_rows,\n",
        "        'test_ratio': n_test / total_rows,\n",
        "        'train_start_index': 0,\n",
        "        'train_end_index': train_end,\n",
        "        'validation_start_index': train_end,\n",
        "        'validation_end_index': validation_end,\n",
        "        'test_start_index': validation_end,\n",
        "        'test_end_index': total_rows,\n",
        "        'train_index_range': (X_train.index[0], X_train.index[-1]) if len(X_train) > 0 else None,\n",
        "        'validation_index_range': (X_validation.index[0], X_validation.index[-1]) if len(X_validation) > 0 else None,\n",
        "        'test_index_range': (X_test.index[0], X_test.index[-1]) if len(X_test) > 0 else None\n",
        "    }\n",
        "    \n",
        "    if verbose >= 1:\n",
        "        print(f\"\\n分割结果:\")\n",
        "        print(f\"  训练段: {n_train} 个样本 ({n_train/total_rows:.2%})\")\n",
        "        print(f\"    索引范围: {split_info['train_index_range']}\")\n",
        "        print(f\"  验证段: {n_validation} 个样本 ({n_validation/total_rows:.2%})\")\n",
        "        print(f\"    索引范围: {split_info['validation_index_range']}\")\n",
        "        print(f\"  测试段: {n_test} 个样本 ({n_test/total_rows:.2%})\")\n",
        "        print(f\"    索引范围: {split_info['test_index_range']}\")\n",
        "        print(f\"\\n✓ 数据分割完成，时间顺序验证通过\")\n",
        "        print(\"=\"*70)\n",
        "    \n",
        "    return X_train, y_train, X_validation, y_validation, X_test, y_test, split_info\n",
        "\n",
        "print(\"✓ 已定义严谨的三段数据分割函数: split_data_three_way\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 特征选择器参数配置已加载\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 特征选择器参数配置（可调节）\n",
        "# ============================================================\n",
        "# 在这里修改参数，所有使用 AutoFeatureSelectorWithLoss 的地方都会使用这些参数\n",
        "feature_selector_params = {\n",
        "    'n_clusters': None,              # 聚类数量，None 表示自动选择\n",
        "    'min_cluster_size': 3,           # 每个聚类的最小特征数\n",
        "    'max_clusters': 15,              # 最大聚类数量\n",
        "    'loss_threshold': None,          # 损失函数阈值，高于此值的组将被删除（None 表示自动）\n",
        "    'top_k_per_group': 0.65,        # 从每个组内保留最重要的特征比例（0-1之间）\n",
        "    'clustering_method': 'kmeans',   # 聚类方法：'kmeans' 或 'hierarchical'\n",
        "    'n_repeats': 3,                  # 置换测试的重复次数\n",
        "    'random_state': 42,              # 随机种子\n",
        "    'verbose_clustering': None       # 聚类日志详细程度，None 表示使用 verbose 的值\n",
        "}\n",
        "\n",
        "print(\"✓ 特征选择器参数配置已加载\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T19:36:51.932157Z",
          "iopub.status.busy": "2025-11-10T19:36:51.931899Z",
          "iopub.status.idle": "2025-11-10T19:36:51.982743Z",
          "shell.execute_reply": "2025-11-10T19:36:51.981875Z",
          "shell.execute_reply.started": "2025-11-10T19:36:51.932136Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preparing features (X)...\n",
            "Preparing target (y) for model training...\n",
            "\n",
            "原始数据准备完成:\n",
            "  X shape: (9021, 97)\n",
            "  y shape: (9021,)\n",
            "  X 索引范围: 0 - 9020\n",
            "  y 索引范围: 0 - 9020\n",
            "\n",
            "--- NA Counts ---\n",
            "X NAs: 87\n",
            "y NAs: 0\n",
            "\n",
            "======================================================================\n",
            "注意：数据分割将在主执行代码中使用 split_data_three_way() 完成\n",
            "这确保了 X_train, X_validation, X_test 的索引不会重叠\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "TARGET = \"market_forward_excess_returns\"\n",
        "FEATURES = [col for col in train_df.columns if col not in [TARGET]]\n",
        "\n",
        "\n",
        "print(\"Preparing features (X)...\")\n",
        "X = train_df[FEATURES].copy()\n",
        "X['forward_returns_lag'] = X['forward_returns'].shift(1)\n",
        "X['risk_free_rate_lag'] = X['risk_free_rate'].shift(1)\n",
        "X.drop('forward_returns', axis = 1, inplace = True)\n",
        "X.drop('risk_free_rate', axis =1, inplace = True)\n",
        "\n",
        "\n",
        "print(\"Preparing target (y) for model training...\")\n",
        "y = train_df[TARGET].copy()\n",
        "\n",
        "\n",
        "#———————————————————————————————————————————————————————————————————————————————————————\n",
        "\n",
        "# ============================================================\n",
        "# 重要：不再在这里进行数据分割！\n",
        "# 数据分割改为在主执行代码中使用 split_data_three_way() 完成\n",
        "# 这样可以确保 X_train, X_validation, X_test 的索引不重叠\n",
        "# ============================================================\n",
        "\n",
        "# 保留原始数据的副本（用于诊断和验证）\n",
        "X_original = X.copy()\n",
        "y_original = y.copy()\n",
        "\n",
        "print(f\"\\n原始数据准备完成:\")\n",
        "print(f\"  X shape: {X.shape}\")\n",
        "print(f\"  y shape: {y.shape}\")\n",
        "print(f\"  X 索引范围: {X.index[0]} - {X.index[-1]}\")\n",
        "print(f\"  y 索引范围: {y.index[0]} - {y.index[-1]}\")\n",
        "\n",
        "print(\"\\n--- NA Counts ---\")\n",
        "print(f\"X NAs: {X.isna().any().sum()}\")\n",
        "print(f\"y NAs: {y.isna().any().sum()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"注意：数据分割将在主执行代码中使用 split_data_three_way() 完成\")\n",
        "print(\"这确保了 X_train, X_validation, X_test 的索引不会重叠\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-11-10T19:36:51.984024Z",
          "iopub.status.busy": "2025-11-10T19:36:51.983682Z",
          "iopub.status.idle": "2025-11-10T19:36:51.988686Z",
          "shell.execute_reply": "2025-11-10T19:36:51.987730Z",
          "shell.execute_reply.started": "2025-11-10T19:36:51.984001Z"
        },
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 已定义 ad_sharpe_ratio_scorer 和 generate_hft_positions\n"
          ]
        }
      ],
      "source": [
        "# CatBoost 超参数配置\n",
        "# 针对 250-350 个特征优化（经过特征选择后）\n",
        "cat_parameters = {\n",
        "    'iterations': 800,              # 最大迭代次数（保持，early stopping 会控制）\n",
        "    'learning_rate': 0.004,          # 学习率（保持，小学习率更稳定）\n",
        "    'depth': 6,                      # 树深度（从 5 增加到 6，捕捉更多特征交互）\n",
        "    'min_data_in_leaf': 20,          # 叶节点最小样本数（从 19 增加到 20，增强正则化）\n",
        "    'l2_leaf_reg': 7.0,              # L2 正则化（从 5.4 增加到 7.0，防止过拟合）\n",
        "    'random_strength': 5.5,          # 随机强度（从 5.2 增加到 5.5，增加随机性）\n",
        "    'colsample_bylevel': 0.78,       # 每层特征采样比例（从 0.86 降低到 0.78，增加随机性）\n",
        "    'early_stopping_rounds': 50,     # 早停轮数（保持）\n",
        "    'bootstrap_type': 'Bernoulli',   # 新增：Bernoulli 采样（增强正则化）\n",
        "    'subsample': 0.85                # 新增：行采样比例（增强正则化）\n",
        "}\n",
        "\n",
        "# ============================================================\n",
        "# 辅助类和函数\n",
        "# ============================================================\n",
        "\n",
        "class PandasPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    简洁的预处理器：填充缺失值 + RobustScaler\n",
        "    直接使用 pandas 操作，保持 DataFrame 格式，避免列名丢失问题\n",
        "    \"\"\"\n",
        "    def __init__(self, impute_strategy='median'):\n",
        "        self.impute_strategy = impute_strategy\n",
        "        \n",
        "    def fit(self, X, y=None):\n",
        "        # 计算填充值（中位数）\n",
        "        if self.impute_strategy == 'median':\n",
        "            self.fill_values_ = X.median()\n",
        "        elif self.impute_strategy == 'mean':\n",
        "            self.fill_values_ = X.mean()\n",
        "        else:\n",
        "            self.fill_values_ = X.median()\n",
        "        \n",
        "        # 对于全 NaN 列，用 0 填充\n",
        "        self.fill_values_ = self.fill_values_.fillna(0)\n",
        "        \n",
        "        # 计算 RobustScaler 参数\n",
        "        X_filled = X.fillna(self.fill_values_)\n",
        "        self.median_ = X_filled.median()\n",
        "        q1 = X_filled.quantile(0.25)\n",
        "        q3 = X_filled.quantile(0.75)\n",
        "        self.iqr_ = q3 - q1\n",
        "        # 避免除以零\n",
        "        self.iqr_ = self.iqr_.replace(0, 1)\n",
        "        \n",
        "        self.columns_ = X.columns.tolist()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # 只处理训练时见过的列\n",
        "        X = X[[c for c in self.columns_ if c in X.columns]].copy()\n",
        "        \n",
        "        # 填充缺失值\n",
        "        X = X.fillna(self.fill_values_)\n",
        "        \n",
        "        # RobustScaler: (X - median) / IQR\n",
        "        X = (X - self.median_) / self.iqr_\n",
        "        \n",
        "        return X\n",
        "\n",
        "# ============================================================\n",
        "# 1. 定义 ad_sharpe_ratio\n",
        "# ============================================================\n",
        "\n",
        "def ad_sharpe_ratio_scorer(positions, market_excess_returns, risk_free_rate):\n",
        "    \"\"\"\n",
        "    计算调整后的夏普率 (Adjusted Sharpe Ratio)\n",
        "    \n",
        "    参数:\n",
        "    positions (np.array): 模型的最终仓位 (0 to 2)\n",
        "    market_excess_returns (np.array): 市场的超额回报\n",
        "    risk_free_rate (np.array): 同期的无风险利率\n",
        "    \"\"\"\n",
        "    pos = np.asarray(positions, dtype=float)\n",
        "    y_excess = np.asarray(market_excess_returns, dtype=float)\n",
        "    rf = np.asarray(risk_free_rate, dtype=float)\n",
        "    \n",
        "    # 检查长度是否匹配\n",
        "    min_len = min(len(pos), len(y_excess), len(rf))\n",
        "    if min_len == 0:\n",
        "        return 0.0\n",
        "    \n",
        "    # 如果长度不匹配，截取到最小长度\n",
        "    if len(pos) != min_len or len(y_excess) != min_len or len(rf) != min_len:\n",
        "        pos = pos[:min_len]\n",
        "        y_excess = y_excess[:min_len]\n",
        "        rf = rf[:min_len]\n",
        "        \n",
        "    TRADING_DAYS_PER_YR = 252\n",
        "    epsilon = 1e-8\n",
        "\n",
        "    # 重建总回报\n",
        "    y_total = y_excess + rf\n",
        "    \n",
        "    # 计算策略的总回报\n",
        "    strategy_total_returns = rf * (1 - pos) + pos * y_total\n",
        "    \n",
        "    # 计算策略的超额回报\n",
        "    strategy_excess_returns = strategy_total_returns - rf\n",
        "    \n",
        "    # 检查输入数据中是否有 nan\n",
        "    if np.isnan(pos).any() or np.isnan(y_excess).any() or np.isnan(rf).any():\n",
        "        raise ValueError(\n",
        "            f\"ad_sharpe_ratio_scorer 输入包含 nan 值: \"\n",
        "            f\"positions nan count={np.isnan(pos).sum()}, \"\n",
        "            f\"market_excess nan count={np.isnan(y_excess).sum()}, \"\n",
        "            f\"rf nan count={np.isnan(rf).sum()}\"\n",
        "        )\n",
        "    \n",
        "    # 计算年化夏普率\n",
        "    mean_excess = np.mean(strategy_excess_returns)\n",
        "    std_excess = np.std(strategy_excess_returns)\n",
        "    \n",
        "    if std_excess < epsilon:\n",
        "        return 0.0\n",
        "    \n",
        "    sharpe_daily = mean_excess / std_excess\n",
        "    sharpe_annualized = sharpe_daily * np.sqrt(TRADING_DAYS_PER_YR)\n",
        "    \n",
        "    return sharpe_annualized\n",
        "\n",
        "def generate_hft_positions(\n",
        "    y_pred_series: pd.Series, \n",
        "    y_true_lag1_series: pd.Series, \n",
        "    span_N: int = 60, \n",
        "    sensitivity_k: float = 1,\n",
        "    allow_short: bool = False, # 新增：是否允许做空\n",
        "    target_volatility: float = 0.20 # 新增：目标波动率（可选）\n",
        ") -> pd.Series:\n",
        "    \n",
        "    # ... (前序数据对齐代码保持不变) ...\n",
        "    if not isinstance(y_pred_series, pd.Series):\n",
        "        y_pred_series = pd.Series(y_pred_series)\n",
        "    if not isinstance(y_true_lag1_series, pd.Series):\n",
        "        y_true_lag1_series = pd.Series(y_true_lag1_series)\n",
        "    y_pred_aligned, y_true_lag1_aligned = y_pred_series.align(y_true_lag1_series, join='outer')\n",
        "\n",
        "    # --- 改进 1: 更标准的波动率计算 ---\n",
        "    # 使用标准差而不是平方均值，更稳健\n",
        "    rolling_std = y_true_lag1_aligned.ewm(span=span_N, min_periods=max(5, span_N // 10)).std()\n",
        "    \n",
        "    # 填充缺失值和防止除零\n",
        "    global_std = y_true_lag1_aligned.std() if not np.isnan(y_true_lag1_aligned.std()) else 1e-5\n",
        "    rolling_std = rolling_std.fillna(global_std).clip(lower=1e-6)\n",
        "\n",
        "    # --- 改进 2: 信号标准化 ---\n",
        "    # 将预测值根据当前市场波动率进行标准化\n",
        "    # 含义：当前的预测值相对于近期的波动来说，算强还是弱？\n",
        "    normalized_pred = y_pred_aligned / rolling_std\n",
        "    \n",
        "    # --- 改进 3: 信号生成 (tanh) ---\n",
        "    raw_signal = np.tanh(normalized_pred * sensitivity_k)\n",
        "    \n",
        "    # --- 改进 4: 灵活的方向控制 ---\n",
        "    if allow_short:\n",
        "        # 如果允许做空，仓位在 [-2, 2] 之间\n",
        "        positions = raw_signal * 2 \n",
        "    else:\n",
        "        # 如果只做多，保留 Long 仓位，负预测值对应 0 仓位\n",
        "        positions = np.maximum(0, raw_signal) * 2\n",
        "\n",
        "    positions = positions.fillna(0.0)\n",
        "    \n",
        "    if not isinstance(positions, pd.Series):\n",
        "        positions = pd.Series(positions, index=y_pred_aligned.index)\n",
        "        \n",
        "    return positions\n",
        "\n",
        "print(\"✓ 已定义 ad_sharpe_ratio_scorer 和 generate_hft_positions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 已定义 AutoFeatureSelectorWithLoss (使用损失函数选择因子)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 2. 定义自动选择因子模型（具备损失函数来选择因子）\n",
        "# ============================================================\n",
        "\n",
        "import warnings\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class AutoFeatureSelectorWithLoss(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    自动因子筛选模型 - 使用损失函数来选择因子\n",
        "    \n",
        "    核心功能：\n",
        "    1. 因子聚类（将相似因子分组）\n",
        "    2. 使用自定义损失函数评估每个因子组的重要性\n",
        "    3. 动态筛选：删除损失函数值高的组，保留损失函数值低的组\n",
        "    4. 每次训练都重新筛选\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        base_model,\n",
        "        loss_function=None,  # 自定义损失函数，用于评估因子重要性\n",
        "        n_clusters=None,\n",
        "        min_cluster_size=3,\n",
        "        max_clusters=15,\n",
        "        loss_threshold=None,  # 损失函数阈值，高于此值的组将被删除\n",
        "        top_k_per_group=0.65,  # 从 0.80 降低到 0.65，只保留每个组内最重要的 65% 特征\n",
        "        clustering_method='kmeans',\n",
        "        n_repeats=5,\n",
        "        random_state=42,\n",
        "        verbose=1,\n",
        "        verbose_clustering=None  # 是否输出聚类日志，None表示使用verbose的值\n",
        "    ):\n",
        "        \"\"\"\n",
        "        参数:\n",
        "        base_model: 基础模型（用于训练和评估）\n",
        "        loss_function: 自定义损失函数，接收 (y_true, y_pred, positions, market_excess, rf) 返回损失值\n",
        "        loss_threshold: 损失函数阈值，None表示自动选择\n",
        "        verbose_clustering: 是否输出聚类相关日志，None表示使用verbose的值，True/False表示强制开启/关闭\n",
        "        \"\"\"\n",
        "        self.base_model = base_model\n",
        "        self.loss_function = loss_function\n",
        "        self.n_clusters = n_clusters\n",
        "        self.min_cluster_size = min_cluster_size\n",
        "        self.max_clusters = max_clusters\n",
        "        self.loss_threshold = loss_threshold\n",
        "        self.top_k_per_group = top_k_per_group\n",
        "        self.clustering_method = clustering_method\n",
        "        self.n_repeats = n_repeats\n",
        "        self.random_state = random_state\n",
        "        self.verbose = verbose\n",
        "        self.verbose_clustering = verbose_clustering if verbose_clustering is not None else verbose\n",
        "        \n",
        "        # 存储结果\n",
        "        self.feature_groups_ = None\n",
        "        self.group_loss_ = None\n",
        "        self.selected_features_ = None\n",
        "        self.feature_to_group_ = None\n",
        "        self.clustering_model_ = None\n",
        "    \n",
        "    def _should_log_clustering(self, level=1):\n",
        "        \"\"\"判断是否应该输出聚类日志\"\"\"\n",
        "        return self.verbose_clustering >= level\n",
        "    \n",
        "    def _default_loss_function(self, y_true, y_pred, positions, market_excess, rf):\n",
        "        \"\"\"\n",
        "        默认损失函数：负的调整后夏普率\n",
        "        \n",
        "        损失函数设计原则：返回需要最小化的值\n",
        "        - 如果夏普率越大越好，则损失 = -夏普率（越小越好）\n",
        "        - 这样在置换测试中：损失增加 = 置换后损失 - 基线损失 > 0 表示因子重要\n",
        "        \n",
        "        示例：\n",
        "        - 基准：Sharpe=2.0 -> Loss=-2.0\n",
        "        - 置换后：Sharpe=1.0 -> Loss=-1.0\n",
        "        - loss_increase = -1.0 - (-2.0) = 1.0 > 0，表示损失增加，因子重要\n",
        "        \"\"\"\n",
        "        sharpe = ad_sharpe_ratio_scorer(positions, market_excess, rf)\n",
        "        return -sharpe  # 返回负值，因为我们要最小化损失（损失越小，夏普率越大）\n",
        "    \n",
        "    def _compute_feature_correlation(self, X):\n",
        "        \"\"\"计算因子之间的相关性矩阵\"\"\"\n",
        "        if self._should_log_clustering(2):\n",
        "            print(\"  计算因子相关性矩阵...\")\n",
        "        \n",
        "        # 处理输入：如果是 DataFrame，先处理 NaN\n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            # 检查 NaN 情况\n",
        "            nan_counts = X.isna().sum()\n",
        "            if nan_counts.sum() > 0:\n",
        "                if self._should_log_clustering(2):\n",
        "                    print(f\"  警告: 发现 {nan_counts.sum()} 个 NaN 值，使用前向填充和均值填充\")\n",
        "                # 先尝试前向填充，然后均值填充\n",
        "                X_clean = X.fillna(method='ffill').fillna(X.mean())\n",
        "            else:\n",
        "                X_clean = X\n",
        "            corr_matrix = X_clean.corr().abs()\n",
        "        else:\n",
        "            # 如果是 numpy 数组，转换为 DataFrame 并处理 NaN\n",
        "            X_df = pd.DataFrame(X)\n",
        "            nan_counts = X_df.isna().sum()\n",
        "            if nan_counts.sum() > 0:\n",
        "                if self._should_log_clustering(2):\n",
        "                    print(f\"  警告: 发现 {nan_counts.sum()} 个 NaN 值，使用均值填充\")\n",
        "                X_clean = X_df.fillna(X_df.mean())\n",
        "            else:\n",
        "                X_clean = X_df\n",
        "            corr_matrix = X_clean.corr().abs()\n",
        "        \n",
        "        # 确保相关性矩阵中没有 NaN（如果仍有 NaN，用 0 填充）\n",
        "        corr_matrix = corr_matrix.fillna(0)\n",
        "        \n",
        "        # 计算距离矩阵：1 - abs(correlation)\n",
        "        # 相关性越高（接近1），距离越小（接近0）\n",
        "        # 相关性越低（接近0），距离越大（接近1）\n",
        "        distance_matrix = 1 - corr_matrix.values\n",
        "        \n",
        "        # 确保距离矩阵是有效的：\n",
        "        # 1. 没有 NaN\n",
        "        distance_matrix = np.nan_to_num(distance_matrix, nan=1.0, posinf=1.0, neginf=1.0)\n",
        "        \n",
        "        # 2. 确保非负（距离不能为负）\n",
        "        distance_matrix = np.clip(distance_matrix, 0, 1)\n",
        "        \n",
        "        # 3. 确保对称（距离矩阵应该是对称的）\n",
        "        distance_matrix = (distance_matrix + distance_matrix.T) / 2\n",
        "        \n",
        "        # 4. 对角线应该是 0（特征与自己的距离为 0）\n",
        "        np.fill_diagonal(distance_matrix, 0)\n",
        "        \n",
        "        if self._should_log_clustering(2):\n",
        "            print(f\"  距离矩阵统计: min={distance_matrix.min():.4f}, max={distance_matrix.max():.4f}, \"\n",
        "                  f\"mean={distance_matrix.mean():.4f}, has_nan={np.isnan(distance_matrix).any()}\")\n",
        "        \n",
        "        return distance_matrix, corr_matrix\n",
        "    \n",
        "    def _find_optimal_clusters(self, X, distance_matrix):\n",
        "        \"\"\"自动选择最优聚类数量\"\"\"\n",
        "        if self._should_log_clustering(1):\n",
        "            print(\"  自动选择最优聚类数量...\")\n",
        "        \n",
        "        n_features = X.shape[1]\n",
        "        max_k = min(self.max_clusters, n_features // self.min_cluster_size, 15)\n",
        "        min_k = max(2, n_features // 20)\n",
        "        \n",
        "        if max_k < min_k:\n",
        "            return min_k\n",
        "        \n",
        "        best_k = min_k\n",
        "        best_score = -1\n",
        "        \n",
        "        # 验证距离矩阵的有效性\n",
        "        if np.isnan(distance_matrix).any() or np.isinf(distance_matrix).any():\n",
        "            if self._should_log_clustering(1):\n",
        "                print(\"  警告: 距离矩阵包含 NaN 或 Inf，跳过轮廓系数计算，使用默认聚类数量\")\n",
        "            return min_k\n",
        "        \n",
        "        for k in range(min_k, max_k + 1):\n",
        "            try:\n",
        "                if self.clustering_method == 'kmeans':\n",
        "                    from sklearn.decomposition import PCA\n",
        "                    # 对于 kmeans，使用原始数据（需要先处理 NaN）\n",
        "                    X_clean = X.fillna(method='ffill').fillna(X.mean()) if isinstance(X, pd.DataFrame) else pd.DataFrame(X).fillna(method='ffill').fillna(pd.DataFrame(X).mean())\n",
        "                    X_values = X_clean.values if isinstance(X_clean, pd.DataFrame) else X_clean\n",
        "                    \n",
        "                    # PCA 降维\n",
        "                    n_components = min(k*2, n_features-1, X_values.shape[0]-1)\n",
        "                    if n_components < 1:\n",
        "                        n_components = 1\n",
        "                    if X_values.shape[1] > n_components:\n",
        "                        pca = PCA(n_components=n_components, random_state=self.random_state)\n",
        "                        X_pca = pca.fit_transform(X_values)\n",
        "                    else:\n",
        "                        X_pca = X_values\n",
        "                    \n",
        "                    kmeans = KMeans(n_clusters=k, random_state=self.random_state, n_init=10)\n",
        "                    labels = kmeans.fit_predict(X_pca)\n",
        "                    \n",
        "                    # 对于 kmeans，使用原始数据计算轮廓系数（不使用 precomputed 距离）\n",
        "                    if len(np.unique(labels)) > 1 and X_pca.shape[0] > k:\n",
        "                        sil_score = silhouette_score(X_pca, labels, metric='euclidean')\n",
        "                        if sil_score > best_score:\n",
        "                            best_score = sil_score\n",
        "                            best_k = k\n",
        "                else:\n",
        "                    # 层次聚类：使用距离矩阵\n",
        "                    # 确保距离矩阵是有效的\n",
        "                    if np.isnan(distance_matrix).any() or np.isinf(distance_matrix).any():\n",
        "                        continue\n",
        "                    \n",
        "                    linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
        "                    labels = fcluster(linkage_matrix, k, criterion='maxclust') - 1\n",
        "                    \n",
        "                    # 对于层次聚类，使用距离矩阵计算轮廓系数\n",
        "                    if len(np.unique(labels)) > 1:\n",
        "                        sil_score = silhouette_score(distance_matrix, labels, metric='precomputed')\n",
        "                        if not np.isnan(sil_score) and sil_score > best_score:\n",
        "                            best_score = sil_score\n",
        "                            best_k = k\n",
        "            except Exception as e:\n",
        "                if self._should_log_clustering(2):\n",
        "                    print(f\"  警告: k={k} 时聚类失败: {str(e)}\")\n",
        "                continue\n",
        "        \n",
        "        if self._should_log_clustering(1):\n",
        "            print(f\"  最优聚类数量: {best_k} (轮廓系数: {best_score:.4f})\")\n",
        "        \n",
        "        return best_k\n",
        "    \n",
        "    def _cluster_features(self, X):\n",
        "        \"\"\"对因子进行聚类分组\"\"\"\n",
        "        if self._should_log_clustering(1):\n",
        "            print(\"  对因子进行聚类...\")\n",
        "        \n",
        "        feature_names = X.columns.tolist() if isinstance(X, pd.DataFrame) else [f'feature_{i}' for i in range(X.shape[1])]\n",
        "        n_features = len(feature_names)\n",
        "        \n",
        "        # 如果特征数量太少，直接返回每个特征一个组\n",
        "        if n_features <= self.min_cluster_size:\n",
        "            feature_groups = {0: feature_names}\n",
        "            feature_to_group = {name: 0 for name in feature_names}\n",
        "            if self._should_log_clustering(1):\n",
        "                print(f\"  特征数量太少 ({n_features})，跳过聚类，每个特征一个组\")\n",
        "            return feature_groups, feature_to_group\n",
        "        \n",
        "        distance_matrix, corr_matrix = self._compute_feature_correlation(X)\n",
        "        \n",
        "        if self.n_clusters is None:\n",
        "            n_clusters = self._find_optimal_clusters(X, distance_matrix)\n",
        "        else:\n",
        "            n_clusters = self.n_clusters\n",
        "        \n",
        "        # 确保聚类数量不超过特征数量\n",
        "        n_clusters = min(n_clusters, n_features)\n",
        "        \n",
        "        if self.clustering_method == 'kmeans':\n",
        "            from sklearn.decomposition import PCA\n",
        "            # 对于特征聚类，我们需要转置数据：每个特征是一个样本\n",
        "            # X 的形状是 (n_samples, n_features)，我们需要 (n_features, n_features) 的相关性表示\n",
        "            # 或者使用特征的相关性矩阵进行降维\n",
        "            n_components = min(n_clusters * 2, n_features - 1, 50, X.shape[0] - 1)\n",
        "            if n_components < 1:\n",
        "                n_components = 1\n",
        "            \n",
        "            # 使用特征的相关性矩阵进行降维（转置后每个特征是一个样本）\n",
        "            if isinstance(X, pd.DataFrame):\n",
        "                X_for_clustering = X.T.values  # 转置：每个特征是一个样本\n",
        "            else:\n",
        "                X_for_clustering = X.T\n",
        "            \n",
        "            # 如果特征数量大于样本数量，使用 PCA 降维\n",
        "            if X_for_clustering.shape[1] > n_components:\n",
        "                pca = PCA(n_components=n_components, random_state=self.random_state)\n",
        "                X_pca = pca.fit_transform(X_for_clustering)\n",
        "            else:\n",
        "                X_pca = X_for_clustering\n",
        "            \n",
        "            kmeans = KMeans(n_clusters=n_clusters, random_state=self.random_state, n_init=10)\n",
        "            labels = kmeans.fit_predict(X_pca)\n",
        "            self.clustering_model_ = kmeans\n",
        "        else:\n",
        "            # 层次聚类：使用距离矩阵\n",
        "            linkage_matrix = linkage(squareform(distance_matrix), method='ward')\n",
        "            labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust') - 1\n",
        "        \n",
        "        # 确保 labels 长度与 feature_names 匹配\n",
        "        if len(labels) != len(feature_names):\n",
        "            if self._should_log_clustering(1):\n",
        "                print(f\"  警告: labels 长度 ({len(labels)}) 与 feature_names 长度 ({len(feature_names)}) 不匹配\")\n",
        "            # 如果长度不匹配，使用索引范围\n",
        "            min_len = min(len(labels), len(feature_names))\n",
        "            labels = labels[:min_len]\n",
        "            feature_names = feature_names[:min_len]\n",
        "        \n",
        "        feature_groups = {}\n",
        "        feature_to_group = {}\n",
        "        \n",
        "        for i, label in enumerate(labels):\n",
        "            if i >= len(feature_names):\n",
        "                break\n",
        "            if label not in feature_groups:\n",
        "                feature_groups[label] = []\n",
        "            feature_groups[label].append(feature_names[i])\n",
        "            feature_to_group[feature_names[i]] = label\n",
        "        \n",
        "        filtered_groups = {}\n",
        "        for group_id, features in feature_groups.items():\n",
        "            if len(features) >= self.min_cluster_size:\n",
        "                filtered_groups[group_id] = features\n",
        "        \n",
        "        if self._should_log_clustering(1):\n",
        "            print(f\"  聚类完成: {len(filtered_groups)} 个有效组 (总因子数: {len(feature_names)})\")\n",
        "        \n",
        "        return filtered_groups, feature_to_group\n",
        "    \n",
        "    def _compute_group_loss(self, model, X, y, feature_groups, market_excess, rf):\n",
        "        \"\"\"\n",
        "        使用损失函数计算每个因子组的重要性\n",
        "        \n",
        "        方法：对每个组的因子进行随机置换，使用已训练模型直接预测（推理阶段置换）\n",
        "        损失增加越大，因子组越重要\n",
        "        \n",
        "        性能优化：不重新训练模型，直接使用已训练模型预测置换后的数据\n",
        "        \"\"\"\n",
        "        if self.verbose >= 1:\n",
        "            print(\"  使用损失函数计算因子组重要性（推理阶段置换，不重新训练）...\")\n",
        "        \n",
        "        # 确保 market_excess 和 rf 与 X 的索引/长度对齐\n",
        "        n_samples = len(X) if hasattr(X, '__len__') else (len(y) if hasattr(y, '__len__') else len(market_excess) if hasattr(market_excess, '__len__') else len(rf))\n",
        "        \n",
        "        # 处理 market_excess：确保长度匹配和索引对齐\n",
        "        if isinstance(market_excess, pd.Series):\n",
        "            if isinstance(X, pd.DataFrame):\n",
        "                market_excess = market_excess.reindex(X.index, fill_value=np.nan).values\n",
        "            else:\n",
        "                market_excess = market_excess.values[:n_samples] if len(market_excess) >= n_samples else np.pad(market_excess.values, (0, n_samples - len(market_excess)), constant_values=np.nan)\n",
        "        elif hasattr(market_excess, '__len__'):\n",
        "            if len(market_excess) != n_samples:\n",
        "                if len(market_excess) > n_samples:\n",
        "                    market_excess = market_excess[:n_samples]\n",
        "                else:\n",
        "                    market_excess = np.pad(np.asarray(market_excess), (0, n_samples - len(market_excess)), constant_values=np.nan)\n",
        "            market_excess = np.asarray(market_excess, dtype=float)\n",
        "        else:\n",
        "            market_excess = np.full(n_samples, np.nan)\n",
        "        \n",
        "        # 处理 rf：确保长度匹配和索引对齐\n",
        "        if isinstance(rf, pd.Series):\n",
        "            if isinstance(X, pd.DataFrame):\n",
        "                rf = rf.reindex(X.index, fill_value=np.nan).values\n",
        "            else:\n",
        "                rf = rf.values[:n_samples] if len(rf) >= n_samples else np.pad(rf.values, (0, n_samples - len(rf)), constant_values=np.nan)\n",
        "        elif hasattr(rf, '__len__'):\n",
        "            if len(rf) != n_samples:\n",
        "                if len(rf) > n_samples:\n",
        "                    rf = rf[:n_samples]\n",
        "                else:\n",
        "                    rf = np.pad(np.asarray(rf), (0, n_samples - len(rf)), constant_values=np.nan)\n",
        "            rf = np.asarray(rf, dtype=float)\n",
        "        else:\n",
        "            rf = np.full(n_samples, np.nan)\n",
        "        \n",
        "        # 检查是否有 nan 值（不应该有，如果有则报错）\n",
        "        if np.isnan(market_excess).any():\n",
        "            nan_count = np.isnan(market_excess).sum()\n",
        "            nan_indices = np.where(np.isnan(market_excess))[0][:10]  # 只显示前10个\n",
        "            raise ValueError(\n",
        "                f\"market_excess 包含 {nan_count} 个 nan 值（索引示例: {nan_indices.tolist()}）。\"\n",
        "                f\"这不应该发生，请检查数据预处理流程。\"\n",
        "                f\"X.shape={X.shape if hasattr(X, 'shape') else 'N/A'}, \"\n",
        "                f\"market_excess.shape={market_excess.shape}, \"\n",
        "                f\"X.index={X.index[:5].tolist() if isinstance(X, pd.DataFrame) else 'N/A'}...\"\n",
        "            )\n",
        "        \n",
        "        if np.isnan(rf).any():\n",
        "            nan_count = np.isnan(rf).sum()\n",
        "            nan_indices = np.where(np.isnan(rf))[0][:10]  # 只显示前10个\n",
        "            raise ValueError(\n",
        "                f\"rf 包含 {nan_count} 个 nan 值（索引示例: {nan_indices.tolist()}）。\"\n",
        "                f\"这不应该发生，请检查数据预处理流程。\"\n",
        "                f\"X.shape={X.shape if hasattr(X, 'shape') else 'N/A'}, \"\n",
        "                f\"rf.shape={rf.shape}, \"\n",
        "                f\"X.index={X.index[:5].tolist() if isinstance(X, pd.DataFrame) else 'N/A'}...\"\n",
        "            )\n",
        "        \n",
        "        # 使用默认损失函数或自定义损失函数\n",
        "        loss_func = self.loss_function if self.loss_function is not None else self._default_loss_function\n",
        "        \n",
        "        # 检测损失函数类型（使用函数名称检测，避免未定义函数的引用错误）\n",
        "        loss_func_name = getattr(loss_func, '__name__', '')\n",
        "        is_direction_loss = loss_func_name == 'direction_loss_function'\n",
        "        is_metalabel_loss = loss_func_name == 'metalabel_loss_function'\n",
        "        is_opportunity_loss = loss_func_name == 'opportunity_loss_function'\n",
        "        \n",
        "        # 使用已训练的模型（不再重新训练）\n",
        "        # 注意：model 应该已经在外部训练好了\n",
        "        y_pred_baseline = model.predict(X)\n",
        "        \n",
        "        # 生成仓位（根据损失函数类型决定）\n",
        "        y_pred_series = pd.Series(y_pred_baseline, index=X.index if isinstance(X, pd.DataFrame) else range(len(y_pred_baseline)))\n",
        "        \n",
        "        if is_direction_loss or is_opportunity_loss or is_metalabel_loss:\n",
        "            # 对于并行模型的损失函数，损失函数内部会自己处理 y_pred\n",
        "            # positions 参数不会被使用，生成一个占位符\n",
        "            positions_baseline = np.zeros(len(y_pred_series))\n",
        "        else:\n",
        "            # 对于其他模型（如默认损失函数），y_pred 是收益率，需要通过 generate_hft_positions 生成仓位\n",
        "            forward_returns_lag = X['forward_returns_lag'] if 'forward_returns_lag' in X.columns else pd.Series(np.zeros(len(y)), index=y_pred_series.index)\n",
        "            positions_baseline = generate_hft_positions(y_pred_series, forward_returns_lag).values\n",
        "        \n",
        "        # 计算基线损失\n",
        "        baseline_loss = loss_func(y, y_pred_baseline, positions_baseline, market_excess, rf)\n",
        "        \n",
        "        # 检查基线损失是否为 nan 或 inf（不应该发生，如果有则报错）\n",
        "        if np.isnan(baseline_loss) or np.isinf(baseline_loss):\n",
        "            raise ValueError(\n",
        "                f\"基线损失计算异常 (baseline_loss={baseline_loss})。\"\n",
        "                f\"这不应该发生，请检查：\"\n",
        "                f\"1. market_excess 和 rf 是否正确传递（无 nan）\"\n",
        "                f\"2. positions_baseline 是否正确生成（无 nan）\"\n",
        "                f\"3. 损失函数计算是否正确\"\n",
        "                f\"market_excess nan count={np.isnan(market_excess).sum() if hasattr(market_excess, '__len__') else 'N/A'}, \"\n",
        "                f\"rf nan count={np.isnan(rf).sum() if hasattr(rf, '__len__') else 'N/A'}, \"\n",
        "                f\"positions_baseline nan count={np.isnan(positions_baseline).sum() if hasattr(positions_baseline, '__len__') else 'N/A'}\"\n",
        "            )\n",
        "        \n",
        "        group_loss = {}\n",
        "        np.random.seed(self.random_state)\n",
        "        \n",
        "        # 对每个组进行损失函数测试\n",
        "        for group_id, features in tqdm(feature_groups.items(), desc=\"  测试各组\", disable=self.verbose < 1):\n",
        "            valid_features = [f for f in features if f in X.columns]\n",
        "            if len(valid_features) == 0:\n",
        "                group_loss[group_id] = -np.inf  # 不重要组，损失增加为负\n",
        "                continue\n",
        "            \n",
        "            loss_scores = []\n",
        "            \n",
        "            for repeat in range(self.n_repeats):\n",
        "                X_permuted = X.copy()\n",
        "                \n",
        "                # 随机打乱当前组的所有因子\n",
        "                for feat in valid_features:\n",
        "                    X_permuted[feat] = np.random.permutation(X_permuted[feat].values)\n",
        "                \n",
        "                # 【性能优化】直接使用已训练模型预测，不重新训练\n",
        "                y_pred_permuted = model.predict(X_permuted)\n",
        "                \n",
        "                # 生成仓位（根据损失函数类型决定）\n",
        "                y_pred_series_perm = pd.Series(y_pred_permuted, index=X_permuted.index if isinstance(X_permuted, pd.DataFrame) else range(len(y_pred_permuted)))\n",
        "                \n",
        "                if is_direction_loss or is_opportunity_loss or is_metalabel_loss:\n",
        "                    # 对于并行模型的损失函数，损失函数内部会自己处理 y_pred\n",
        "                    # positions 参数不会被使用，生成一个占位符\n",
        "                    positions_permuted = np.zeros(len(y_pred_series_perm))\n",
        "                else:\n",
        "                    # 对于其他模型，需要通过 generate_hft_positions 生成仓位\n",
        "                    forward_returns_lag_perm = X_permuted['forward_returns_lag'] if 'forward_returns_lag' in X_permuted.columns else pd.Series(np.zeros(len(y)), index=y_pred_series_perm.index)\n",
        "                    positions_permuted = generate_hft_positions(y_pred_series_perm, forward_returns_lag_perm).values\n",
        "                \n",
        "                # 计算损失\n",
        "                permuted_loss = loss_func(y, y_pred_permuted, positions_permuted, market_excess, rf)\n",
        "                \n",
        "                # 检查 permuted_loss 是否为 nan 或 inf（不应该发生，如果有则报错）\n",
        "                if np.isnan(permuted_loss) or np.isinf(permuted_loss):\n",
        "                    raise ValueError(\n",
        "                        f\"组 {group_id} 第 {repeat+1} 次重复的损失计算异常 (permuted_loss={permuted_loss})。\"\n",
        "                        f\"这不应该发生，请检查 positions_permuted 是否正确生成（无 nan）\"\n",
        "                    )\n",
        "                \n",
        "                # 损失增加 = 置换后损失 - 基线损失\n",
        "                # 如果损失函数是负夏普率（越小越好）：\n",
        "                #   基准：Sharpe=2.0 -> Loss=-2.0\n",
        "                #   置换后：Sharpe=1.0 -> Loss=-1.0\n",
        "                #   loss_increase = -1.0 - (-2.0) = 1.0 > 0，表示损失增加，因子重要\n",
        "                loss_increase = permuted_loss - baseline_loss\n",
        "                \n",
        "                # 检查 loss_increase 是否为 nan 或 inf（不应该发生）\n",
        "                if np.isnan(loss_increase) or np.isinf(loss_increase):\n",
        "                    raise ValueError(\n",
        "                        f\"组 {group_id} 第 {repeat+1} 次重复的损失增加异常 (loss_increase={loss_increase})。\"\n",
        "                        f\"baseline_loss={baseline_loss}, permuted_loss={permuted_loss}\"\n",
        "                    )\n",
        "                \n",
        "                loss_scores.append(loss_increase)\n",
        "            \n",
        "            # 平均损失增加\n",
        "            if len(loss_scores) == 0:\n",
        "                raise ValueError(f\"组 {group_id} 的所有损失计算都失败，loss_scores 为空\")\n",
        "            \n",
        "            avg_loss_increase = np.mean(loss_scores)\n",
        "            \n",
        "            # 最终检查（不应该发生）\n",
        "            if np.isnan(avg_loss_increase) or np.isinf(avg_loss_increase):\n",
        "                raise ValueError(\n",
        "                    f\"组 {group_id} 的平均损失增加异常 (avg_loss_increase={avg_loss_increase})。\"\n",
        "                    f\"loss_scores={loss_scores[:5]}... (显示前5个)\"\n",
        "                )\n",
        "            \n",
        "            group_loss[group_id] = avg_loss_increase\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            print(f\"  组损失计算完成\")\n",
        "            sorted_groups = sorted(group_loss.items(), key=lambda x: x[1], reverse=True)\n",
        "            print(f\"  Top 5 重要组 (损失增加越大越重要):\")\n",
        "            for group_id, loss_inc in sorted_groups[:5]:\n",
        "                n_features = len(feature_groups[group_id])\n",
        "                print(f\"    组 {group_id} ({n_features}个因子): 损失增加 = {loss_inc:.6f}\")\n",
        "        \n",
        "        return group_loss\n",
        "    \n",
        "    def _select_features_from_groups(self, feature_groups, group_loss, X, y, trained_model=None):\n",
        "        \"\"\"\n",
        "        从重要组中选择因子\n",
        "        \n",
        "        改进：使用全局模型的特征重要性，而不是单独训练组内模型\n",
        "        理由：有些因子是\"辅助因子\"，单独训练时重要性低，但在全量模型中配合其他因子非常有效\n",
        "        \"\"\"\n",
        "        if self.verbose >= 1:\n",
        "            print(\"  从重要组中选择因子（使用全局模型特征重要性）...\")\n",
        "        \n",
        "        # 确定损失阈值\n",
        "        if self.loss_threshold is None:\n",
        "            # 自动选择：保留损失增加大于中位数的组\n",
        "            loss_values = list(group_loss.values())\n",
        "            if len(loss_values) > 0:\n",
        "                # 检查是否有 nan 或 inf（不应该发生）\n",
        "                invalid_values = [v for v in loss_values if np.isnan(v) or np.isinf(v)]\n",
        "                if len(invalid_values) > 0:\n",
        "                    raise ValueError(\n",
        "                        f\"发现 {len(invalid_values)} 个无效的损失值（nan 或 inf）。\"\n",
        "                        f\"这不应该发生，请检查组损失计算流程。\"\n",
        "                        f\"无效值示例: {invalid_values[:5]}\"\n",
        "                    )\n",
        "                threshold = np.median(loss_values)\n",
        "            else:\n",
        "                threshold = 0.0\n",
        "        else:\n",
        "            threshold = self.loss_threshold\n",
        "        \n",
        "        # 筛选重要组（损失增加大于阈值的组）\n",
        "        important_groups = {\n",
        "            group_id: features \n",
        "            for group_id, features in feature_groups.items()\n",
        "            if group_loss[group_id] > threshold\n",
        "        }\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            print(f\"  筛选出 {len(important_groups)} 个重要组 (阈值: {threshold:.6f})\")\n",
        "        \n",
        "        # 如果提供了已训练的模型，使用全局模型的特征重要性\n",
        "        if trained_model is not None:\n",
        "            # 获取全局模型的特征重要性\n",
        "            if hasattr(trained_model, 'get_feature_importance'):\n",
        "                global_feat_importance = pd.Series(\n",
        "                    trained_model.get_feature_importance(),\n",
        "                    index=X.columns\n",
        "                )\n",
        "            elif hasattr(trained_model, 'feature_importances_'):\n",
        "                global_feat_importance = pd.Series(\n",
        "                    trained_model.feature_importances_,\n",
        "                    index=X.columns\n",
        "                )\n",
        "            else:\n",
        "                global_feat_importance = None\n",
        "        else:\n",
        "            # 如果没有提供已训练模型，训练一个全局模型获取特征重要性\n",
        "            if self.verbose >= 1:\n",
        "                print(\"  训练全局模型以获取特征重要性...\")\n",
        "            global_model = clone(self.base_model)\n",
        "            global_model.fit(X, y)\n",
        "            \n",
        "            if hasattr(global_model, 'get_feature_importance'):\n",
        "                global_feat_importance = pd.Series(\n",
        "                    global_model.get_feature_importance(),\n",
        "                    index=X.columns\n",
        "                )\n",
        "            elif hasattr(global_model, 'feature_importances_'):\n",
        "                global_feat_importance = pd.Series(\n",
        "                    global_model.feature_importances_,\n",
        "                    index=X.columns\n",
        "                )\n",
        "            else:\n",
        "                global_feat_importance = None\n",
        "        \n",
        "        # 从重要组中选择因子\n",
        "        selected_features = []\n",
        "        total_before_selection = 0\n",
        "        total_after_selection = 0\n",
        "        \n",
        "        for group_id, features in important_groups.items():\n",
        "            total_before_selection += len(features)\n",
        "            \n",
        "            if global_feat_importance is not None:\n",
        "                # 只使用在 X.columns 和 global_feat_importance 中都存在的特征\n",
        "                valid_features = [\n",
        "                    f for f in features \n",
        "                    if f in X.columns and f in global_feat_importance.index\n",
        "                ]\n",
        "                \n",
        "                if len(valid_features) > 0:\n",
        "                    # 使用全局模型的特征重要性选择组内因子\n",
        "                    group_feat_importance = global_feat_importance[valid_features]\n",
        "                    \n",
        "                    # 基于实际可用特征数量计算选择数量\n",
        "                    n_select = max(1, int(len(valid_features) * self.top_k_per_group))\n",
        "                    \n",
        "                    # 选择 top_k 个最重要的因子\n",
        "                    top_features = group_feat_importance.nlargest(n_select).index.tolist()\n",
        "                    selected_features.extend(top_features)\n",
        "                    total_after_selection += len(top_features)\n",
        "                    \n",
        "                    if self.verbose >= 1:\n",
        "                        print(f\"    组 {group_id}: {len(valid_features)} 个有效因子 -> 选择 {len(top_features)} 个 (top_k={self.top_k_per_group:.2f})\")\n",
        "                else:\n",
        "                    if self.verbose >= 1:\n",
        "                        print(f\"    组 {group_id}: 没有有效特征，跳过\")\n",
        "            else:\n",
        "                # 如果没有特征重要性，保留组内所有因子（但只保留在 X.columns 中的）\n",
        "                valid_features = [f for f in features if f in X.columns]\n",
        "                selected_features.extend(valid_features)\n",
        "                total_after_selection += len(valid_features)\n",
        "                if self.verbose >= 1:\n",
        "                    print(f\"    组 {group_id}: 无法获取特征重要性，保留所有 {len(valid_features)} 个有效因子\")\n",
        "        \n",
        "        self.selected_features_ = list(set(selected_features))  # 去重\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            print(f\"  组内因子选择统计:\")\n",
        "            print(f\"    重要组总因子数: {total_before_selection}\")\n",
        "            print(f\"    选择后因子数: {total_after_selection}\")\n",
        "            print(f\"    去重后因子数: {len(self.selected_features_)}\")\n",
        "            print(f\"    最终选择 {len(self.selected_features_)} 个因子 (top_k_per_group={self.top_k_per_group:.2f})\")\n",
        "        \n",
        "        return self.selected_features_\n",
        "    \n",
        "    def fit(self, X, y, market_excess=None, rf=None):\n",
        "        \"\"\"\n",
        "        拟合模型并选择因子\n",
        "        \n",
        "        参数:\n",
        "        X: 特征数据\n",
        "        y: 目标变量\n",
        "        market_excess: 市场超额回报（用于损失函数计算）\n",
        "        rf: 无风险利率（用于损失函数计算）\n",
        "        \"\"\"\n",
        "        if market_excess is None:\n",
        "            # 如果没有提供，尝试从X中获取\n",
        "            if 'market_forward_excess_returns' in X.columns:\n",
        "                market_excess = X['market_forward_excess_returns'].values\n",
        "            else:\n",
        "                market_excess = np.zeros(len(y))\n",
        "        \n",
        "        if rf is None:\n",
        "            if 'risk_free_rate' in X.columns:\n",
        "                rf = X['risk_free_rate'].values\n",
        "            else:\n",
        "                rf = np.zeros(len(y))\n",
        "        \n",
        "        # 聚类因子\n",
        "        self.feature_groups_, self.feature_to_group_ = self._cluster_features(X)\n",
        "        \n",
        "        # 训练全局模型（只训练一次，用于计算组损失和特征重要性）\n",
        "        trained_model = clone(self.base_model)\n",
        "        trained_model.fit(X, y)\n",
        "        \n",
        "        # 计算组损失（使用已训练的模型，不重新训练）\n",
        "        self.group_loss_ = self._compute_group_loss(\n",
        "            trained_model, X, y, self.feature_groups_, market_excess, rf\n",
        "        )\n",
        "        \n",
        "        # 选择因子（使用已训练的全局模型的特征重要性）\n",
        "        self.selected_features_ = self._select_features_from_groups(\n",
        "            self.feature_groups_, self.group_loss_, X, y, trained_model=trained_model\n",
        "        )\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def transform(self, X):\n",
        "        \"\"\"返回选中的因子\"\"\"\n",
        "        if self.selected_features_ is None:\n",
        "            raise ValueError(\"模型尚未拟合，请先调用 fit()\")\n",
        "        \n",
        "        # 只返回存在的特征\n",
        "        available_features = [f for f in self.selected_features_ if f in X.columns]\n",
        "        \n",
        "        if isinstance(X, pd.DataFrame):\n",
        "            # 确保返回的 DataFrame 保持原始索引\n",
        "            return X[available_features].copy()\n",
        "        else:\n",
        "            # 如果是numpy数组，需要映射索引\n",
        "            feature_indices = [X.columns.get_loc(f) for f in available_features if f in X.columns]\n",
        "            return X[:, feature_indices]\n",
        "    \n",
        "    def fit_transform(self, X, y, market_excess=None, rf=None):\n",
        "        \"\"\"拟合并转换\"\"\"\n",
        "        return self.fit(X, y, market_excess, rf).transform(X)\n",
        "\n",
        "print(\"✓ 已定义 AutoFeatureSelectorWithLoss (使用损失函数选择因子)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 已定义 opportunity_loss_function（机会模型特征选择损失函数）\n",
            "✓ 已定义 direction_loss_function（方向模型特征选择损失函数）\n",
            "✓ 已定义 OpportunityPipeline（机会模型，MSE，带特征选择）\n",
            "✓ 已定义 DirectionClassifierPipeline（方向分类模型，Logloss，带特征选择）\n",
            "✓ 已定义 ParallelEnsemblePipeline（并行集成模型）\n",
            "✓ 已定义 move_forward_two_way_test（两段滑动窗口测试）\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 并行模型架构（带特征选择）\n",
        "# ============================================================\n",
        "# 1. OpportunityRegressor: 预测机会大小 |HFT position|，使用 MSE\n",
        "# 2. DirectionClassifier: 预测方向 {0=空, 1=多}，使用 Logloss\n",
        "# 3. 两个模型各自独立做特征选择（AutoFeatureSelectorWithLoss）\n",
        "# 4. 最终仓位 = 方向(±1) × 机会\n",
        "# ============================================================\n",
        "\n",
        "# ============================================================\n",
        "# 1. 机会模型的损失函数（用于特征选择）\n",
        "# ============================================================\n",
        "\n",
        "def opportunity_loss_function(y_true, y_pred, positions, market_excess, rf):\n",
        "    \"\"\"\n",
        "    机会模型的损失函数：基于预测机会大小的 Adjusted Sharpe Ratio\n",
        "    \"\"\"\n",
        "    if isinstance(y_true, pd.Series):\n",
        "        true_direction = np.sign(y_true.values)\n",
        "    else:\n",
        "        true_direction = np.sign(y_true)\n",
        "    \n",
        "    if isinstance(y_pred, pd.Series):\n",
        "        opportunity_pred = y_pred.values\n",
        "    else:\n",
        "        opportunity_pred = np.array(y_pred)\n",
        "    \n",
        "    test_positions = np.abs(opportunity_pred)\n",
        "    test_positions = np.clip(test_positions, 0, 2)\n",
        "    final_positions = np.where(true_direction >= 0, test_positions, 0)\n",
        "    \n",
        "    sharpe = ad_sharpe_ratio_scorer(final_positions, market_excess, rf)\n",
        "    return -sharpe\n",
        "\n",
        "\n",
        "def direction_loss_function(y_true, y_pred, positions, market_excess, rf):\n",
        "    \"\"\"\n",
        "    方向模型的损失函数：基于方向预测准确率的 Adjusted Sharpe Ratio\n",
        "    \"\"\"\n",
        "    if isinstance(y_true, pd.Series):\n",
        "        y_true_values = y_true.values\n",
        "    else:\n",
        "        y_true_values = np.array(y_true)\n",
        "    \n",
        "    if isinstance(y_pred, pd.Series):\n",
        "        y_pred_values = y_pred.values\n",
        "    else:\n",
        "        y_pred_values = np.array(y_pred)\n",
        "    \n",
        "    if np.max(y_pred_values) <= 1 and np.min(y_pred_values) >= 0:\n",
        "        direction_pred = np.where(y_pred_values > 0.5, 1, -1)\n",
        "    else:\n",
        "        direction_pred = np.sign(y_pred_values)\n",
        "    \n",
        "    test_positions = np.where(direction_pred > 0, 1.0, 0.0)\n",
        "    sharpe = ad_sharpe_ratio_scorer(test_positions, market_excess, rf)\n",
        "    return -sharpe\n",
        "\n",
        "\n",
        "print(\"✓ 已定义 opportunity_loss_function（机会模型特征选择损失函数）\")\n",
        "print(\"✓ 已定义 direction_loss_function（方向模型特征选择损失函数）\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. OpportunityPipeline - 预测机会大小（回归）\n",
        "# ============================================================\n",
        "\n",
        "class OpportunityPipeline(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"\n",
        "    机会模型 Pipeline：预测 HFT 理想仓位的绝对值\n",
        "    目标：|generate_hft_positions(market_excess_return, ...)|\n",
        "    损失函数：MSE\n",
        "    特征选择：AutoFeatureSelectorWithLoss (使用 opportunity_loss_function)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_creator=None,\n",
        "        feature_selector=None,\n",
        "        feature_selector_params=None,\n",
        "        model=None,\n",
        "        verbose=1\n",
        "    ):\n",
        "        self.feature_creator = feature_creator\n",
        "        self.feature_selector = feature_selector\n",
        "        self.feature_selector_params = feature_selector_params\n",
        "        self.model = model or CatBoostRegressor(\n",
        "            iterations=800, learning_rate=0.004, depth=6,\n",
        "            min_data_in_leaf=20, l2_leaf_reg=7.0, random_strength=5.5,\n",
        "            colsample_bylevel=0.78, early_stopping_rounds=50,\n",
        "            bootstrap_type='Bernoulli', subsample=0.85,\n",
        "            loss_function='RMSE', verbose=0, random_seed=42\n",
        "        )\n",
        "        self.verbose = verbose\n",
        "        self.is_fitted_ = False\n",
        "        self.feature_pipeline_ = None\n",
        "        self.feature_selector_ = None\n",
        "        self.feature_names_ = None\n",
        "    \n",
        "    def _create_feature_pipeline(self):\n",
        "        steps = []\n",
        "        if self.feature_creator is not None:\n",
        "            steps.append(('feature_creator', FunctionTransformer(self.feature_creator)))\n",
        "        steps.append(('preprocessor', PandasPreprocessor(impute_strategy='median')))\n",
        "        return Pipeline(steps)\n",
        "    \n",
        "    def _get_or_create_feature_selector(self):\n",
        "        if self.feature_selector is not None:\n",
        "            return clone(self.feature_selector)\n",
        "        if self.feature_selector_params is not None:\n",
        "            selector_base_model = CatBoostRegressor(\n",
        "                iterations=800, learning_rate=0.004, depth=6, verbose=0, random_seed=42\n",
        "            )\n",
        "            return AutoFeatureSelectorWithLoss(\n",
        "                base_model=selector_base_model,\n",
        "                loss_function=opportunity_loss_function,\n",
        "                **self.feature_selector_params\n",
        "            )\n",
        "        return None\n",
        "    \n",
        "    def fit(self, X, y, market_excess=None, rf=None, sample_weight=None):\n",
        "        if self.verbose >= 1:\n",
        "            print(\"=\"*70)\n",
        "            print(\"OpportunityPipeline - 开始拟合（机会模型，MSE）\")\n",
        "            print(\"=\"*70)\n",
        "        \n",
        "        y_series = pd.Series(y, index=X.index) if not isinstance(y, pd.Series) else y\n",
        "        if market_excess is None:\n",
        "            market_excess = y_series\n",
        "        if rf is None:\n",
        "            rf = X['risk_free_rate_lag'].fillna(0) if 'risk_free_rate_lag' in X.columns else pd.Series(0, index=X.index)\n",
        "        \n",
        "        if 'forward_returns_lag' in X.columns:\n",
        "            forward_returns_lag = X['forward_returns_lag']\n",
        "        else:\n",
        "            forward_returns_lag = y_series.shift(1).fillna(0)\n",
        "        \n",
        "        ideal_positions = generate_hft_positions(y_series, forward_returns_lag, span_N=60, sensitivity_k=1, allow_short=True)\n",
        "        opportunity_target = np.abs(ideal_positions)\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            print(f\"机会目标统计: min={opportunity_target.min():.4f}, max={opportunity_target.max():.4f}, mean={opportunity_target.mean():.4f}\")\n",
        "            print(\"\\n[步骤1/3] 特征工程...\")\n",
        "        \n",
        "        self.feature_pipeline_ = self._create_feature_pipeline()\n",
        "        X_transformed = self.feature_pipeline_.fit_transform(X, opportunity_target)\n",
        "        if isinstance(X_transformed, pd.DataFrame):\n",
        "            X_transformed.index = X.index\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            print(f\"特征工程后特征数量: {X_transformed.shape[1]}\")\n",
        "        \n",
        "        self.feature_selector_ = self._get_or_create_feature_selector()\n",
        "        if self.feature_selector_ is not None:\n",
        "            if self.verbose >= 1:\n",
        "                print(\"\\n[步骤2/3] 特征选择（使用 opportunity_loss_function）...\")\n",
        "            me_aligned = market_excess.loc[X_transformed.index] if isinstance(market_excess, pd.Series) else market_excess\n",
        "            rf_aligned = rf.loc[X_transformed.index] if isinstance(rf, pd.Series) else rf\n",
        "            X_transformed = self.feature_selector_.fit_transform(X_transformed, opportunity_target, market_excess=me_aligned, rf=rf_aligned)\n",
        "            if self.verbose >= 1:\n",
        "                print(f\"特征选择后特征数量: {X_transformed.shape[1]}\")\n",
        "        else:\n",
        "            if self.verbose >= 1:\n",
        "                print(\"\\n[步骤2/3] 跳过特征选择（未配置）\")\n",
        "        \n",
        "        self.feature_names_ = X_transformed.columns.tolist() if isinstance(X_transformed, pd.DataFrame) else None\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            print(\"\\n[步骤3/3] 训练机会模型（MSE）...\")\n",
        "        \n",
        "        from sklearn.model_selection import train_test_split\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_transformed, opportunity_target, test_size=0.15, shuffle=False)\n",
        "        if sample_weight is not None:\n",
        "            sw_train = sample_weight[:len(X_train)] if not isinstance(sample_weight, pd.Series) else sample_weight.iloc[:len(X_train)]\n",
        "            self.model.fit(X_train, y_train, eval_set=(X_val, y_val), sample_weight=sw_train)\n",
        "        else:\n",
        "            self.model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
        "        \n",
        "        self.is_fitted_ = True\n",
        "        if self.verbose >= 1:\n",
        "            print(\"OpportunityPipeline - 拟合完成\")\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        if not self.is_fitted_:\n",
        "            raise ValueError(\"模型尚未拟合，请先调用 fit()\")\n",
        "        original_index = X.index if hasattr(X, 'index') else None\n",
        "        \n",
        "        X_transformed = self.feature_pipeline_.transform(X)\n",
        "        if isinstance(X_transformed, pd.DataFrame):\n",
        "            X_transformed.index = X.index\n",
        "        if self.feature_selector_ is not None:\n",
        "            X_transformed = self.feature_selector_.transform(X_transformed)\n",
        "        predictions = self.model.predict(X_transformed)\n",
        "        predictions = np.clip(predictions, 0, 2)\n",
        "        if original_index is not None:\n",
        "            predictions = pd.Series(predictions, index=original_index, name='opportunity_prediction')\n",
        "        return predictions\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. DirectionClassifierPipeline - 预测方向（二分类）\n",
        "# ============================================================\n",
        "\n",
        "class DirectionClassifierPipeline(BaseEstimator, ClassifierMixin):\n",
        "    \"\"\"\n",
        "    方向分类模型 Pipeline：预测市场方向 {0=空, 1=多}\n",
        "    目标：sign(market_excess_return) > 0 ? 1 : 0\n",
        "    损失函数：Logloss (Cross Entropy)\n",
        "    特征选择：AutoFeatureSelectorWithLoss (使用 direction_loss_function)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, feature_creator=None, feature_selector=None, feature_selector_params=None, model=None, verbose=1):\n",
        "        self.feature_creator = feature_creator\n",
        "        self.feature_selector = feature_selector\n",
        "        self.feature_selector_params = feature_selector_params\n",
        "        self.model = model or CatBoostClassifier(\n",
        "            iterations=800, learning_rate=0.004, depth=6,\n",
        "            min_data_in_leaf=20, l2_leaf_reg=7.0, random_strength=5.5,\n",
        "            colsample_bylevel=0.78, early_stopping_rounds=50,\n",
        "            bootstrap_type='Bernoulli', subsample=0.85,\n",
        "            loss_function='Logloss', verbose=0, random_seed=42\n",
        "        )\n",
        "        self.verbose = verbose\n",
        "        self.is_fitted_ = False\n",
        "        self.feature_pipeline_ = None\n",
        "        self.feature_selector_ = None\n",
        "        self.feature_names_ = None\n",
        "    \n",
        "    def _create_feature_pipeline(self):\n",
        "        steps = []\n",
        "        if self.feature_creator is not None:\n",
        "            steps.append(('feature_creator', FunctionTransformer(self.feature_creator)))\n",
        "        steps.append(('preprocessor', PandasPreprocessor(impute_strategy='median')))\n",
        "        return Pipeline(steps)\n",
        "    \n",
        "    def _get_or_create_feature_selector(self):\n",
        "        if self.feature_selector is not None:\n",
        "            return clone(self.feature_selector)\n",
        "        if self.feature_selector_params is not None:\n",
        "            selector_base_model = CatBoostClassifier(iterations=800, learning_rate=0.004, depth=6, verbose=0, random_seed=42)\n",
        "            return AutoFeatureSelectorWithLoss(\n",
        "                base_model=selector_base_model,\n",
        "                loss_function=direction_loss_function,\n",
        "                **self.feature_selector_params\n",
        "            )\n",
        "        return None\n",
        "    \n",
        "    def fit(self, X, y, market_excess=None, rf=None, sample_weight=None):\n",
        "        if self.verbose >= 1:\n",
        "            print(\"=\"*70)\n",
        "            print(\"DirectionClassifierPipeline - 开始拟合（方向分类，Logloss）\")\n",
        "            print(\"=\"*70)\n",
        "        \n",
        "        y_series = pd.Series(y, index=X.index) if not isinstance(y, pd.Series) else y\n",
        "        if market_excess is None:\n",
        "            market_excess = y_series\n",
        "        if rf is None:\n",
        "            rf = X['risk_free_rate_lag'].fillna(0) if 'risk_free_rate_lag' in X.columns else pd.Series(0, index=X.index)\n",
        "        \n",
        "        direction_labels = (y_series > 0).astype(int)\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            n_long = (direction_labels == 1).sum()\n",
        "            n_short = (direction_labels == 0).sum()\n",
        "            print(f\"方向标签分布: 多(1)={n_long} ({n_long/len(direction_labels):.2%}), 空(0)={n_short} ({n_short/len(direction_labels):.2%})\")\n",
        "            print(\"\\n[步骤1/3] 特征工程...\")\n",
        "        \n",
        "        self.feature_pipeline_ = self._create_feature_pipeline()\n",
        "        X_transformed = self.feature_pipeline_.fit_transform(X, direction_labels)\n",
        "        if isinstance(X_transformed, pd.DataFrame):\n",
        "            X_transformed.index = X.index\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            print(f\"特征工程后特征数量: {X_transformed.shape[1]}\")\n",
        "        \n",
        "        self.feature_selector_ = self._get_or_create_feature_selector()\n",
        "        if self.feature_selector_ is not None:\n",
        "            if self.verbose >= 1:\n",
        "                print(\"\\n[步骤2/3] 特征选择（使用 direction_loss_function）...\")\n",
        "            me_aligned = market_excess.loc[X_transformed.index] if isinstance(market_excess, pd.Series) else market_excess\n",
        "            rf_aligned = rf.loc[X_transformed.index] if isinstance(rf, pd.Series) else rf\n",
        "            X_transformed = self.feature_selector_.fit_transform(X_transformed, direction_labels, market_excess=me_aligned, rf=rf_aligned)\n",
        "            if self.verbose >= 1:\n",
        "                print(f\"特征选择后特征数量: {X_transformed.shape[1]}\")\n",
        "        else:\n",
        "            if self.verbose >= 1:\n",
        "                print(\"\\n[步骤2/3] 跳过特征选择（未配置）\")\n",
        "        \n",
        "        self.feature_names_ = X_transformed.columns.tolist() if isinstance(X_transformed, pd.DataFrame) else None\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            print(\"\\n[步骤3/3] 训练方向分类模型（Logloss）...\")\n",
        "        \n",
        "        from sklearn.model_selection import train_test_split\n",
        "        X_train, X_val, y_train, y_val = train_test_split(X_transformed, direction_labels, test_size=0.15, shuffle=False)\n",
        "        if sample_weight is not None:\n",
        "            sw_train = sample_weight[:len(X_train)] if not isinstance(sample_weight, pd.Series) else sample_weight.iloc[:len(X_train)]\n",
        "            self.model.fit(X_train, y_train, eval_set=(X_val, y_val), sample_weight=sw_train)\n",
        "        else:\n",
        "            self.model.fit(X_train, y_train, eval_set=(X_val, y_val))\n",
        "        \n",
        "        self.is_fitted_ = True\n",
        "        if self.verbose >= 1:\n",
        "            print(\"DirectionClassifierPipeline - 拟合完成\")\n",
        "        return self\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        if not self.is_fitted_:\n",
        "            raise ValueError(\"模型尚未拟合，请先调用 fit()\")\n",
        "        original_index = X.index if hasattr(X, 'index') else None\n",
        "        X_transformed = self.feature_pipeline_.transform(X)\n",
        "        if isinstance(X_transformed, pd.DataFrame):\n",
        "            X_transformed.index = X.index\n",
        "        if self.feature_selector_ is not None:\n",
        "            X_transformed = self.feature_selector_.transform(X_transformed)\n",
        "        proba = self.model.predict_proba(X_transformed)\n",
        "        return proba\n",
        "    \n",
        "    def predict(self, X):\n",
        "        proba = self.predict_proba(X)\n",
        "        direction = np.where(proba[:, 1] > 0.5, 1, -1)\n",
        "        original_index = X.index if hasattr(X, 'index') else None\n",
        "        if original_index is not None:\n",
        "            direction = pd.Series(direction, index=original_index, name='direction_prediction')\n",
        "        return direction\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. ParallelEnsemblePipeline - 并行集成模型\n",
        "# ============================================================\n",
        "\n",
        "class ParallelEnsemblePipeline(BaseEstimator, RegressorMixin):\n",
        "    \"\"\"\n",
        "    并行集成 Pipeline：封装机会模型和方向模型，生成最终仓位\n",
        "    最终仓位 = 方向(±1) × 机会([0, 2])\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, opportunity_pipeline=None, direction_pipeline=None, opportunity_model=None,\n",
        "                 direction_model=None, feature_creator=None, feature_selector_params=None, verbose=1):\n",
        "        self.feature_creator = feature_creator\n",
        "        self.feature_selector_params = feature_selector_params\n",
        "        self.opportunity_model = opportunity_model\n",
        "        self.direction_model = direction_model\n",
        "        self.verbose = verbose\n",
        "        \n",
        "        if opportunity_pipeline is None:\n",
        "            self.opportunity_pipeline = OpportunityPipeline(\n",
        "                feature_creator=feature_creator,\n",
        "                feature_selector_params=feature_selector_params,\n",
        "                model=opportunity_model,\n",
        "                verbose=verbose\n",
        "            )\n",
        "        else:\n",
        "            self.opportunity_pipeline = opportunity_pipeline\n",
        "        \n",
        "        if direction_pipeline is None:\n",
        "            self.direction_pipeline = DirectionClassifierPipeline(\n",
        "                feature_creator=feature_creator,\n",
        "                feature_selector_params=feature_selector_params,\n",
        "                model=direction_model,\n",
        "                verbose=verbose\n",
        "            )\n",
        "        else:\n",
        "            self.direction_pipeline = direction_pipeline\n",
        "        \n",
        "        self.is_fitted_ = False\n",
        "    \n",
        "    def fit(self, X, y, market_excess=None, rf=None, sample_weight=None):\n",
        "        if self.verbose >= 1:\n",
        "            print(\"=\"*70)\n",
        "            print(\"ParallelEnsemblePipeline - 开始拟合（并行双模型）\")\n",
        "            print(\"=\"*70)\n",
        "            print(f\"样本数: {len(X)}\")\n",
        "        \n",
        "        if market_excess is None:\n",
        "            market_excess = y\n",
        "        if rf is None:\n",
        "            rf = X['risk_free_rate_lag'].fillna(0) if 'risk_free_rate_lag' in X.columns else pd.Series(0, index=X.index)\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"[模型1/2] 训练机会模型（Regressor，MSE）\")\n",
        "            print(\"=\"*70)\n",
        "        self.opportunity_pipeline.fit(X, y, market_excess=market_excess, rf=rf, sample_weight=sample_weight)\n",
        "        \n",
        "        if self.verbose >= 1:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"[模型2/2] 训练方向模型（Classifier，Logloss）\")\n",
        "            print(\"=\"*70)\n",
        "        self.direction_pipeline.fit(X, y, market_excess=market_excess, rf=rf, sample_weight=sample_weight)\n",
        "        \n",
        "        self.is_fitted_ = True\n",
        "        if self.verbose >= 1:\n",
        "            print(\"\\n\" + \"=\"*70)\n",
        "            print(\"ParallelEnsemblePipeline - 拟合完成\")\n",
        "            print(\"=\"*70)\n",
        "        return self\n",
        "    \n",
        "    def predict(self, X, allow_short=False):\n",
        "        if not self.is_fitted_:\n",
        "            raise ValueError(\"模型尚未拟合，请先调用 fit()\")\n",
        "        original_index = X.index if hasattr(X, 'index') else None\n",
        "        opportunity = self.opportunity_pipeline.predict(X)\n",
        "        direction = self.direction_pipeline.predict(X)\n",
        "        if isinstance(opportunity, pd.Series) and isinstance(direction, pd.Series):\n",
        "            final_position = direction * opportunity\n",
        "        else:\n",
        "            final_position = np.array(direction) * np.array(opportunity)\n",
        "        if not allow_short:\n",
        "            if isinstance(final_position, pd.Series):\n",
        "                final_position = final_position.clip(lower=0)\n",
        "            else:\n",
        "                final_position = np.clip(final_position, 0, 2)\n",
        "        if original_index is not None and not isinstance(final_position, pd.Series):\n",
        "            final_position = pd.Series(final_position, index=original_index, name='final_position')\n",
        "        return final_position\n",
        "    \n",
        "    def predict_components(self, X):\n",
        "        if not self.is_fitted_:\n",
        "            raise ValueError(\"模型尚未拟合，请先调用 fit()\")\n",
        "        return {\n",
        "            'opportunity': self.opportunity_pipeline.predict(X),\n",
        "            'direction': self.direction_pipeline.predict(X),\n",
        "            'direction_proba': self.direction_pipeline.predict_proba(X)\n",
        "        }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. move_forward_two_way_test - 两段滑动窗口测试\n",
        "# ============================================================\n",
        "\n",
        "def move_forward_two_way_test(X, y, pipeline, train_size=3000, test_size=63, start_index=0,\n",
        "                               expanding_window=False, market_excess=None, rf=None, allow_short=False, verbose=1):\n",
        "    \"\"\"两段滑动窗口测试 - 适用于并行模型，返回详细组件预测\"\"\"\n",
        "    from sklearn.base import clone\n",
        "    n_samples = len(X)\n",
        "    train_start = start_index\n",
        "    train_end = start_index + train_size\n",
        "    test_end = train_end + test_size\n",
        "    \n",
        "    min_required = train_size + test_size + start_index\n",
        "    if min_required > n_samples:\n",
        "        raise ValueError(f\"数据量不足！需要至少 {min_required} 个样本，但只有 {n_samples} 个。\")\n",
        "    \n",
        "    # 主预测结果\n",
        "    oof_predictions = pd.Series(np.nan, index=y.index, dtype=float, name=\"prediction\")\n",
        "    # 组件预测结果\n",
        "    oof_direction = pd.Series(np.nan, index=y.index, dtype=float, name=\"direction\")\n",
        "    oof_opportunity = pd.Series(np.nan, index=y.index, dtype=float, name=\"opportunity\")\n",
        "    oof_direction_proba = pd.Series(np.nan, index=y.index, dtype=float, name=\"direction_proba\")\n",
        "    \n",
        "    round_stats = []\n",
        "    round_num = 0\n",
        "    \n",
        "    if verbose >= 1:\n",
        "        print(\"=\"*80)\n",
        "        print(\"两段滑动窗口测试 (Move Forward Two-Way Test)\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"总样本数: {n_samples}, 训练窗口: {train_size}, 测试窗口: {test_size}\")\n",
        "        print(f\"窗口模式: {'扩展窗口' if expanding_window else '滑动窗口'}\")\n",
        "        remaining_after_first = n_samples - (start_index + train_size + test_size)\n",
        "        estimated_rounds = 1 + max(0, remaining_after_first // test_size)\n",
        "        print(f\"预计轮数: ~{estimated_rounds}\")\n",
        "        print(\"=\"*80)\n",
        "    \n",
        "    while test_end <= n_samples:\n",
        "        round_num += 1\n",
        "        if verbose >= 1:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"第 {round_num} 轮\")\n",
        "            print(f\"{'='*80}\")\n",
        "            print(f\"  训练段: [{train_start}:{train_end}) = {train_end - train_start} 样本\")\n",
        "            print(f\"  测试段: [{train_end}:{test_end}) = {test_end - train_end} 样本\")\n",
        "        \n",
        "        X_train = X.iloc[train_start:train_end].copy()\n",
        "        y_train = y.iloc[train_start:train_end].copy()\n",
        "        X_test = X.iloc[train_end:test_end].copy()\n",
        "        y_test = y.iloc[train_end:test_end].copy()\n",
        "        \n",
        "        me_train = market_excess.iloc[train_start:train_end] if isinstance(market_excess, pd.Series) else None\n",
        "        rf_train = rf.iloc[train_start:train_end] if isinstance(rf, pd.Series) else None\n",
        "        \n",
        "        if verbose >= 1:\n",
        "            print(f\"\\n  [步骤1/2] 并行训练模型...\")\n",
        "        pipeline_clone = clone(pipeline)\n",
        "        pipeline_clone.fit(X_train, y_train, market_excess=me_train, rf=rf_train)\n",
        "        \n",
        "        if verbose >= 1:\n",
        "            print(f\"\\n  [步骤2/2] 在测试段上预测...\")\n",
        "        test_predictions = pipeline_clone.predict(X_test, allow_short=allow_short)\n",
        "        \n",
        "        # 获取组件预测\n",
        "        components = pipeline_clone.predict_components(X_test)\n",
        "        \n",
        "        if isinstance(test_predictions, pd.Series):\n",
        "            oof_predictions.loc[X_test.index] = test_predictions.values\n",
        "        else:\n",
        "            oof_predictions.loc[X_test.index] = test_predictions\n",
        "        \n",
        "        # 存储组件预测\n",
        "        direction_vals = components['direction'].values if isinstance(components['direction'], pd.Series) else components['direction']\n",
        "        opportunity_vals = components['opportunity'].values if isinstance(components['opportunity'], pd.Series) else components['opportunity']\n",
        "        direction_proba_vals = components['direction_proba'][:, 1] if components['direction_proba'].ndim > 1 else components['direction_proba']\n",
        "        \n",
        "        oof_direction.loc[X_test.index] = direction_vals\n",
        "        oof_opportunity.loc[X_test.index] = opportunity_vals\n",
        "        oof_direction_proba.loc[X_test.index] = direction_proba_vals\n",
        "        \n",
        "        round_stats.append({\n",
        "            'round': round_num, \n",
        "            'train_range': (train_start, train_end), \n",
        "            'test_range': (train_end, test_end),\n",
        "            'test_indices': X_test.index.tolist()\n",
        "        })\n",
        "        \n",
        "        if verbose >= 1:\n",
        "            pred_mean = test_predictions.mean() if hasattr(test_predictions, 'mean') else np.mean(test_predictions)\n",
        "            print(f\"\\n  预测均值: {pred_mean:.4f}, 真实均值: {y_test.mean():.4f}\")\n",
        "        \n",
        "        if expanding_window:\n",
        "            train_end += test_size\n",
        "        else:\n",
        "            train_start += test_size\n",
        "            train_end += test_size\n",
        "        test_end += test_size\n",
        "    \n",
        "    valid_predictions = oof_predictions.dropna()\n",
        "    if verbose >= 1:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"两段滑动窗口测试完成！\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"总轮数: {round_num}, 有效预测数: {len(valid_predictions)}\")\n",
        "        if len(valid_predictions) > 0:\n",
        "            y_test_all = y.loc[valid_predictions.index]\n",
        "            corr = np.corrcoef(valid_predictions.values, y_test_all.values)[0, 1]\n",
        "            print(f\"整体IC: {corr:.4f}\")\n",
        "            if market_excess is not None and rf is not None:\n",
        "                me_test = market_excess.loc[valid_predictions.index]\n",
        "                rf_test = rf.loc[valid_predictions.index]\n",
        "                sharpe = ad_sharpe_ratio_scorer(valid_predictions.values, me_test, rf_test)\n",
        "                print(f\"整体夏普率: {sharpe:.4f}\")\n",
        "        print(\"=\"*80)\n",
        "    \n",
        "    # 返回包含所有信息的结果字典\n",
        "    result = {\n",
        "        'predictions': oof_predictions,\n",
        "        'direction': oof_direction,\n",
        "        'opportunity': oof_opportunity,\n",
        "        'direction_proba': oof_direction_proba,\n",
        "        'round_stats': round_stats\n",
        "    }\n",
        "    return result\n",
        "\n",
        "\n",
        "print(\"✓ 已定义 OpportunityPipeline（机会模型，MSE，带特征选择）\")\n",
        "print(\"✓ 已定义 DirectionClassifierPipeline（方向分类模型，Logloss，带特征选择）\")\n",
        "print(\"✓ 已定义 ParallelEnsemblePipeline（并行集成模型）\")\n",
        "print(\"✓ 已定义 move_forward_two_way_test（两段滑动窗口测试）\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 已定义 ic_psr_csr_test\n",
            "✓ 已定义 ic_test_by_market_regime\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 5. IC PSR CSR 测试\n",
        "# ============================================================\n",
        "\n",
        "def ic_psr_csr_test(\n",
        "    y_pred, \n",
        "    y_true, \n",
        "    positions=None,\n",
        "    market_excess=None,\n",
        "    rf=None,\n",
        "    window_size=30,\n",
        "    n_trials=10\n",
        "):\n",
        "    \"\"\"\n",
        "    综合测试：IC (信息系数), PSR (概率夏普率), CSR (条件夏普率)\n",
        "    \n",
        "    参数:\n",
        "    y_pred: 预测值\n",
        "    y_true: 真实值\n",
        "    positions: 仓位序列（可选）\n",
        "    market_excess: 市场超额回报（可选）\n",
        "    rf: 无风险利率（可选）\n",
        "    window_size: 滚动窗口大小\n",
        "    n_trials: 用于DSR计算的试验次数\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"IC PSR CSR 综合测试\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    y_pred = np.asarray(y_pred)\n",
        "    y_true = np.asarray(y_true)\n",
        "    \n",
        "    # ============================================================\n",
        "    # 1. IC (信息系数) 分析\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"1. 信息系数 (IC) 分析\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # 整体IC\n",
        "    ic_pearson = np.corrcoef(y_pred, y_true)[0, 1]\n",
        "    ic_spearman = stats.spearmanr(y_pred, y_true)[0]\n",
        "    \n",
        "    # IC显著性检验\n",
        "    n = len(y_pred)\n",
        "    ic_t_stat = ic_pearson * np.sqrt(n - 2) / np.sqrt(1 - ic_pearson**2 + 1e-8)\n",
        "    ic_p_value = 2 * (1 - stats.t.cdf(abs(ic_t_stat), n - 2))\n",
        "    \n",
        "    print(f\"整体 IC (Pearson):  {ic_pearson:.4f}\")\n",
        "    print(f\"整体 IC (Spearman): {ic_spearman:.4f}\")\n",
        "    print(f\"IC t-statistic: {ic_t_stat:.4f}\")\n",
        "    print(f\"IC p-value: {ic_p_value:.6f}\")\n",
        "    \n",
        "    if ic_p_value < 0.05:\n",
        "        print(\"✓ IC 在 5% 水平上显著（模型具备预测能力）\")\n",
        "    else:\n",
        "        print(\"✗ IC 不显著\")\n",
        "    \n",
        "    # 滚动IC\n",
        "    rolling_ic = []\n",
        "    for i in range(window_size, len(y_pred)):\n",
        "        window_pred = y_pred[i-window_size:i]\n",
        "        window_true = y_true[i-window_size:i]\n",
        "        ic_window = np.corrcoef(window_pred, window_true)[0, 1]\n",
        "        rolling_ic.append(ic_window)\n",
        "    \n",
        "    rolling_ic = np.array(rolling_ic)\n",
        "    ic_mean = np.nanmean(rolling_ic)\n",
        "    ic_std = np.nanstd(rolling_ic)\n",
        "    ic_ir = ic_mean / (ic_std + 1e-8)  # IC比率\n",
        "    \n",
        "    print(f\"\\n--- 滚动IC分析（{window_size}天窗口）---\")\n",
        "    print(f\"滚动IC均值: {ic_mean:.4f}\")\n",
        "    print(f\"滚动IC标准差: {ic_std:.4f}\")\n",
        "    print(f\"滚动IC > 0 的比例: {(rolling_ic > 0).mean():.2%}\")\n",
        "    print(f\"IC比率 (IC_IR): {ic_ir:.4f}\")\n",
        "    if ic_ir > 0.5:\n",
        "        print(\"  ✓ IC稳定性良好（IC_IR > 0.5）\")\n",
        "    else:\n",
        "        print(\"  ⚠ IC稳定性一般\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # 2. PSR (概率夏普率) 分析\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"2. PSR (概率夏普率) 分析\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if positions is not None and market_excess is not None and rf is not None:\n",
        "        # 计算策略收益\n",
        "        pos = np.asarray(positions)\n",
        "        y_excess = np.asarray(market_excess)\n",
        "        rf_arr = np.asarray(rf)\n",
        "        \n",
        "        y_total = y_excess + rf_arr\n",
        "        strategy_returns = rf_arr * (1 - pos) + pos * y_total\n",
        "        strategy_excess_returns = strategy_returns - rf_arr\n",
        "        \n",
        "        # 计算夏普率\n",
        "        mean_ret = np.mean(strategy_excess_returns)\n",
        "        std_ret = np.std(strategy_excess_returns)\n",
        "        sharpe_ratio = mean_ret / (std_ret + 1e-8) * np.sqrt(252)\n",
        "        \n",
        "        # 计算PSR (Probability Sharpe Ratio)\n",
        "        # PSR = P(SR > 0)\n",
        "        n = len(strategy_excess_returns)\n",
        "        skewness = stats.skew(strategy_excess_returns)\n",
        "        excess_kurtosis = stats.kurtosis(strategy_excess_returns)\n",
        "        kurtosis = excess_kurtosis + 3\n",
        "        \n",
        "        # SR的标准差\n",
        "        var_sr = (1 + (kurtosis - 1) / 4 * sharpe_ratio**2 / 252 - skewness * sharpe_ratio / np.sqrt(252)) / (n - 1)\n",
        "        std_sr = np.sqrt(var_sr) if var_sr > 0 else 1e-8\n",
        "        \n",
        "        # PSR = P(SR > 0) = 1 - CDF(0)\n",
        "        z_score = (sharpe_ratio / np.sqrt(252) - 0) / std_sr\n",
        "        psr = 1 - stats.norm.cdf(-z_score)\n",
        "        \n",
        "        print(f\"传统夏普比率 (年化): {sharpe_ratio:.4f}\")\n",
        "        print(f\"PSR (P(SR > 0)): {psr:.4f} ({psr*100:.2f}%)\")\n",
        "        \n",
        "        if psr > 0.90:\n",
        "            print(\"  ✓ PSR > 0.90: 策略夏普比率显著大于0\")\n",
        "        elif psr > 0.75:\n",
        "            print(\"  ⚠ PSR > 0.75: 策略夏普比率可能大于0\")\n",
        "        else:\n",
        "            print(\"  ✗ PSR < 0.75: 策略夏普比率可能不显著\")\n",
        "        \n",
        "        # ============================================================\n",
        "        # 3. DSR (Deflated Sharpe Ratio) 分析\n",
        "        # ============================================================\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"3. DSR (Deflated Sharpe Ratio) 分析\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        # 计算预期最大SR\n",
        "        gamma = 0.5772156649  # 欧拉常数\n",
        "        if n_trials > 1:\n",
        "            z_n = stats.norm.ppf(1 - 1.0 / n_trials)\n",
        "            z_ne = stats.norm.ppf(1 - 1.0 / (n_trials * np.e))\n",
        "            expected_max_sr = std_sr * ((1 - gamma) * z_n + gamma * z_ne) * np.sqrt(252)\n",
        "        else:\n",
        "            expected_max_sr = 0\n",
        "        \n",
        "        # DSR概率\n",
        "        z_score_dsr = (sharpe_ratio - expected_max_sr) / (std_sr * np.sqrt(252))\n",
        "        dsr_prob = stats.norm.cdf(z_score_dsr)\n",
        "        dsr_pvalue = 1 - dsr_prob\n",
        "        \n",
        "        print(f\"设定尝试次数 (N): {n_trials} 次\")\n",
        "        print(f\"实际夏普比率 (年化): {sharpe_ratio:.4f}\")\n",
        "        print(f\"运气能产生的最大SR (年化): {expected_max_sr:.4f} (阈值)\")\n",
        "        print(f\"DSR 概率值: {dsr_prob:.4f} ({dsr_prob*100:.2f}%)\")\n",
        "        print(f\"DSR p-value: {dsr_pvalue:.6f}\")\n",
        "        \n",
        "        if dsr_prob > 0.95:\n",
        "            print(\"  ✅ 结果: 非常显著 (DSR > 95%)\")\n",
        "        elif dsr_prob > 0.90:\n",
        "            print(\"  ☑️ 结果: 显著 (DSR > 90%)\")\n",
        "        elif dsr_prob > 0.50:\n",
        "            print(\"  ⚠ 结果: 不显著 (DSR < 90%)\")\n",
        "        else:\n",
        "            print(\"  ❌ 结果: 失败 (DSR < 50%)\")\n",
        "    else:\n",
        "        print(\"⚠ 缺少仓位或市场数据，跳过PSR和DSR计算\")\n",
        "        sharpe_ratio = None\n",
        "        psr = None\n",
        "        dsr_prob = None\n",
        "        dsr_pvalue = None\n",
        "    \n",
        "    # ============================================================\n",
        "    # 4. 分层回测分析\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"4. 分层回测分析（Decile Analysis）\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # 将预测值分为10个分位\n",
        "    pred_quantiles = pd.qcut(y_pred, q=10, labels=False, duplicates='drop')\n",
        "    \n",
        "    decile_results = []\n",
        "    for decile in range(10):\n",
        "        mask = pred_quantiles == decile\n",
        "        if mask.sum() > 0:\n",
        "            decile_returns = y_true[mask]\n",
        "            mean_return = decile_returns.mean()\n",
        "            sharpe_decile = mean_return / (decile_returns.std() + 1e-8) * np.sqrt(252)\n",
        "            decile_results.append({\n",
        "                'Decile': decile + 1,\n",
        "                'Mean_Return': mean_return,\n",
        "                'Sharpe': sharpe_decile,\n",
        "                'N_Samples': mask.sum()\n",
        "            })\n",
        "    \n",
        "    decile_df = pd.DataFrame(decile_results)\n",
        "    print(decile_df.to_string(index=False))\n",
        "    \n",
        "    # 计算收益差和单调性\n",
        "    if len(decile_df) >= 2:\n",
        "        spread = decile_df['Mean_Return'].iloc[-1] - decile_df['Mean_Return'].iloc[0]\n",
        "        spread_annualized = spread * 252 * 100\n",
        "        monotonicity = stats.spearmanr(decile_df['Decile'], decile_df['Mean_Return'])[0]\n",
        "        \n",
        "        print(f\"\\n收益差 (最高分位 - 最低分位): {spread:.6f} ({spread_annualized:.2f}% 年化)\")\n",
        "        print(f\"单调性 (Spearman): {monotonicity:.4f}\")\n",
        "        \n",
        "        if abs(monotonicity) > 0.7:\n",
        "            print(\"  ✓ 预测信号单调性良好（|ρ| > 0.7）\")\n",
        "        else:\n",
        "            print(\"  ⚠ 预测信号单调性一般\")\n",
        "    \n",
        "    # 返回结果\n",
        "    results = {\n",
        "        'ic_pearson': ic_pearson,\n",
        "        'ic_spearman': ic_spearman,\n",
        "        'ic_p_value': ic_p_value,\n",
        "        'ic_ir': ic_ir,\n",
        "        'rolling_ic': rolling_ic,\n",
        "        'sharpe_ratio': sharpe_ratio,\n",
        "        'psr': psr,\n",
        "        'dsr_prob': dsr_prob,\n",
        "        'dsr_pvalue': dsr_pvalue,\n",
        "        'decile_df': decile_df,\n",
        "        'spread': spread if len(decile_df) >= 2 else None,\n",
        "        'monotonicity': monotonicity if len(decile_df) >= 2 else None\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "def ic_test_by_market_regime(\n",
        "    y_pred, \n",
        "    y_true, \n",
        "    market_returns,\n",
        "    threshold_bull=0.02,  # 牛市阈值（日收益率 > threshold_bull）\n",
        "    threshold_bear=-0.02,  # 熊市阈值（日收益率 < threshold_bear）\n",
        "    window_size=30\n",
        "):\n",
        "    \"\"\"\n",
        "    按市场状态（牛市/熊市/混合市场）分别测试 IC\n",
        "    \n",
        "    参数:\n",
        "    y_pred: 预测值\n",
        "    y_true: 真实值\n",
        "    market_returns: 市场收益率序列（用于判断市场状态）\n",
        "    threshold_bull: 牛市阈值（默认：日收益率 > 2%）\n",
        "    threshold_bear: 熊市阈值（默认：日收益率 < -2%）\n",
        "    window_size: 滚动窗口大小（用于计算滚动IC）\n",
        "    \n",
        "    返回:\n",
        "    results: 包含各市场状态IC测试结果的字典\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"按市场状态分组 IC 测试\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    y_pred = np.asarray(y_pred)\n",
        "    y_true = np.asarray(y_true)\n",
        "    market_returns = np.asarray(market_returns)\n",
        "    \n",
        "    # 确保长度一致\n",
        "    min_len = min(len(y_pred), len(y_true), len(market_returns))\n",
        "    y_pred = y_pred[:min_len]\n",
        "    y_true = y_true[:min_len]\n",
        "    market_returns = market_returns[:min_len]\n",
        "    \n",
        "    # 判断市场状态\n",
        "    bull_mask = market_returns > threshold_bull\n",
        "    bear_mask = market_returns < threshold_bear\n",
        "    mixed_mask = ~(bull_mask | bear_mask)\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    print(f\"\\n市场状态划分标准:\")\n",
        "    print(f\"  牛市: 市场收益率 > {threshold_bull:.2%}\")\n",
        "    print(f\"  熊市: 市场收益率 < {threshold_bear:.2%}\")\n",
        "    print(f\"  混合市场: {threshold_bear:.2%} <= 市场收益率 <= {threshold_bull:.2%}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # 1. 牛市 IC 测试\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"1. 牛市 IC 测试\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if bull_mask.sum() > 10:  # 至少需要10个样本\n",
        "        y_pred_bull = y_pred[bull_mask]\n",
        "        y_true_bull = y_true[bull_mask]\n",
        "        \n",
        "        # 整体IC\n",
        "        ic_pearson_bull = np.corrcoef(y_pred_bull, y_true_bull)[0, 1]\n",
        "        ic_spearman_bull = stats.spearmanr(y_pred_bull, y_true_bull)[0]\n",
        "        \n",
        "        # IC显著性检验\n",
        "        n_bull = len(y_pred_bull)\n",
        "        ic_t_stat_bull = ic_pearson_bull * np.sqrt(n_bull - 2) / np.sqrt(1 - ic_pearson_bull**2 + 1e-8)\n",
        "        ic_p_value_bull = 2 * (1 - stats.t.cdf(abs(ic_t_stat_bull), n_bull - 2))\n",
        "        \n",
        "        # 滚动IC\n",
        "        rolling_ic_bull = []\n",
        "        for i in range(window_size, len(y_pred_bull)):\n",
        "            window_pred = y_pred_bull[i-window_size:i]\n",
        "            window_true = y_true_bull[i-window_size:i]\n",
        "            ic_window = np.corrcoef(window_pred, window_true)[0, 1]\n",
        "            rolling_ic_bull.append(ic_window)\n",
        "        \n",
        "        rolling_ic_bull = np.array(rolling_ic_bull)\n",
        "        ic_mean_bull = np.nanmean(rolling_ic_bull) if len(rolling_ic_bull) > 0 else np.nan\n",
        "        ic_std_bull = np.nanstd(rolling_ic_bull) if len(rolling_ic_bull) > 0 else np.nan\n",
        "        ic_ir_bull = ic_mean_bull / (ic_std_bull + 1e-8) if not np.isnan(ic_std_bull) else np.nan\n",
        "        \n",
        "        print(f\"样本数: {n_bull}\")\n",
        "        print(f\"整体 IC (Pearson):  {ic_pearson_bull:.4f}\")\n",
        "        print(f\"整体 IC (Spearman): {ic_spearman_bull:.4f}\")\n",
        "        print(f\"IC t-statistic: {ic_t_stat_bull:.4f}\")\n",
        "        print(f\"IC p-value: {ic_p_value_bull:.6f}\")\n",
        "        if ic_p_value_bull < 0.05:\n",
        "            print(\"  ✓ IC 在 5% 水平上显著\")\n",
        "        else:\n",
        "            print(\"  ✗ IC 不显著\")\n",
        "        \n",
        "        if len(rolling_ic_bull) > 0:\n",
        "            print(f\"\\n滚动IC分析（{window_size}天窗口）:\")\n",
        "            print(f\"  滚动IC均值: {ic_mean_bull:.4f}\")\n",
        "            print(f\"  滚动IC标准差: {ic_std_bull:.4f}\")\n",
        "            print(f\"  滚动IC > 0 的比例: {(rolling_ic_bull > 0).mean():.2%}\")\n",
        "            print(f\"  IC比率 (IC_IR): {ic_ir_bull:.4f}\")\n",
        "        \n",
        "        results['bull'] = {\n",
        "            'ic_pearson': ic_pearson_bull,\n",
        "            'ic_spearman': ic_spearman_bull,\n",
        "            'ic_p_value': ic_p_value_bull,\n",
        "            'ic_ir': ic_ir_bull,\n",
        "            'rolling_ic': rolling_ic_bull,\n",
        "            'n_samples': n_bull,\n",
        "            'sample_ratio': n_bull / min_len\n",
        "        }\n",
        "    else:\n",
        "        print(f\"样本数不足（{bull_mask.sum()} 个），跳过牛市IC测试\")\n",
        "        results['bull'] = None\n",
        "    \n",
        "    # ============================================================\n",
        "    # 2. 熊市 IC 测试\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"2. 熊市 IC 测试\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if bear_mask.sum() > 10:  # 至少需要10个样本\n",
        "        y_pred_bear = y_pred[bear_mask]\n",
        "        y_true_bear = y_true[bear_mask]\n",
        "        \n",
        "        # 整体IC\n",
        "        ic_pearson_bear = np.corrcoef(y_pred_bear, y_true_bear)[0, 1]\n",
        "        ic_spearman_bear = stats.spearmanr(y_pred_bear, y_true_bear)[0]\n",
        "        \n",
        "        # IC显著性检验\n",
        "        n_bear = len(y_pred_bear)\n",
        "        ic_t_stat_bear = ic_pearson_bear * np.sqrt(n_bear - 2) / np.sqrt(1 - ic_pearson_bear**2 + 1e-8)\n",
        "        ic_p_value_bear = 2 * (1 - stats.t.cdf(abs(ic_t_stat_bear), n_bear - 2))\n",
        "        \n",
        "        # 滚动IC\n",
        "        rolling_ic_bear = []\n",
        "        for i in range(window_size, len(y_pred_bear)):\n",
        "            window_pred = y_pred_bear[i-window_size:i]\n",
        "            window_true = y_true_bear[i-window_size:i]\n",
        "            ic_window = np.corrcoef(window_pred, window_true)[0, 1]\n",
        "            rolling_ic_bear.append(ic_window)\n",
        "        \n",
        "        rolling_ic_bear = np.array(rolling_ic_bear)\n",
        "        ic_mean_bear = np.nanmean(rolling_ic_bear) if len(rolling_ic_bear) > 0 else np.nan\n",
        "        ic_std_bear = np.nanstd(rolling_ic_bear) if len(rolling_ic_bear) > 0 else np.nan\n",
        "        ic_ir_bear = ic_mean_bear / (ic_std_bear + 1e-8) if not np.isnan(ic_std_bear) else np.nan\n",
        "        \n",
        "        print(f\"样本数: {n_bear}\")\n",
        "        print(f\"整体 IC (Pearson):  {ic_pearson_bear:.4f}\")\n",
        "        print(f\"整体 IC (Spearman): {ic_spearman_bear:.4f}\")\n",
        "        print(f\"IC t-statistic: {ic_t_stat_bear:.4f}\")\n",
        "        print(f\"IC p-value: {ic_p_value_bear:.6f}\")\n",
        "        if ic_p_value_bear < 0.05:\n",
        "            print(\"  ✓ IC 在 5% 水平上显著\")\n",
        "        else:\n",
        "            print(\"  ✗ IC 不显著\")\n",
        "        \n",
        "        if len(rolling_ic_bear) > 0:\n",
        "            print(f\"\\n滚动IC分析（{window_size}天窗口）:\")\n",
        "            print(f\"  滚动IC均值: {ic_mean_bear:.4f}\")\n",
        "            print(f\"  滚动IC标准差: {ic_std_bear:.4f}\")\n",
        "            print(f\"  滚动IC > 0 的比例: {(rolling_ic_bear > 0).mean():.2%}\")\n",
        "            print(f\"  IC比率 (IC_IR): {ic_ir_bear:.4f}\")\n",
        "        \n",
        "        results['bear'] = {\n",
        "            'ic_pearson': ic_pearson_bear,\n",
        "            'ic_spearman': ic_spearman_bear,\n",
        "            'ic_p_value': ic_p_value_bear,\n",
        "            'ic_ir': ic_ir_bear,\n",
        "            'rolling_ic': rolling_ic_bear,\n",
        "            'n_samples': n_bear,\n",
        "            'sample_ratio': n_bear / min_len\n",
        "        }\n",
        "    else:\n",
        "        print(f\"样本数不足（{bear_mask.sum()} 个），跳过熊市IC测试\")\n",
        "        results['bear'] = None\n",
        "    \n",
        "    # ============================================================\n",
        "    # 3. 混合市场 IC 测试\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"3. 混合市场 IC 测试\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if mixed_mask.sum() > 10:  # 至少需要10个样本\n",
        "        y_pred_mixed = y_pred[mixed_mask]\n",
        "        y_true_mixed = y_true[mixed_mask]\n",
        "        \n",
        "        # 整体IC\n",
        "        ic_pearson_mixed = np.corrcoef(y_pred_mixed, y_true_mixed)[0, 1]\n",
        "        ic_spearman_mixed = stats.spearmanr(y_pred_mixed, y_true_mixed)[0]\n",
        "        \n",
        "        # IC显著性检验\n",
        "        n_mixed = len(y_pred_mixed)\n",
        "        ic_t_stat_mixed = ic_pearson_mixed * np.sqrt(n_mixed - 2) / np.sqrt(1 - ic_pearson_mixed**2 + 1e-8)\n",
        "        ic_p_value_mixed = 2 * (1 - stats.t.cdf(abs(ic_t_stat_mixed), n_mixed - 2))\n",
        "        \n",
        "        # 滚动IC\n",
        "        rolling_ic_mixed = []\n",
        "        for i in range(window_size, len(y_pred_mixed)):\n",
        "            window_pred = y_pred_mixed[i-window_size:i]\n",
        "            window_true = y_true_mixed[i-window_size:i]\n",
        "            ic_window = np.corrcoef(window_pred, window_true)[0, 1]\n",
        "            rolling_ic_mixed.append(ic_window)\n",
        "        \n",
        "        rolling_ic_mixed = np.array(rolling_ic_mixed)\n",
        "        ic_mean_mixed = np.nanmean(rolling_ic_mixed) if len(rolling_ic_mixed) > 0 else np.nan\n",
        "        ic_std_mixed = np.nanstd(rolling_ic_mixed) if len(rolling_ic_mixed) > 0 else np.nan\n",
        "        ic_ir_mixed = ic_mean_mixed / (ic_std_mixed + 1e-8) if not np.isnan(ic_std_mixed) else np.nan\n",
        "        \n",
        "        print(f\"样本数: {n_mixed}\")\n",
        "        print(f\"整体 IC (Pearson):  {ic_pearson_mixed:.4f}\")\n",
        "        print(f\"整体 IC (Spearman): {ic_spearman_mixed:.4f}\")\n",
        "        print(f\"IC t-statistic: {ic_t_stat_mixed:.4f}\")\n",
        "        print(f\"IC p-value: {ic_p_value_mixed:.6f}\")\n",
        "        if ic_p_value_mixed < 0.05:\n",
        "            print(\"  ✓ IC 在 5% 水平上显著\")\n",
        "        else:\n",
        "            print(\"  ✗ IC 不显著\")\n",
        "        \n",
        "        if len(rolling_ic_mixed) > 0:\n",
        "            print(f\"\\n滚动IC分析（{window_size}天窗口）:\")\n",
        "            print(f\"  滚动IC均值: {ic_mean_mixed:.4f}\")\n",
        "            print(f\"  滚动IC标准差: {ic_std_mixed:.4f}\")\n",
        "            print(f\"  滚动IC > 0 的比例: {(rolling_ic_mixed > 0).mean():.2%}\")\n",
        "            print(f\"  IC比率 (IC_IR): {ic_ir_mixed:.4f}\")\n",
        "        \n",
        "        results['mixed'] = {\n",
        "            'ic_pearson': ic_pearson_mixed,\n",
        "            'ic_spearman': ic_spearman_mixed,\n",
        "            'ic_p_value': ic_p_value_mixed,\n",
        "            'ic_ir': ic_ir_mixed,\n",
        "            'rolling_ic': rolling_ic_mixed,\n",
        "            'n_samples': n_mixed,\n",
        "            'sample_ratio': n_mixed / min_len\n",
        "        }\n",
        "    else:\n",
        "        print(f\"样本数不足（{mixed_mask.sum()} 个），跳过混合市场IC测试\")\n",
        "        results['mixed'] = None\n",
        "    \n",
        "    # ============================================================\n",
        "    # 4. 汇总对比\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"4. 市场状态 IC 对比汇总\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    summary_data = []\n",
        "    if results['bull'] is not None:\n",
        "        summary_data.append({\n",
        "            '市场状态': '牛市',\n",
        "            'IC (Pearson)': results['bull']['ic_pearson'],\n",
        "            'IC (Spearman)': results['bull']['ic_spearman'],\n",
        "            'IC_IR': results['bull']['ic_ir'],\n",
        "            '样本数': results['bull']['n_samples'],\n",
        "            '样本占比': f\"{results['bull']['sample_ratio']:.2%}\"\n",
        "        })\n",
        "    if results['bear'] is not None:\n",
        "        summary_data.append({\n",
        "            '市场状态': '熊市',\n",
        "            'IC (Pearson)': results['bear']['ic_pearson'],\n",
        "            'IC (Spearman)': results['bear']['ic_spearman'],\n",
        "            'IC_IR': results['bear']['ic_ir'],\n",
        "            '样本数': results['bear']['n_samples'],\n",
        "            '样本占比': f\"{results['bear']['sample_ratio']:.2%}\"\n",
        "        })\n",
        "    if results['mixed'] is not None:\n",
        "        summary_data.append({\n",
        "            '市场状态': '混合市场',\n",
        "            'IC (Pearson)': results['mixed']['ic_pearson'],\n",
        "            'IC (Spearman)': results['mixed']['ic_spearman'],\n",
        "            'IC_IR': results['mixed']['ic_ir'],\n",
        "            '样本数': results['mixed']['n_samples'],\n",
        "            '样本占比': f\"{results['mixed']['sample_ratio']:.2%}\"\n",
        "        })\n",
        "    \n",
        "    if len(summary_data) > 0:\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        print(summary_df.to_string(index=False))\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✓ 已定义 ic_psr_csr_test\")\n",
        "print(\"✓ 已定义 ic_test_by_market_regime\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 已定义修复后的 jittering_test（支持 EnsemblePipeline）\n",
            "⚠ 请用此函数替换 Cell 7 中的旧版本\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 修复后的 jittering_test 函数（支持 EnsemblePipeline）\n",
        "# ============================================================\n",
        "# 请用此函数替换 Cell 7 中的 jittering_test 函数\n",
        "\n",
        "def jittering_test(\n",
        "    test_pipeline, \n",
        "    X_test, \n",
        "    y_test,\n",
        "    N_TEST_SAMPLES=100,\n",
        "    NOISE_LEVELS=[0.01, 0.05, 0.10, 0.20],\n",
        "    N_JITTERS=50,\n",
        "    market_excess=None,\n",
        "    rf=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Jittering测试：评估模型对输入噪声的稳定性 - 支持 EnsemblePipeline\n",
        "    \n",
        "    参数:\n",
        "    test_pipeline: 训练好的 Pipeline 实例（EnsemblePipeline 或 FactorSelectionPipeline）\n",
        "    X_test: 测试特征\n",
        "    y_test: 测试目标\n",
        "    N_TEST_SAMPLES: 测试样本数量\n",
        "    NOISE_LEVELS: 噪声水平列表\n",
        "    N_JITTERS: 每个噪声水平的重复次数\n",
        "    market_excess: 市场超额回报（用于计算 sharpe ratio，可选）\n",
        "    rf: 无风险利率（用于计算 sharpe ratio，可选）\n",
        "    \"\"\"\n",
        "    if not test_pipeline.is_fitted_:\n",
        "        raise ValueError(\"Pipeline 尚未拟合，请先调用 fit()\")\n",
        "    \n",
        "    # 检测 pipeline 类型（支持新的 ParallelEnsemblePipeline）\n",
        "    is_ensemble_pipeline = isinstance(test_pipeline, ParallelEnsemblePipeline)\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"Jittering Test - 模型稳定性测试\")\n",
        "    if is_ensemble_pipeline:\n",
        "        print(\"Pipeline 类型: ParallelEnsemblePipeline (并行双模型)\")\n",
        "    else:\n",
        "        print(\"Pipeline 类型: 其他 Pipeline\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # 随机选择测试样本\n",
        "    test_indices = np.random.choice(len(X_test), min(N_TEST_SAMPLES, len(X_test)), replace=False)\n",
        "    X_test_sample = X_test.iloc[test_indices].copy()\n",
        "    y_test_sample = y_test.iloc[test_indices].copy()\n",
        "    \n",
        "    # 准备市场数据（用于计算 sharpe ratio）\n",
        "    if market_excess is None:\n",
        "        # 尝试从 X_test_sample 中获取\n",
        "        if 'market_forward_excess_returns' in X_test_sample.columns:\n",
        "            market_excess = X_test_sample['market_forward_excess_returns'].values\n",
        "        else:\n",
        "            market_excess = y_test_sample.values if hasattr(y_test_sample, 'values') else y_test_sample\n",
        "    else:\n",
        "        market_excess = market_excess[test_indices] if hasattr(market_excess, '__getitem__') else market_excess\n",
        "    \n",
        "    if rf is None:\n",
        "        # 尝试从 X_test_sample 中获取\n",
        "        if 'risk_free_rate_lag' in X_test_sample.columns:\n",
        "            rf = X_test_sample['risk_free_rate_lag'].values\n",
        "        elif 'risk_free_rate' in X_test_sample.columns:\n",
        "            rf = X_test_sample['risk_free_rate'].values\n",
        "        else:\n",
        "            rf = np.zeros(len(y_test_sample))\n",
        "    else:\n",
        "        rf = rf[test_indices] if hasattr(rf, '__getitem__') else rf\n",
        "    \n",
        "    # 原始预测（使用 pipeline 的 predict 方法，内部自动处理特征工程和因子选择）\n",
        "    y_pred_original = test_pipeline.predict(X_test_sample)\n",
        "    \n",
        "    # 计算原始 sharpe ratio\n",
        "    if is_ensemble_pipeline:\n",
        "        # EnsemblePipeline.predict() 已经返回最终仓位 [0, 2]，直接使用\n",
        "        positions_original = pd.Series(y_pred_original, index=X_test_sample.index) if not isinstance(y_pred_original, pd.Series) else y_pred_original\n",
        "    else:\n",
        "        # FactorSelectionPipeline.predict() 返回收益率预测，需要转换为仓位\n",
        "        forward_returns_lag = X_test_sample['forward_returns_lag'].values if 'forward_returns_lag' in X_test_sample.columns else np.zeros(len(y_test_sample))\n",
        "        positions_original = generate_hft_positions(\n",
        "            pd.Series(y_pred_original, index=X_test_sample.index),\n",
        "            pd.Series(forward_returns_lag, index=X_test_sample.index)\n",
        "        )\n",
        "    \n",
        "    sharpe_original = ad_sharpe_ratio_scorer(\n",
        "        positions_original.values,\n",
        "        market_excess,\n",
        "        rf\n",
        "    )\n",
        "    \n",
        "    print(f\"\\n测试样本数: {len(X_test_sample)}\")\n",
        "    print(f\"噪声水平: {NOISE_LEVELS}\")\n",
        "    print(f\"每个噪声水平的重复次数: {N_JITTERS}\")\n",
        "    print(f\"原始 Adjusted Sharpe Ratio: {sharpe_original:.4f}\")\n",
        "    \n",
        "    results_dict = {}\n",
        "    \n",
        "    # 对每个噪声水平进行测试\n",
        "    for noise_level in NOISE_LEVELS:\n",
        "        print(f\"\\n--- 噪声水平: {noise_level} ---\")\n",
        "        predictions_list = []\n",
        "        sharpe_ratios = []\n",
        "        \n",
        "        for jitter in range(N_JITTERS):\n",
        "            # 在原始特征上添加噪声（对数值特征）\n",
        "            X_test_jittered = X_test_sample.copy()\n",
        "            for col in X_test_jittered.columns:\n",
        "                if X_test_jittered[col].dtype in [np.float64, np.float32, np.int64, np.int32]:\n",
        "                    noise = np.random.normal(0, noise_level * X_test_jittered[col].std(), len(X_test_jittered))\n",
        "                    X_test_jittered[col] = X_test_jittered[col] + noise\n",
        "            \n",
        "            # 使用 pipeline 的 predict() 方法（自动处理特征工程、因子选择、预测）\n",
        "            y_pred_jittered = test_pipeline.predict(X_test_jittered)\n",
        "            \n",
        "            # 计算仓位和 sharpe ratio\n",
        "            if is_ensemble_pipeline:\n",
        "                # EnsemblePipeline.predict() 返回最终仓位 [0, 2]\n",
        "                positions_jittered = pd.Series(y_pred_jittered, index=X_test_sample.index) if not isinstance(y_pred_jittered, pd.Series) else y_pred_jittered\n",
        "            else:\n",
        "                # FactorSelectionPipeline.predict() 返回收益率预测，需要转换为仓位\n",
        "                forward_returns_lag = X_test_sample['forward_returns_lag'].values if 'forward_returns_lag' in X_test_sample.columns else np.zeros(len(y_test_sample))\n",
        "                positions_jittered = generate_hft_positions(\n",
        "                    pd.Series(y_pred_jittered, index=X_test_sample.index),\n",
        "                    pd.Series(forward_returns_lag, index=X_test_sample.index)\n",
        "                )\n",
        "            \n",
        "            # 保存预测值（用于统计）\n",
        "            predictions_list.append(y_pred_jittered)\n",
        "            \n",
        "            # 计算 sharpe ratio\n",
        "            sharpe_jittered = ad_sharpe_ratio_scorer(\n",
        "                positions_jittered.values,\n",
        "                market_excess,\n",
        "                rf\n",
        "            )\n",
        "            sharpe_ratios.append(sharpe_jittered)\n",
        "        \n",
        "        # 计算统计量\n",
        "        predictions_array = np.array(predictions_list)\n",
        "        pred_mean = predictions_array.mean(axis=0)\n",
        "        pred_std = predictions_array.std(axis=0)\n",
        "        pred_cv = pred_std / (np.abs(pred_mean) + 1e-8)  # 变异系数\n",
        "        \n",
        "        # 计算 sharpe ratio 统计量\n",
        "        sharpe_ratios = np.array(sharpe_ratios)\n",
        "        sharpe_mean = np.mean(sharpe_ratios)\n",
        "        sharpe_std = np.std(sharpe_ratios)\n",
        "        sharpe_min = np.min(sharpe_ratios)\n",
        "        sharpe_max = np.max(sharpe_ratios)\n",
        "        sharpe_range = sharpe_max - sharpe_min\n",
        "        sharpe_cv = sharpe_std / (np.abs(sharpe_mean) + 1e-8)  # sharpe ratio 变异系数\n",
        "        \n",
        "        results_dict[noise_level] = {\n",
        "            'predictions': predictions_array,\n",
        "            'mean': pred_mean,\n",
        "            'std': pred_std,\n",
        "            'cv': pred_cv,\n",
        "            'mean_std': pred_std.mean(),\n",
        "            'mean_cv': pred_cv.mean(),\n",
        "            'max_std': pred_std.max(),\n",
        "            'sharpe_ratios': sharpe_ratios,\n",
        "            'sharpe_mean': sharpe_mean,\n",
        "            'sharpe_std': sharpe_std,\n",
        "            'sharpe_min': sharpe_min,\n",
        "            'sharpe_max': sharpe_max,\n",
        "            'sharpe_range': sharpe_range,\n",
        "            'sharpe_cv': sharpe_cv\n",
        "        }\n",
        "        \n",
        "        print(f\"    平均预测标准差: {results_dict[noise_level]['mean_std']:.6f}\")\n",
        "        print(f\"    平均变异系数: {results_dict[noise_level]['mean_cv']:.4f}\")\n",
        "        print(f\"    最大预测标准差: {results_dict[noise_level]['max_std']:.6f}\")\n",
        "        print(f\"    Sharpe Ratio 均值: {sharpe_mean:.4f}\")\n",
        "        print(f\"    Sharpe Ratio 标准差: {sharpe_std:.4f}\")\n",
        "        print(f\"    Sharpe Ratio 范围: [{sharpe_min:.4f}, {sharpe_max:.4f}]\")\n",
        "        print(f\"    Sharpe Ratio 波动范围: {sharpe_range:.4f}\")\n",
        "        print(f\"    Sharpe Ratio 变异系数: {sharpe_cv:.4f}\")\n",
        "    \n",
        "    # 打印汇总表\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Jittering Test 汇总\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"{'噪声水平':>10}  {'平均标准差':>15}  {'平均CV':>15}  {'最大标准差':>15}\")\n",
        "    print(\"-\"*70)\n",
        "    for noise_level in NOISE_LEVELS:\n",
        "        pred_mean = results_dict[noise_level]['mean']\n",
        "        print(f\"{noise_level:>10.2f}  \"\n",
        "              f\"{results_dict[noise_level]['mean_std']:>15.6f}  \"\n",
        "              f\"{results_dict[noise_level]['mean_cv']:>15.4f}  \"\n",
        "              f\"{results_dict[noise_level]['max_std']:>15.6f}\")\n",
        "    \n",
        "    # 打印 Sharpe Ratio 汇总表\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Sharpe Ratio 波动分析\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"{'噪声水平':>10}  {'Sharpe均值':>15}  {'Sharpe标准差':>15}  {'Sharpe范围':>20}  {'Sharpe CV':>15}\")\n",
        "    print(\"-\"*90)\n",
        "    print(f\"{'原始':>10}  {sharpe_original:>15.4f}  {'-':>15}  {'-':>20}  {'-':>15}\")\n",
        "    for noise_level in NOISE_LEVELS:\n",
        "        sharpe_range_str = f\"[{results_dict[noise_level]['sharpe_min']:.4f}, {results_dict[noise_level]['sharpe_max']:.4f}]\"\n",
        "        print(f\"{noise_level:>10.2f}  \"\n",
        "              f\"{results_dict[noise_level]['sharpe_mean']:>15.4f}  \"\n",
        "              f\"{results_dict[noise_level]['sharpe_std']:>15.4f}  \"\n",
        "              f\"{sharpe_range_str:>20}  \"\n",
        "              f\"{results_dict[noise_level]['sharpe_cv']:>15.4f}\")\n",
        "    \n",
        "    # 可视化\n",
        "    fig, axes = plt.subplots(3, 2, figsize=(14, 15))\n",
        "    \n",
        "    # 子图1: 预测值分布（箱线图）\n",
        "    ax1 = axes[0, 0]\n",
        "    box_data = []\n",
        "    labels = []\n",
        "    for noise_level in NOISE_LEVELS:\n",
        "        sample_predictions = results_dict[noise_level]['predictions'][:, :10].flatten()\n",
        "        box_data.append(sample_predictions)\n",
        "        labels.append(f'{noise_level}')\n",
        "    ax1.boxplot(box_data, labels=labels)\n",
        "    ax1.set_xlabel('噪声水平')\n",
        "    ax1.set_ylabel('预测值')\n",
        "    ax1.set_title('不同噪声水平下的预测值分布')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图2: 标准差 vs 噪声水平\n",
        "    ax2 = axes[0, 1]\n",
        "    mean_stds = [results_dict[nl]['mean_std'] for nl in NOISE_LEVELS]\n",
        "    max_stds = [results_dict[nl]['max_std'] for nl in NOISE_LEVELS]\n",
        "    ax2.plot(NOISE_LEVELS, mean_stds, 'o-', label='平均标准差', linewidth=2)\n",
        "    ax2.plot(NOISE_LEVELS, max_stds, 's-', label='最大标准差', linewidth=2)\n",
        "    ax2.set_xlabel('噪声水平')\n",
        "    ax2.set_ylabel('预测标准差')\n",
        "    ax2.set_title('预测稳定性 vs 噪声水平')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图3: 变异系数分布\n",
        "    ax3 = axes[1, 0]\n",
        "    for noise_level in NOISE_LEVELS:\n",
        "        cv_values = results_dict[noise_level]['cv']\n",
        "        ax3.hist(cv_values, bins=20, alpha=0.5, label=f'{noise_level}', density=True)\n",
        "    ax3.set_xlabel('变异系数')\n",
        "    ax3.set_ylabel('密度')\n",
        "    ax3.set_title('变异系数分布')\n",
        "    ax3.legend()\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图4: 预测值对比（原始 vs 加噪声）\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.scatter(y_pred_original[:20], results_dict[NOISE_LEVELS[0]]['mean'][:20], \n",
        "               alpha=0.6, label=f'噪声={NOISE_LEVELS[0]}')\n",
        "    ax4.scatter(y_pred_original[:20], results_dict[NOISE_LEVELS[-1]]['mean'][:20], \n",
        "               alpha=0.6, label=f'噪声={NOISE_LEVELS[-1]}')\n",
        "    min_val = min(y_pred_original.min(), min([results_dict[nl]['mean'].min() for nl in NOISE_LEVELS]))\n",
        "    max_val = max(y_pred_original.max(), max([results_dict[nl]['mean'].max() for nl in NOISE_LEVELS]))\n",
        "    ax4.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='y=x')\n",
        "    ax4.set_xlabel('原始预测值')\n",
        "    ax4.set_ylabel('加噪声后预测值')\n",
        "    ax4.set_title('预测值稳定性')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图5: Sharpe Ratio 分布（箱线图）\n",
        "    ax5 = axes[2, 0]\n",
        "    sharpe_box_data = []\n",
        "    sharpe_labels = []\n",
        "    # 添加原始 sharpe ratio\n",
        "    sharpe_box_data.append([sharpe_original])\n",
        "    sharpe_labels.append('原始')\n",
        "    for noise_level in NOISE_LEVELS:\n",
        "        sharpe_box_data.append(results_dict[noise_level]['sharpe_ratios'])\n",
        "        sharpe_labels.append(f'{noise_level}')\n",
        "    ax5.boxplot(sharpe_box_data, labels=sharpe_labels)\n",
        "    ax5.set_xlabel('噪声水平')\n",
        "    ax5.set_ylabel('Adjusted Sharpe Ratio')\n",
        "    ax5.set_title('不同噪声水平下的 Sharpe Ratio 分布')\n",
        "    ax5.axhline(y=sharpe_original, color='r', linestyle='--', linewidth=2, label='原始 Sharpe')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图6: Sharpe Ratio 波动 vs 噪声水平\n",
        "    ax6 = axes[2, 1]\n",
        "    sharpe_means = [sharpe_original] + [results_dict[nl]['sharpe_mean'] for nl in NOISE_LEVELS]\n",
        "    sharpe_stds = [0] + [results_dict[nl]['sharpe_std'] for nl in NOISE_LEVELS]\n",
        "    sharpe_mins = [sharpe_original] + [results_dict[nl]['sharpe_min'] for nl in NOISE_LEVELS]\n",
        "    sharpe_maxs = [sharpe_original] + [results_dict[nl]['sharpe_max'] for nl in NOISE_LEVELS]\n",
        "    noise_levels_plot = [0] + list(NOISE_LEVELS)\n",
        "    \n",
        "    ax6.plot(noise_levels_plot, sharpe_means, 'o-', label='Sharpe 均值', linewidth=2, markersize=8)\n",
        "    ax6.fill_between(noise_levels_plot, \n",
        "                     [m - s for m, s in zip(sharpe_means, sharpe_stds)],\n",
        "                     [m + s for m, s in zip(sharpe_means, sharpe_stds)],\n",
        "                     alpha=0.3, label='±1 标准差')\n",
        "    ax6.plot(noise_levels_plot, sharpe_mins, 's--', label='Sharpe 最小值', linewidth=1.5, markersize=6)\n",
        "    ax6.plot(noise_levels_plot, sharpe_maxs, '^--', label='Sharpe 最大值', linewidth=1.5, markersize=6)\n",
        "    ax6.set_xlabel('噪声水平')\n",
        "    ax6.set_ylabel('Adjusted Sharpe Ratio')\n",
        "    ax6.set_title('Sharpe Ratio 波动 vs 噪声水平')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('jittering_test.png', dpi=150, bbox_inches='tight')\n",
        "    print(f\"\\n图表已保存到: jittering_test.png\")\n",
        "    plt.close(fig)  # 关闭图形，避免在 Jupyter notebook 中重复显示\n",
        "    \n",
        "    # 在返回结果中添加原始 sharpe ratio\n",
        "    results_dict['_original_sharpe'] = sharpe_original\n",
        "    \n",
        "    return results_dict\n",
        "\n",
        "print(\"✓ 已定义修复后的 jittering_test（支持 EnsemblePipeline）\")\n",
        "print(\"⚠ 请用此函数替换 Cell 7 中的旧版本\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "重要更新说明\n",
            "======================================================================\n",
            "\n",
            "已完成的修改：\n",
            "\n",
            "1. Meta模型标签生成逻辑（MetaLabelPipeline.fit()）:\n",
            "   ✓ 现在使用HFT生成的理想仓位来判断方向正确性\n",
            "   ✓ 方向模型的理想仓位 vs 真实收益率的理想仓位\n",
            "   ✓ 1代表同号（方向正确），0代表异号（方向错误）\n",
            "   ✓ 逻辑与方向模型一致\n",
            "\n",
            "2. 最终仓位聚合函数（generate_ensemble_positions()）:\n",
            "   ✓ 直接使用 meta * 方向\n",
            "   ✓ 不再使用 generate_hft_positions 函数\n",
            "   ✓ 因为方向模型的预测已经是仓位scale，不需要再次转换\n",
            "\n",
            "需要手动更新的地方：\n",
            "- EnsemblePipeline.predict() 方法（约第2911-2921行）\n",
            "  将以下代码：\n",
            "  \n",
            "  if 'forward_returns_lag' not in X.columns:\n",
            "      raise ValueError(\"X 必须包含 'forward_returns_lag' 列\")\n",
            "  forward_returns_lag = X['forward_returns_lag']\n",
            "  final_positions = generate_ensemble_positions(\n",
            "      direction_predictions,\n",
            "      confidence_predictions,\n",
            "      forward_returns_lag,\n",
            "      allow_short=allow_short\n",
            "  )\n",
            "  \n",
            "  改为：\n",
            "  \n",
            "  final_positions = generate_ensemble_positions(\n",
            "      direction_predictions,\n",
            "      confidence_predictions,\n",
            "      forward_returns_lag=None,  # 不再需要\n",
            "      allow_short=allow_short\n",
            "  )\n",
            "\n",
            "\n",
            "✓ 更新说明已显示\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 重要更新：Meta模型标签生成和最终仓位聚合逻辑\n",
        "# ============================================================\n",
        "# \n",
        "# 已完成的修改：\n",
        "# 1. Meta模型的标签生成：现在使用HFT生成的理想仓位来判断方向正确性\n",
        "#    - 方向模型的理想仓位 vs 真实收益率的理想仓位\n",
        "#    - 1代表同号（方向正确），0代表异号（方向错误）\n",
        "# \n",
        "# 2. 最终仓位聚合：直接使用 meta * 方向\n",
        "#    - 不再使用 generate_hft_positions 函数\n",
        "#    - 因为方向模型的预测已经是仓位scale，不需要再次转换\n",
        "#\n",
        "# 需要手动更新的地方：\n",
        "# - EnsemblePipeline.predict() 方法中的 generate_ensemble_positions 调用\n",
        "#   将 forward_returns_lag 参数改为 None（或移除该参数）\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"重要更新说明\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "已完成的修改：\n",
        "\n",
        "1. Meta模型标签生成逻辑（MetaLabelPipeline.fit()）:\n",
        "   ✓ 现在使用HFT生成的理想仓位来判断方向正确性\n",
        "   ✓ 方向模型的理想仓位 vs 真实收益率的理想仓位\n",
        "   ✓ 1代表同号（方向正确），0代表异号（方向错误）\n",
        "   ✓ 逻辑与方向模型一致\n",
        "\n",
        "2. 最终仓位聚合函数（generate_ensemble_positions()）:\n",
        "   ✓ 直接使用 meta * 方向\n",
        "   ✓ 不再使用 generate_hft_positions 函数\n",
        "   ✓ 因为方向模型的预测已经是仓位scale，不需要再次转换\n",
        "\n",
        "需要手动更新的地方：\n",
        "- EnsemblePipeline.predict() 方法（约第2911-2921行）\n",
        "  将以下代码：\n",
        "  \n",
        "  if 'forward_returns_lag' not in X.columns:\n",
        "      raise ValueError(\"X 必须包含 'forward_returns_lag' 列\")\n",
        "  forward_returns_lag = X['forward_returns_lag']\n",
        "  final_positions = generate_ensemble_positions(\n",
        "      direction_predictions,\n",
        "      confidence_predictions,\n",
        "      forward_returns_lag,\n",
        "      allow_short=allow_short\n",
        "  )\n",
        "  \n",
        "  改为：\n",
        "  \n",
        "  final_positions = generate_ensemble_positions(\n",
        "      direction_predictions,\n",
        "      confidence_predictions,\n",
        "      forward_returns_lag=None,  # 不再需要\n",
        "      allow_short=allow_short\n",
        "  )\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\n✓ 更新说明已显示\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 已定义 plot_fitting_results\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 6. 拟合图（可视化结果）\n",
        "# ============================================================\n",
        "\n",
        "def plot_fitting_results(\n",
        "    results_df,\n",
        "    oof_predictions,\n",
        "    ic_results=None,\n",
        "    save_path='fitting_results.png'\n",
        "):\n",
        "    \"\"\"\n",
        "    生成综合拟合结果图\n",
        "    \n",
        "    参数:\n",
        "    results_df: 包含预测和真实值的DataFrame\n",
        "    oof_predictions: OOF预测序列\n",
        "    ic_results: IC测试结果（可选）\n",
        "    save_path: 保存路径\n",
        "    \"\"\"\n",
        "    # 设置中文字体\n",
        "    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'SimHei', 'DejaVu Sans']  # 支持中文\n",
        "    plt.rcParams['axes.unicode_minus'] = False  # 解决负号显示问题\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"生成拟合结果图\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # 准备数据：确保索引对齐\n",
        "    # 找到 oof_predictions 和 results_df 的交集索引\n",
        "    common_indices = oof_predictions.index.intersection(results_df.index)\n",
        "    oof_predictions_aligned = oof_predictions.loc[common_indices]\n",
        "    results_df_aligned = results_df.loc[common_indices]\n",
        "    \n",
        "    # 创建有效掩码（非NaN的预测值）\n",
        "    valid_mask = ~oof_predictions_aligned.isna()\n",
        "    \n",
        "    # 获取有效数据\n",
        "    valid_indices = oof_predictions_aligned.index[valid_mask]\n",
        "    y_pred = oof_predictions_aligned[valid_mask].values\n",
        "    y_true = results_df_aligned.loc[valid_indices, 'market_forward_excess_returns'].values\n",
        "    \n",
        "    # 创建图形\n",
        "    fig = plt.figure(figsize=(18, 12))\n",
        "    \n",
        "    # 子图1: 预测值 vs 实际值散点图\n",
        "    ax1 = plt.subplot(3, 3, 1)\n",
        "    ax1.scatter(y_pred, y_true, alpha=0.5, s=10)\n",
        "    # 添加回归线\n",
        "    z = np.polyfit(y_pred, y_true, 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax1.plot(y_pred, p(y_pred), \"r--\", alpha=0.8, linewidth=2, \n",
        "            label=f'Regression Line (IC={np.corrcoef(y_pred, y_true)[0,1]:.4f})')\n",
        "    ax1.set_xlabel('Predicted')\n",
        "    ax1.set_ylabel('Actual Returns')\n",
        "    ax1.set_title('Predicted vs Actual Returns')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图2: 时间序列对比\n",
        "    ax2 = plt.subplot(3, 3, 2)\n",
        "    indices = valid_indices  # 使用对齐后的有效索引\n",
        "    ax2.plot(range(len(y_pred)), y_pred, alpha=0.6, label='Predicted', linewidth=1)\n",
        "    ax2.plot(range(len(y_true)), y_true, alpha=0.6, label='Actual', linewidth=1)\n",
        "    ax2.set_xlabel('Time Index')\n",
        "    ax2.set_ylabel('Returns')\n",
        "    ax2.set_title('Predicted vs Actual Time Series')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图3: 滚动IC时间序列\n",
        "    ax3 = plt.subplot(3, 3, 3)\n",
        "    if ic_results is not None and 'rolling_ic' in ic_results:\n",
        "        rolling_ic = ic_results['rolling_ic']\n",
        "        ax3.plot(range(len(rolling_ic)), rolling_ic, alpha=0.7, linewidth=1)\n",
        "        ax3.axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "        ax3.axhline(y=np.nanmean(rolling_ic), color='g', linestyle='--', linewidth=1, \n",
        "                   label=f'Mean: {np.nanmean(rolling_ic):.4f}')\n",
        "        ax3.set_xlabel('Time Point')\n",
        "        ax3.set_ylabel('Rolling IC (30 days)')\n",
        "        ax3.set_title('Rolling IC Time Series')\n",
        "        ax3.legend()\n",
        "        ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图4: 分层收益\n",
        "    ax4 = plt.subplot(3, 3, 4)\n",
        "    if ic_results is not None and 'decile_df' in ic_results:\n",
        "        decile_df = ic_results['decile_df']\n",
        "        ax4.bar(decile_df['Decile'], decile_df['Mean_Return'] * 252 * 100, \n",
        "               color='steelblue', alpha=0.7)\n",
        "        ax4.axhline(y=0, color='r', linestyle='--', linewidth=1)\n",
        "        ax4.set_xlabel('Prediction Quantile')\n",
        "        ax4.set_ylabel('Annualized Avg Returns (%)')\n",
        "        ax4.set_title('Decile Analysis - Avg Returns by Quantile')\n",
        "        ax4.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # 子图5: 预测值分布\n",
        "    ax5 = plt.subplot(3, 3, 5)\n",
        "    ax5.hist(y_pred, bins=50, alpha=0.6, color='steelblue', label='Predicted', density=True)\n",
        "    ax5.axvline(y_pred.mean(), color='r', linestyle='--', linewidth=2, \n",
        "               label=f'Mean: {y_pred.mean():.4f}')\n",
        "    ax5.set_xlabel('Predicted')\n",
        "    ax5.set_ylabel('Density')\n",
        "    ax5.set_title('Predicted Distribution')\n",
        "    ax5.legend()\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图6: 实际收益分布\n",
        "    ax6 = plt.subplot(3, 3, 6)\n",
        "    ax6.hist(y_true, bins=50, alpha=0.6, color='coral', label='Actual Returns', density=True)\n",
        "    ax6.axvline(y_true.mean(), color='r', linestyle='--', linewidth=2, \n",
        "               label=f'Mean: {y_true.mean():.4f}')\n",
        "    ax6.set_xlabel('Actual Returns')\n",
        "    ax6.set_ylabel('Density')\n",
        "    ax6.set_title('Actual Returns Distribution')\n",
        "    ax6.legend()\n",
        "    ax6.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图7: 残差分析\n",
        "    ax7 = plt.subplot(3, 3, 7)\n",
        "    residuals = y_true - y_pred\n",
        "    ax7.scatter(y_pred, residuals, alpha=0.5, s=10)\n",
        "    ax7.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "    ax7.set_xlabel('Predicted')\n",
        "    ax7.set_ylabel('Residuals (Actual - Predicted)')\n",
        "    ax7.set_title('Residual Analysis')\n",
        "    ax7.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图8: 累积收益对比（如果有仓位数据）\n",
        "    ax8 = plt.subplot(3, 3, 8)\n",
        "    if 'positions' in results_df_aligned.columns:\n",
        "        # 计算策略累积收益\n",
        "        pos = results_df_aligned.loc[valid_indices, 'positions'].values\n",
        "        rf = results_df_aligned.loc[valid_indices, 'risk_free_rate'].values if 'risk_free_rate' in results_df_aligned.columns else np.zeros(len(pos))\n",
        "        \n",
        "        # 使用 market_forward_excess_returns 或 forward_returns\n",
        "        if 'forward_returns' in results_df_aligned.columns:\n",
        "            forward_returns = results_df_aligned.loc[valid_indices, 'forward_returns'].values\n",
        "        else:\n",
        "            forward_returns = y_true + rf  # 如果没有 forward_returns，从超额收益重建\n",
        "        \n",
        "        strategy_returns = rf * (1 - pos) + pos * forward_returns\n",
        "        benchmark_returns = forward_returns\n",
        "        \n",
        "        strategy_cumulative = (1 + strategy_returns).cumprod()\n",
        "        benchmark_cumulative = (1 + benchmark_returns).cumprod()\n",
        "        \n",
        "        ax8.plot(range(len(strategy_cumulative)), strategy_cumulative, label='Strategy', linewidth=2)\n",
        "        ax8.plot(range(len(benchmark_cumulative)), benchmark_cumulative, label='Benchmark', linewidth=2, linestyle='--')\n",
        "        ax8.set_xlabel('Time Index')\n",
        "        ax8.set_ylabel('Cumulative Returns')\n",
        "        ax8.set_title('Strategy vs Benchmark Cumulative Returns')\n",
        "        ax8.legend()\n",
        "        ax8.grid(True, alpha=0.3)\n",
        "    \n",
        "    # 子图9: IC分布\n",
        "    ax9 = plt.subplot(3, 3, 9)\n",
        "    if ic_results is not None and 'rolling_ic' in ic_results:\n",
        "        rolling_ic = ic_results['rolling_ic']\n",
        "        ax9.hist(rolling_ic[~np.isnan(rolling_ic)], bins=30, alpha=0.6, color='green', density=True)\n",
        "        ax9.axvline(np.nanmean(rolling_ic), color='r', linestyle='--', linewidth=2, \n",
        "                   label=f'Mean: {np.nanmean(rolling_ic):.4f}')\n",
        "        ax9.axvline(0, color='k', linestyle='-', linewidth=1)\n",
        "        ax9.set_xlabel('IC Value')\n",
        "        ax9.set_ylabel('Density')\n",
        "        ax9.set_title('Rolling IC Distribution')\n",
        "        ax9.legend()\n",
        "        ax9.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"\\n图表已保存到: {save_path}\")\n",
        "    \n",
        "    # 关闭图形，避免在 Jupyter notebook 中重复显示\n",
        "    plt.close(fig)\n",
        "    \n",
        "    return fig\n",
        "\n",
        "print(\"✓ 已定义 plot_fitting_results\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 已定义数据流诊断工具: diagnose_ic_data_flow\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 数据流诊断工具 - 用于检查IC计算前的数据质量\n",
        "# ============================================================\n",
        "\n",
        "def diagnose_ic_data_flow(y_pred, y_true, market_returns=None, name=\"数据\"):\n",
        "    \"\"\"\n",
        "    完整诊断数据流，用于IC计算前的检查\n",
        "    \n",
        "    参数:\n",
        "    y_pred: 预测值\n",
        "    y_true: 真实值\n",
        "    market_returns: 市场收益率（可选，用于市场状态分组）\n",
        "    name: 数据名称\n",
        "    \n",
        "    返回:\n",
        "    diagnosis: 诊断结果字典\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(f\"数据流诊断: {name}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    y_pred = np.asarray(y_pred)\n",
        "    y_true = np.asarray(y_true)\n",
        "    \n",
        "    diagnosis = {\n",
        "        'is_valid': True,\n",
        "        'issues': [],\n",
        "        'stats': {}\n",
        "    }\n",
        "    \n",
        "    # 1. 检查长度\n",
        "    print(f\"\\n1. 长度检查:\")\n",
        "    print(f\"   y_pred长度: {len(y_pred)}\")\n",
        "    print(f\"   y_true长度: {len(y_true)}\")\n",
        "    \n",
        "    if len(y_pred) != len(y_true):\n",
        "        diagnosis['is_valid'] = False\n",
        "        diagnosis['issues'].append(f\"长度不一致: y_pred={len(y_pred)}, y_true={len(y_true)}\")\n",
        "        print(f\"   ✗ 错误: 长度不一致！\")\n",
        "    else:\n",
        "        print(f\"   ✓ 长度一致\")\n",
        "    \n",
        "    if len(y_pred) < 2:\n",
        "        diagnosis['is_valid'] = False\n",
        "        diagnosis['issues'].append(f\"数据长度不足: 需要至少2个样本，实际只有{len(y_pred)}个\")\n",
        "        print(f\"   ✗ 错误: 数据长度不足（需要至少2个样本）\")\n",
        "    else:\n",
        "        print(f\"   ✓ 长度足够\")\n",
        "    \n",
        "    # 2. 检查NaN\n",
        "    print(f\"\\n2. NaN检查:\")\n",
        "    nan_pred = np.isnan(y_pred).sum()\n",
        "    nan_true = np.isnan(y_true).sum()\n",
        "    print(f\"   y_pred NaN数量: {nan_pred}\")\n",
        "    print(f\"   y_true NaN数量: {nan_true}\")\n",
        "    \n",
        "    if nan_pred > 0:\n",
        "        nan_indices_pred = np.where(np.isnan(y_pred))[0][:10]\n",
        "        diagnosis['is_valid'] = False\n",
        "        diagnosis['issues'].append(f\"y_pred包含{nan_pred}个NaN值（位置示例: {nan_indices_pred.tolist()}）\")\n",
        "        print(f\"   ✗ y_pred包含NaN值！位置示例: {nan_indices_pred.tolist()}\")\n",
        "    else:\n",
        "        print(f\"   ✓ y_pred无NaN\")\n",
        "    \n",
        "    if nan_true > 0:\n",
        "        nan_indices_true = np.where(np.isnan(y_true))[0][:10]\n",
        "        diagnosis['is_valid'] = False\n",
        "        diagnosis['issues'].append(f\"y_true包含{nan_true}个NaN值（位置示例: {nan_indices_true.tolist()}）\")\n",
        "        print(f\"   ✗ y_true包含NaN值！位置示例: {nan_indices_true.tolist()}\")\n",
        "    else:\n",
        "        print(f\"   ✓ y_true无NaN\")\n",
        "    \n",
        "    # 3. 检查Inf\n",
        "    print(f\"\\n3. Inf检查:\")\n",
        "    inf_pred = np.isinf(y_pred).sum()\n",
        "    inf_true = np.isinf(y_true).sum()\n",
        "    print(f\"   y_pred Inf数量: {inf_pred}\")\n",
        "    print(f\"   y_true Inf数量: {inf_true}\")\n",
        "    \n",
        "    if inf_pred > 0 or inf_true > 0:\n",
        "        diagnosis['is_valid'] = False\n",
        "        diagnosis['issues'].append(f\"数据包含Inf值: y_pred={inf_pred}, y_true={inf_true}\")\n",
        "        print(f\"   ✗ 数据包含Inf值！\")\n",
        "    else:\n",
        "        print(f\"   ✓ 无Inf值\")\n",
        "    \n",
        "    # 4. 检查方差\n",
        "    print(f\"\\n4. 方差检查:\")\n",
        "    var_pred = np.var(y_pred)\n",
        "    var_true = np.var(y_true)\n",
        "    print(f\"   y_pred方差: {var_pred:.10f}\")\n",
        "    print(f\"   y_true方差: {var_true:.10f}\")\n",
        "    \n",
        "    if var_pred == 0:\n",
        "        diagnosis['is_valid'] = False\n",
        "        diagnosis['issues'].append(f\"y_pred方差为0（所有值相同: {y_pred[0] if len(y_pred) > 0 else 'N/A'}）\")\n",
        "        print(f\"   ✗ y_pred方差为0（所有值相同: {y_pred[0] if len(y_pred) > 0 else 'N/A'}）\")\n",
        "    else:\n",
        "        print(f\"   ✓ y_pred有方差\")\n",
        "    \n",
        "    if var_true == 0:\n",
        "        diagnosis['is_valid'] = False\n",
        "        diagnosis['issues'].append(f\"y_true方差为0（所有值相同: {y_true[0] if len(y_true) > 0 else 'N/A'}）\")\n",
        "        print(f\"   ✗ y_true方差为0（所有值相同: {y_true[0] if len(y_true) > 0 else 'N/A'}）\")\n",
        "    else:\n",
        "        print(f\"   ✓ y_true有方差\")\n",
        "    \n",
        "    # 5. 统计信息\n",
        "    print(f\"\\n5. 统计信息:\")\n",
        "    stats_pred = {\n",
        "        'min': np.min(y_pred),\n",
        "        'max': np.max(y_pred),\n",
        "        'mean': np.mean(y_pred),\n",
        "        'std': np.std(y_pred),\n",
        "        'median': np.median(y_pred)\n",
        "    }\n",
        "    stats_true = {\n",
        "        'min': np.min(y_true),\n",
        "        'max': np.max(y_true),\n",
        "        'mean': np.mean(y_true),\n",
        "        'std': np.std(y_true),\n",
        "        'median': np.median(y_true)\n",
        "    }\n",
        "    diagnosis['stats'] = {'y_pred': stats_pred, 'y_true': stats_true}\n",
        "    \n",
        "    print(f\"   y_pred: min={stats_pred['min']:.6f}, max={stats_pred['max']:.6f}, mean={stats_pred['mean']:.6f}, std={stats_pred['std']:.6f}, median={stats_pred['median']:.6f}\")\n",
        "    print(f\"   y_true: min={stats_true['min']:.6f}, max={stats_true['max']:.6f}, mean={stats_true['mean']:.6f}, std={stats_true['std']:.6f}, median={stats_true['median']:.6f}\")\n",
        "    \n",
        "    # 6. 检查市场收益率（如果提供）\n",
        "    if market_returns is not None:\n",
        "        print(f\"\\n6. 市场收益率检查:\")\n",
        "        market_returns = np.asarray(market_returns)\n",
        "        nan_market = np.isnan(market_returns).sum()\n",
        "        print(f\"   市场收益率长度: {len(market_returns)}\")\n",
        "        print(f\"   NaN数量: {nan_market}\")\n",
        "        \n",
        "        if nan_market > 0:\n",
        "            diagnosis['issues'].append(f\"市场收益率包含{nan_market}个NaN值\")\n",
        "            print(f\"   ⚠ 市场收益率包含NaN值\")\n",
        "        else:\n",
        "            print(f\"   ✓ 市场收益率无NaN\")\n",
        "        \n",
        "        # 检查市场状态分组\n",
        "        if len(market_returns) == len(y_pred):\n",
        "            threshold_bull = 0.02\n",
        "            threshold_bear = -0.02\n",
        "            bull_mask = market_returns > threshold_bull\n",
        "            bear_mask = market_returns < threshold_bear\n",
        "            mixed_mask = ~(bull_mask | bear_mask)\n",
        "            \n",
        "            print(f\"   市场状态分布:\")\n",
        "            print(f\"     牛市 (> {threshold_bull:.2%}): {bull_mask.sum()} 个样本\")\n",
        "            print(f\"     熊市 (< {threshold_bear:.2%}): {bear_mask.sum()} 个样本\")\n",
        "            print(f\"     混合市场: {mixed_mask.sum()} 个样本\")\n",
        "            \n",
        "            # 检查每个市场状态的数据质量\n",
        "            if bull_mask.sum() > 0:\n",
        "                y_pred_bull = y_pred[bull_mask]\n",
        "                y_true_bull = y_true[bull_mask]\n",
        "                nan_bull_pred = np.isnan(y_pred_bull).sum()\n",
        "                nan_bull_true = np.isnan(y_true_bull).sum()\n",
        "                var_bull_pred = np.var(y_pred_bull)\n",
        "                var_bull_true = np.var(y_true_bull)\n",
        "                print(f\"     牛市数据质量: y_pred NaN={nan_bull_pred}, y_true NaN={nan_bull_true}, y_pred方差={var_bull_pred:.6f}, y_true方差={var_bull_true:.6f}\")\n",
        "                if nan_bull_pred > 0 or nan_bull_true > 0 or var_bull_pred == 0 or var_bull_true == 0:\n",
        "                    diagnosis['issues'].append(f\"牛市数据有问题: NaN或方差为0\")\n",
        "            \n",
        "            if bear_mask.sum() > 0:\n",
        "                y_pred_bear = y_pred[bear_mask]\n",
        "                y_true_bear = y_true[bear_mask]\n",
        "                nan_bear_pred = np.isnan(y_pred_bear).sum()\n",
        "                nan_bear_true = np.isnan(y_true_bear).sum()\n",
        "                var_bear_pred = np.var(y_pred_bear)\n",
        "                var_bear_true = np.var(y_true_bear)\n",
        "                print(f\"     熊市数据质量: y_pred NaN={nan_bear_pred}, y_true NaN={nan_bear_true}, y_pred方差={var_bear_pred:.6f}, y_true方差={var_bear_true:.6f}\")\n",
        "                if nan_bear_pred > 0 or nan_bear_true > 0 or var_bear_pred == 0 or var_bear_true == 0:\n",
        "                    diagnosis['issues'].append(f\"熊市数据有问题: NaN或方差为0\")\n",
        "    \n",
        "    # 总结\n",
        "    print(f\"\\n\" + \"=\"*70)\n",
        "    if diagnosis['is_valid'] and len(diagnosis['issues']) == 0:\n",
        "        print(f\"✓ 数据质量检查通过，可以计算IC\")\n",
        "    else:\n",
        "        print(f\"✗ 数据质量检查失败，发现问题:\")\n",
        "        for issue in diagnosis['issues']:\n",
        "            print(f\"  - {issue}\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    return diagnosis\n",
        "\n",
        "print(\"✓ 已定义数据流诊断工具: diagnose_ic_data_flow\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ 已定义改进版本的 ic_test_by_market_regime_improved（包含完整数据验证）\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 改进版本的 ic_test_by_market_regime - 包含完整数据验证\n",
        "# ============================================================\n",
        "\n",
        "def ic_test_by_market_regime_improved(\n",
        "    y_pred, \n",
        "    y_true, \n",
        "    market_returns,\n",
        "    threshold_bull=0.02,\n",
        "    threshold_bear=-0.02,\n",
        "    window_size=30\n",
        "):\n",
        "    \"\"\"\n",
        "    改进版本：按市场状态（牛市/熊市/混合市场）分别测试 IC\n",
        "    包含完整的数据验证和诊断\n",
        "    \n",
        "    参数:\n",
        "    y_pred: 预测值\n",
        "    y_true: 真实值\n",
        "    market_returns: 市场收益率序列（用于判断市场状态）\n",
        "    threshold_bull: 牛市阈值（默认：日收益率 > 2%）\n",
        "    threshold_bear: 熊市阈值（默认：日收益率 < -2%）\n",
        "    window_size: 滚动窗口大小（用于计算滚动IC）\n",
        "    \n",
        "    返回:\n",
        "    results: 包含各市场状态IC测试结果的字典\n",
        "    \"\"\"\n",
        "    print(\"=\"*70)\n",
        "    print(\"按市场状态分组 IC 测试（改进版 - 含数据验证）\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    y_pred = np.asarray(y_pred)\n",
        "    y_true = np.asarray(y_true)\n",
        "    market_returns = np.asarray(market_returns)\n",
        "    \n",
        "    # 确保长度一致\n",
        "    min_len = min(len(y_pred), len(y_true), len(market_returns))\n",
        "    y_pred = y_pred[:min_len]\n",
        "    y_true = y_true[:min_len]\n",
        "    market_returns = market_returns[:min_len]\n",
        "    \n",
        "    # 先进行整体数据诊断\n",
        "    diagnosis = diagnose_ic_data_flow(y_pred, y_true, market_returns, name=\"整体数据\")\n",
        "    \n",
        "    if not diagnosis['is_valid']:\n",
        "        print(\"\\n⚠ 警告: 整体数据存在问题，但继续尝试按市场状态分组计算...\")\n",
        "    \n",
        "    # 判断市场状态\n",
        "    bull_mask = market_returns > threshold_bull\n",
        "    bear_mask = market_returns < threshold_bear\n",
        "    mixed_mask = ~(bull_mask | bear_mask)\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    print(f\"\\n市场状态划分标准:\")\n",
        "    print(f\"  牛市: 市场收益率 > {threshold_bull:.2%}\")\n",
        "    print(f\"  熊市: 市场收益率 < {threshold_bear:.2%}\")\n",
        "    print(f\"  混合市场: {threshold_bear:.2%} <= 市场收益率 <= {threshold_bull:.2%}\")\n",
        "    \n",
        "    # 辅助函数：安全计算IC\n",
        "    def safe_calculate_ic(y_pred_subset, y_true_subset, name=\"\"):\n",
        "        \"\"\"安全计算IC，包含数据验证\"\"\"\n",
        "        # 检查NaN\n",
        "        nan_pred = np.isnan(y_pred_subset).sum()\n",
        "        nan_true = np.isnan(y_true_subset).sum()\n",
        "        if nan_pred > 0 or nan_true > 0:\n",
        "            print(f\"  ✗ {name}数据包含NaN: y_pred={nan_pred}, y_true={nan_true}\")\n",
        "            return np.nan, np.nan\n",
        "        \n",
        "        # 检查方差\n",
        "        var_pred = np.var(y_pred_subset)\n",
        "        var_true = np.var(y_true_subset)\n",
        "        if var_pred == 0:\n",
        "            print(f\"  ✗ {name}y_pred方差为0（所有值相同: {y_pred_subset[0] if len(y_pred_subset) > 0 else 'N/A'}）\")\n",
        "            return np.nan, np.nan\n",
        "        if var_true == 0:\n",
        "            print(f\"  ✗ {name}y_true方差为0（所有值相同: {y_true_subset[0] if len(y_true_subset) > 0 else 'N/A'}）\")\n",
        "            return np.nan, np.nan\n",
        "        \n",
        "        # 计算IC\n",
        "        try:\n",
        "            ic_pearson = np.corrcoef(y_pred_subset, y_true_subset)[0, 1]\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ 计算Pearson IC时出错: {e}\")\n",
        "            ic_pearson = np.nan\n",
        "        \n",
        "        try:\n",
        "            ic_spearman_result = stats.spearmanr(y_pred_subset, y_true_subset)\n",
        "            ic_spearman = ic_spearman_result[0] if hasattr(ic_spearman_result, '__len__') else ic_spearman_result\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ 计算Spearman IC时出错: {e}\")\n",
        "            ic_spearman = np.nan\n",
        "        \n",
        "        return ic_pearson, ic_spearman\n",
        "    \n",
        "    # ============================================================\n",
        "    # 1. 牛市 IC 测试\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"1. 牛市 IC 测试\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if bull_mask.sum() > 10:\n",
        "        y_pred_bull = y_pred[bull_mask]\n",
        "        y_true_bull = y_true[bull_mask]\n",
        "        \n",
        "        print(f\"样本数: {len(y_pred_bull)}\")\n",
        "        ic_pearson_bull, ic_spearman_bull = safe_calculate_ic(y_pred_bull, y_true_bull, \"牛市\")\n",
        "        \n",
        "        # IC显著性检验\n",
        "        n_bull = len(y_pred_bull)\n",
        "        if not np.isnan(ic_pearson_bull):\n",
        "            ic_t_stat_bull = ic_pearson_bull * np.sqrt(n_bull - 2) / np.sqrt(1 - ic_pearson_bull**2 + 1e-8)\n",
        "            ic_p_value_bull = 2 * (1 - stats.t.cdf(abs(ic_t_stat_bull), n_bull - 2))\n",
        "        else:\n",
        "            ic_t_stat_bull = np.nan\n",
        "            ic_p_value_bull = np.nan\n",
        "        \n",
        "        # 滚动IC\n",
        "        rolling_ic_bull = []\n",
        "        for i in range(window_size, len(y_pred_bull)):\n",
        "            window_pred = y_pred_bull[i-window_size:i]\n",
        "            window_true = y_true_bull[i-window_size:i]\n",
        "            ic_window, _ = safe_calculate_ic(window_pred, window_true, \"\")\n",
        "            rolling_ic_bull.append(ic_window)\n",
        "        \n",
        "        rolling_ic_bull = np.array(rolling_ic_bull)\n",
        "        ic_mean_bull = np.nanmean(rolling_ic_bull) if len(rolling_ic_bull) > 0 else np.nan\n",
        "        ic_std_bull = np.nanstd(rolling_ic_bull) if len(rolling_ic_bull) > 0 else np.nan\n",
        "        ic_ir_bull = ic_mean_bull / (ic_std_bull + 1e-8) if not np.isnan(ic_std_bull) else np.nan\n",
        "        \n",
        "        print(f\"整体 IC (Pearson):  {ic_pearson_bull:.4f}\")\n",
        "        print(f\"整体 IC (Spearman): {ic_spearman_bull:.4f}\")\n",
        "        print(f\"IC t-statistic: {ic_t_stat_bull:.4f}\")\n",
        "        print(f\"IC p-value: {ic_p_value_bull:.6f}\")\n",
        "        if not np.isnan(ic_p_value_bull) and ic_p_value_bull < 0.05:\n",
        "            print(\"  ✓ IC 在 5% 水平上显著\")\n",
        "        else:\n",
        "            print(\"  ✗ IC 不显著\")\n",
        "        \n",
        "        if len(rolling_ic_bull) > 0:\n",
        "            print(f\"\\n滚动IC分析（{window_size}天窗口）:\")\n",
        "            print(f\"  滚动IC均值: {ic_mean_bull:.4f}\")\n",
        "            print(f\"  滚动IC标准差: {ic_std_bull:.4f}\")\n",
        "            print(f\"  滚动IC > 0 的比例: {(rolling_ic_bull > 0).mean():.2%}\")\n",
        "            print(f\"  IC比率 (IC_IR): {ic_ir_bull:.4f}\")\n",
        "        \n",
        "        results['bull'] = {\n",
        "            'ic_pearson': ic_pearson_bull,\n",
        "            'ic_spearman': ic_spearman_bull,\n",
        "            'ic_p_value': ic_p_value_bull,\n",
        "            'ic_ir': ic_ir_bull,\n",
        "            'rolling_ic': rolling_ic_bull,\n",
        "            'n_samples': n_bull,\n",
        "            'sample_ratio': n_bull / min_len\n",
        "        }\n",
        "    else:\n",
        "        print(f\"样本数不足（{bull_mask.sum()} 个），跳过牛市IC测试\")\n",
        "        results['bull'] = None\n",
        "    \n",
        "    # ============================================================\n",
        "    # 2. 熊市 IC 测试\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"2. 熊市 IC 测试\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    if bear_mask.sum() > 10:\n",
        "        y_pred_bear = y_pred[bear_mask]\n",
        "        y_true_bear = y_true[bear_mask]\n",
        "        \n",
        "        print(f\"样本数: {len(y_pred_bear)}\")\n",
        "        ic_pearson_bear, ic_spearman_bear = safe_calculate_ic(y_pred_bear, y_true_bear, \"熊市\")\n",
        "        \n",
        "        # IC显著性检验\n",
        "        n_bear = len(y_pred_bear)\n",
        "        if not np.isnan(ic_pearson_bear):\n",
        "            ic_t_stat_bear = ic_pearson_bear * np.sqrt(n_bear - 2) / np.sqrt(1 - ic_pearson_bear**2 + 1e-8)\n",
        "            ic_p_value_bear = 2 * (1 - stats.t.cdf(abs(ic_t_stat_bear), n_bear - 2))\n",
        "        else:\n",
        "            ic_t_stat_bear = np.nan\n",
        "            ic_p_value_bear = np.nan\n",
        "        \n",
        "        # 滚动IC\n",
        "        rolling_ic_bear = []\n",
        "        for i in range(window_size, len(y_pred_bear)):\n",
        "            window_pred = y_pred_bear[i-window_size:i]\n",
        "            window_true = y_true_bear[i-window_size:i]\n",
        "            ic_window, _ = safe_calculate_ic(window_pred, window_true, \"\")\n",
        "            rolling_ic_bear.append(ic_window)\n",
        "        \n",
        "        rolling_ic_bear = np.array(rolling_ic_bear)\n",
        "        ic_mean_bear = np.nanmean(rolling_ic_bear) if len(rolling_ic_bear) > 0 else np.nan\n",
        "        ic_std_bear = np.nanstd(rolling_ic_bear) if len(rolling_ic_bear) > 0 else np.nan\n",
        "        ic_ir_bear = ic_mean_bear / (ic_std_bear + 1e-8) if not np.isnan(ic_std_bear) else np.nan\n",
        "        \n",
        "        print(f\"整体 IC (Pearson):  {ic_pearson_bear:.4f}\")\n",
        "        print(f\"整体 IC (Spearman): {ic_spearman_bear:.4f}\")\n",
        "        print(f\"IC t-statistic: {ic_t_stat_bear:.4f}\")\n",
        "        print(f\"IC p-value: {ic_p_value_bear:.6f}\")\n",
        "        if not np.isnan(ic_p_value_bear) and ic_p_value_bear < 0.05:\n",
        "            print(\"  ✓ IC 在 5% 水平上显著\")\n",
        "        else:\n",
        "            print(\"  ✗ IC 不显著\")\n",
        "        \n",
        "        if len(rolling_ic_bear) > 0:\n",
        "            print(f\"\\n滚动IC分析（{window_size}天窗口）:\")\n",
        "            print(f\"  滚动IC均值: {ic_mean_bear:.4f}\")\n",
        "            print(f\"  滚动IC标准差: {ic_std_bear:.4f}\")\n",
        "            print(f\"  滚动IC > 0 的比例: {(rolling_ic_bear > 0).mean():.2%}\")\n",
        "            print(f\"  IC比率 (IC_IR): {ic_ir_bear:.4f}\")\n",
        "        \n",
        "        results['bear'] = {\n",
        "            'ic_pearson': ic_pearson_bear,\n",
        "            'ic_spearman': ic_spearman_bear,\n",
        "            'ic_p_value': ic_p_value_bear,\n",
        "            'ic_ir': ic_ir_bear,\n",
        "            'rolling_ic': rolling_ic_bear,\n",
        "            'n_samples': n_bear,\n",
        "            'sample_ratio': n_bear / min_len\n",
        "        }\n",
        "    else:\n",
        "        print(f\"样本数不足（{bear_mask.sum()} 个），跳过熊市IC测试\")\n",
        "        results['bear'] = None\n",
        "    \n",
        "    # 返回结果\n",
        "    return results\n",
        "\n",
        "print(\"✓ 已定义改进版本的 ic_test_by_market_regime_improved（包含完整数据验证）\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# feature_creator\n",
        "# ============================================================\n",
        "def create_drop_features_classic(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    global log\n",
        "    \n",
        "    # 原始的特征列表 (保持不变)\n",
        "    I_cols = ['M4_roll_mean_5', 'M4', 'V13', 'M4_lag_1', 'S12', 'forward_returns_lag_roll_mean_5', 'S5_lag_1', \n",
        "              'S5_roll_mean_20', 'forward_returns_lag', 'I2', 'S2', 'P7', 'P5', 'M17', 'S2_lag_1',  'M2', 'S5', \n",
        "              'V7', 'S5_roll_mean_60', 'S2_roll_std_60', 'M17_lag_1', 'V12', 'M17_lag_5', 'M11', 'forward_returns_lag_lag_1', \n",
        "              'M8', 'M17_lag_20', 'S2_roll_mean_5', 'S2_roll_mean_20', 'E19_roll_std_5', 'E11', 'P10', 'V8', 'forward_returns_lag_roll_mean_60', \n",
        "              'M17_roll_mean_20', 'M12', 'E11_roll_std_20', 'E11_lag_20', 'S5_roll_mean_5',\"feat_I2_poshinge_x_M17low\"]\n",
        "    \n",
        "    MP_cols = [\"feat_M4_minus_P7\", \"feat_M17_minus_P5\"]\n",
        "    SV_cols = []\n",
        "    \n",
        "    df_out = df.copy()\n",
        "    \n",
        "    # 基础特征生成配置\n",
        "    TOP_FEATURES= ['forward_returns_lag', 'M4','M17','S5','S2','E19','E11']\n",
        "    LAG_PERIODS = [1, 5, 20]\n",
        "    ROLLING_WINDOWS = [5, 20, 60]\n",
        "    COLS_TO_DROP =  ['date_id', 'risk_free_rate_lag']\n",
        "    epsilon = 1e-6 # 防止除以零\n",
        "    \n",
        "    # --- 1. 批量生成 Lag 和 Rolling 特征 ---\n",
        "    features_dict = {}\n",
        "    for col in TOP_FEATURES:\n",
        "        if col in df_out.columns:\n",
        "            for lag in LAG_PERIODS:\n",
        "                features_dict[f'{col}_lag_{lag}'] = df_out[col].shift(lag)\n",
        "            for window in ROLLING_WINDOWS:\n",
        "                features_dict[f'{col}_roll_mean_{window}'] = df_out[col].rolling(window=window, min_periods=1).mean()\n",
        "                features_dict[f'{col}_roll_std_{window}'] = df_out[col].rolling(window=window, min_periods=1).std()\n",
        "                \n",
        "    df_out = pd.concat([df_out, pd.DataFrame(features_dict)], axis=1)\n",
        "    \n",
        "    # --- 2. 原始交互特征计算 (保持不变) ---\n",
        "    if 'I2' in df_out.columns and 'M17' in df_out.columns:\n",
        "        m17_thr = df_out[\"M17\"].median()\n",
        "        df_out[\"feat_I2_poshinge_x_M17low\"] = np.maximum(df_out[\"I2\"], 0.0) * (df_out[\"M17\"] < m17_thr).astype(float)\n",
        "\n",
        "    if 'S12' in df_out.columns and 'V7' in df_out.columns:\n",
        "        df_out[\"feat_S12_minu_V7\"] = df_out['S12']  - df_out['V7'] \n",
        "    \n",
        "    if 'M4' in df_out.columns and 'P7' in df_out.columns:\n",
        "        df_out[\"feat_M4_minus_P7\"] = df_out[\"M4\"] - df_out[\"P7\"]\n",
        "        \n",
        "    if 'M17' in df_out.columns and 'P5' in df_out.columns:\n",
        "        df_out[\"feat_M17_minus_P5\"] = df_out[\"M17\"] - df_out[\"P5\"]\n",
        "        \n",
        "    if 'M4' in df_out.columns and 'E19' in df_out.columns:\n",
        "        df_out['feat_M4_x_E19'] = df_out['M4'] * df_out['E19']\n",
        "\n",
        "    if 'P7' in df_out.columns and 'V7' in df_out.columns:\n",
        "        df_out['feat_P7_x_V7'] = df_out['P7'] * df_out['V7']\n",
        "\n",
        "    if 'S5' in df_out.columns and 'E11' in df_out.columns:\n",
        "        df_out['feat_S5_x_E11'] = df_out['S5'] * df_out['E11']\n",
        "        \n",
        "    if 'feat_M4_minus_P7' in df_out.columns and 'E19' in df_out.columns:\n",
        "        df_out['feat_M4_P7_x_E19'] = df_out['feat_M4_minus_P7'] * df_out['E19']\n",
        "\n",
        "    # ==============================================================================\n",
        "    # 【新增核心逻辑】 熊市/环境感知特征 (Regime Awareness)\n",
        "    # ==============================================================================\n",
        "    \n",
        "    # 【特征A】宏观趋势分 (Red/Green Light)\n",
        "    # 假设 S5 代表某种价格/指数 (如果不准确，请换成你的 Close 价格特征)\n",
        "    # 逻辑：当前价格减去60日均线。 >0 为牛市/震荡，<0 为熊市\n",
        "    if 'S5' in df_out.columns and 'S5_roll_mean_60' in df_out.columns:\n",
        "        df_out['feat_Macro_Trend'] = df_out['S5'] - df_out['S5_roll_mean_60']\n",
        "    else:\n",
        "        # 如果没有 S5，尝试用 M4 (假设是动量特征替代)\n",
        "        if 'M4' in df_out.columns:\n",
        "             df_out['feat_Macro_Trend'] = df_out['M4'] \n",
        "        else:\n",
        "             df_out['feat_Macro_Trend'] = 0 # 兜底\n",
        "    \n",
        "    # 【特征B】高波动崩盘风险 (Volatility Regime)\n",
        "    # 逻辑：当波动率 (S2_roll_std_60) 很高时，往往是熊市底部或大跌过程\n",
        "    if 'S2_roll_std_60' in df_out.columns:\n",
        "        df_out['feat_Vol_Regime'] = df_out['S2_roll_std_60']\n",
        "    \n",
        "    # 【特征C】动量 x 趋势 交互项 (Momentum Interaction)\n",
        "    # 目的：告诉模型，同样的 Lag Return，在 Trend>0 时是好事，在 Trend<0 时可能是补跌信号\n",
        "    # 熊市中 IC 为负的罪魁祸首通常是 forward_returns_lag，这里给它加上环境约束\n",
        "    if 'forward_returns_lag' in df_out.columns and 'feat_Macro_Trend' in df_out.columns:\n",
        "        df_out['feat_Mom_x_Trend'] = df_out['forward_returns_lag'] * df_out['feat_Macro_Trend']\n",
        "\n",
        "    # 定义要加入输出列表的新特征\n",
        "    NEW_REGIME_COLS = ['feat_Macro_Trend', 'feat_Vol_Regime', 'feat_Mom_x_Trend']\n",
        "    \n",
        "    # ==============================================================================\n",
        "    \n",
        "    # 删除废弃列\n",
        "    df_out.drop(columns = [col for col in COLS_TO_DROP if col in df_out.columns], inplace = True)\n",
        "    \n",
        "    if (log):\n",
        "        pprint(f\"Feature X (Top 10): {df_out.columns.tolist()[:10]} ... and {len(df_out.columns)-10} more\")\n",
        "        print(f\"Feature Creator Output Shape: {df_out.shape}\")\n",
        "        # 打印一下新特征是否成功加入\n",
        "        print(f\"New Regime Features Included: {[c for c in NEW_REGIME_COLS if c in df_out.columns]}\")\n",
        "        log = False\n",
        "        \n",
        "    return df_out\n",
        "\n",
        "f_creator_classic = FunctionTransformer(create_drop_features_classic, validate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在对 96 个特征进行平稳性检验 (ADF Test)...\n",
            "无法计算 M16: Invalid input, x is constant\n",
            "无法计算 V11: Invalid input, x is constant\n",
            "无法计算 V12: Invalid input, x is constant\n",
            "无法计算 V8: Invalid input, x is constant\n",
            "\n",
            "需要做 d=0.4 的特征 (45个):\n",
            "['E1', 'E10', 'E11', 'E12', 'E15', 'E17', 'E18', 'E2', 'E20', 'E3', 'E5', 'E6', 'E8', 'E9', 'I1', 'I2', 'I3', 'I4', 'I5', 'I6'] ...\n",
            "\n",
            "本身已平稳的特征 (47个):\n",
            "['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E13', 'E14', 'E16', 'E19', 'E4', 'E7', 'I7', 'M1', 'M15', 'M2', 'M3'] ...\n"
          ]
        }
      ],
      "source": [
        "from statsmodels.tsa.stattools import adfuller\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def identify_cols_to_detrend(df, p_value_threshold=0.05):\n",
        "    \"\"\"\n",
        "    自动识别哪些列是不平稳的（需要 Detrend），哪些是平稳的。\n",
        "    \"\"\"\n",
        "    non_stationary_cols = []  # 需要处理的列 (d=0.4)\n",
        "    stationary_cols = []      # 不需要处理的列\n",
        "    \n",
        "    # 排除非特征列\n",
        "    ignore_cols = ['date_id', 'forward_returns', 'market_forward_excess_returns', \n",
        "                   'risk_free_rate', 'prediction', 'positions']\n",
        "    \n",
        "    features = [c for c in df.columns if c not in ignore_cols]\n",
        "    \n",
        "    print(f\"正在对 {len(features)} 个特征进行平稳性检验 (ADF Test)...\")\n",
        "    \n",
        "    for col in features:\n",
        "        # 获取该列数据，去除 NaN 以便计算\n",
        "        series = df[col].dropna()\n",
        "        \n",
        "        # 如果数据太少，直接跳过\n",
        "        if len(series) < 50:\n",
        "            continue\n",
        "            \n",
        "        # 为了速度，可以只取前 2000 行或者随机采样，但全量跑最准确\n",
        "        # 这里为了演示，我们取前 3000 行来判断属性\n",
        "        sample = series.iloc[:3000].values\n",
        "        \n",
        "        try:\n",
        "            # ADF 检验\n",
        "            result = adfuller(sample)\n",
        "            p_value = result[1]\n",
        "            \n",
        "            if p_value > p_value_threshold:\n",
        "                # p值大 -> 不拒绝原假设 -> 不平稳 -> 需要 Detrend\n",
        "                non_stationary_cols.append(col)\n",
        "            else:\n",
        "                # p值小 -> 拒绝原假设 -> 平稳 -> 保持原样\n",
        "                stationary_cols.append(col)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"无法计算 {col}: {e}\")\n",
        "            \n",
        "    return non_stationary_cols, stationary_cols\n",
        "\n",
        "# ================= 使用方法 =================\n",
        "# 假设 train_df 是你的原始数据\n",
        "cols_to_fix, cols_ok = identify_cols_to_detrend(X_full)\n",
        "\n",
        "print(f\"\\n需要做 d=0.4 的特征 ({len(cols_to_fix)}个):\")\n",
        "print(cols_to_fix[:20], \"...\" if len(cols_to_fix)>20 else \"\")\n",
        "\n",
        "print(f\"\\n本身已平稳的特征 ({len(cols_ok)}个):\")\n",
        "print(cols_ok[:20], \"...\" if len(cols_ok)>20 else \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Delta 清洗：分数差分（Fractional Differencing）\n",
        "# 用于处理非平稳时间序列，同时保留长期记忆\n",
        "# ============================================================\n",
        "\n",
        "def get_weights_ffd(d: float, threshold: float = 1e-5, max_size: int = 500) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    计算固定窗口分数差分（FFD）的权重\n",
        "    \n",
        "    参数:\n",
        "    d: 差分阶数，0 < d < 1\n",
        "    threshold: 权重截断阈值\n",
        "    max_size: 最大权重数量\n",
        "    \n",
        "    返回:\n",
        "    weights: 权重数组\n",
        "    \"\"\"\n",
        "    weights = [1.0]\n",
        "    k = 1\n",
        "    while True:\n",
        "        w = -weights[-1] * (d - k + 1) / k\n",
        "        if abs(w) < threshold or k >= max_size:\n",
        "            break\n",
        "        weights.append(w)\n",
        "        k += 1\n",
        "    return np.array(weights[::-1]).reshape(-1, 1)\n",
        "\n",
        "\n",
        "def frac_diff_ffd(series: pd.Series, d: float = 0.4, threshold: float = 1e-5) -> pd.Series:\n",
        "    \"\"\"\n",
        "    固定窗口分数差分（Fixed-Width Window Fracdiff, FFD）\n",
        "    \n",
        "    参数:\n",
        "    series: 输入时间序列\n",
        "    d: 差分阶数，推荐 0.3-0.5 之间\n",
        "    threshold: 权重截断阈值\n",
        "    \n",
        "    返回:\n",
        "    差分后的序列\n",
        "    \"\"\"\n",
        "    weights = get_weights_ffd(d, threshold)\n",
        "    width = len(weights)\n",
        "    \n",
        "    # 需要足够的历史数据\n",
        "    if len(series) < width:\n",
        "        return pd.Series(np.nan, index=series.index)\n",
        "    \n",
        "    result = np.full(len(series), np.nan)\n",
        "    \n",
        "    for i in range(width - 1, len(series)):\n",
        "        window = series.iloc[i - width + 1:i + 1].values\n",
        "        if not np.any(np.isnan(window)):\n",
        "            result[i] = np.dot(weights.T, window)[0, 0]\n",
        "    \n",
        "    return pd.Series(result, index=series.index)\n",
        "\n",
        "\n",
        "def delta_clean(df: pd.DataFrame, d: float = 0.4, verbose: bool = True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Delta 清洗函数：对非平稳特征进行分数差分处理\n",
        "    \n",
        "    处理的特征类别：\n",
        "    - P (Price/Valuation): PE, PB, CapeRatio 等估值指标 - 必须处理\n",
        "    - E (Macro Economic): GDP, M2, CPI 等宏观经济数据 - 必须处理\n",
        "    - I (Interest Rate): 利率相关特征 - 建议处理\n",
        "    \n",
        "    参数:\n",
        "    df: 输入 DataFrame\n",
        "    d: 分数差分阶数，默认 0.4（保留约 60% 的记忆）\n",
        "    verbose: 是否打印详细信息\n",
        "    \n",
        "    返回:\n",
        "    清洗后的 DataFrame\n",
        "    \"\"\"\n",
        "    df_out = df.copy()\n",
        "    \n",
        "    # 定义需要处理的特征前缀\n",
        "    MUST_CLEAN_PREFIXES = ['P', 'E']  # Price/Valuation, Macro Economic - 必须处理\n",
        "    SHOULD_CLEAN_PREFIXES = ['I']     # Interest Rate - 建议处理\n",
        "    \n",
        "    # 识别需要清洗的列\n",
        "    must_clean_cols = []\n",
        "    should_clean_cols = []\n",
        "    \n",
        "    for col in df_out.columns:\n",
        "        # 检查列名是否以特定前缀开头（后面跟数字）\n",
        "        for prefix in MUST_CLEAN_PREFIXES:\n",
        "            if col.startswith(prefix) and len(col) > 1 and col[1:].split('_')[0].isdigit():\n",
        "                must_clean_cols.append(col)\n",
        "                break\n",
        "        for prefix in SHOULD_CLEAN_PREFIXES:\n",
        "            if col.startswith(prefix) and len(col) > 1 and col[1:].split('_')[0].isdigit():\n",
        "                should_clean_cols.append(col)\n",
        "                break\n",
        "    \n",
        "    all_clean_cols = must_clean_cols + should_clean_cols\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"Delta 清洗（分数差分 d={d}）\")\n",
        "        print(\"=\" * 70)\n",
        "        print(f\"必须处理的特征 (P/E): {len(must_clean_cols)} 个\")\n",
        "        print(f\"建议处理的特征 (I): {len(should_clean_cols)} 个\")\n",
        "        print(f\"总计: {len(all_clean_cols)} 个特征\")\n",
        "    \n",
        "    cleaned_count = 0\n",
        "    \n",
        "    for col in all_clean_cols:\n",
        "        if col in df_out.columns:\n",
        "            original = df_out[col].copy()\n",
        "            cleaned = frac_diff_ffd(original, d=d)\n",
        "            \n",
        "            # 替换原列\n",
        "            df_out[col] = cleaned\n",
        "            cleaned_count += 1\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n已清洗特征数: {cleaned_count}\")\n",
        "        # 显示处理的列\n",
        "        if must_clean_cols:\n",
        "            print(f\"\\nP/E 类特征 (已处理): {must_clean_cols[:10]}{'...' if len(must_clean_cols) > 10 else ''}\")\n",
        "        if should_clean_cols:\n",
        "            print(f\"I 类特征 (已处理): {should_clean_cols[:10]}{'...' if len(should_clean_cols) > 10 else ''}\")\n",
        "        print(\"=\" * 70)\n",
        "    \n",
        "    return df_out\n",
        "\n",
        "\n",
        "print(\"✓ 已定义 delta_clean 函数（分数差分清洗）\")\n",
        "print(\"  - get_weights_ffd: 计算 FFD 权重\")\n",
        "print(\"  - frac_diff_ffd: 固定窗口分数差分\")\n",
        "print(\"  - delta_clean: 主清洗函数，处理 P/E/I 类特征\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 全局变量用于控制日志\n",
        "log = True\n",
        "\n",
        "def create_drop_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    特征工程函数：创建和选择特征\n",
        "    \"\"\"\n",
        "    global log\n",
        "    \n",
        "    \n",
        "    df_out = df.copy()\n",
        "    \n",
        "    # 基础特征生成配置 - 扩展版（目标：生成 250-350 个特征）\n",
        "    # 从 7 个扩展到 28 个基础特征，覆盖所有类别\n",
        "    TOP_FEATURES = [\n",
        "        # 动量类 (Momentum) - 5 个\n",
        "        'M4', 'M17', 'M1', 'M2', 'M3',\n",
        "        \n",
        "        # 情绪类 (Sentiment) - 5 个\n",
        "        'S2', 'S5', 'S1', 'S3', 'S12',\n",
        "        \n",
        "        # 经济类 (Economic) - 5 个\n",
        "        'E11', 'E19', 'E1', 'E2', 'E3',\n",
        "        \n",
        "        # 价格类 (Price) - 4 个（新增，熊市重要）\n",
        "        'P1', 'P5', 'P7', 'P9',\n",
        "        \n",
        "        # 波动率类 (Volatility) - 4 个（新增，熊市重要）\n",
        "        'V1', 'V3', 'V5', 'V7',\n",
        "        \n",
        "        # 流动性类 (Liquidity) - 3 个（新增，熊市重要）\n",
        "        'I1', 'I2', 'I3',\n",
        "        \n",
        "        # 方向类 (Direction) - 2 个（新增）\n",
        "        'D1', 'D2',\n",
        "        \n",
        "        # 滞后收益（重要）\n",
        "        'forward_returns_lag'\n",
        "    ]\n",
        "    \n",
        "    # 优化 Lag 和 Rolling 窗口（减少冗余，提高效率）\n",
        "    LAG_PERIODS = [1, 3, 5, 10, 20]  # 5 个 lag（原来 3 个）\n",
        "    ROLLING_WINDOWS = [5, 10, 20, 60]  # 4 个窗口（原来 3 个）\n",
        "    \n",
        "    COLS_TO_DROP = ['date_id', 'risk_free_rate_lag']\n",
        "    epsilon = 1e-6\n",
        "    \n",
        "    # 批量生成 Lag 和 Rolling 特征\n",
        "    features_dict = {}\n",
        "    for col in TOP_FEATURES:\n",
        "        if col in df_out.columns:\n",
        "            # Lag 特征\n",
        "            for lag in LAG_PERIODS:\n",
        "                features_dict[f'{col}_lag_{lag}'] = df_out[col].shift(lag)\n",
        "            \n",
        "            # Rolling 统计特征\n",
        "            for window in ROLLING_WINDOWS:\n",
        "                features_dict[f'{col}_roll_mean_{window}'] = df_out[col].rolling(window=window, min_periods=1).mean()\n",
        "                features_dict[f'{col}_roll_std_{window}'] = df_out[col].rolling(window=window, min_periods=1).std()\n",
        "            \n",
        "            # 对于重要特征，添加更多滚动统计（仅对部分关键特征）\n",
        "            if col in ['forward_returns_lag', 'M4', 'S5', 'V7', 'I2']:\n",
        "                for window in [20, 60]:\n",
        "                    features_dict[f'{col}_roll_max_{window}'] = df_out[col].rolling(window=window, min_periods=1).max()\n",
        "                    features_dict[f'{col}_roll_min_{window}'] = df_out[col].rolling(window=window, min_periods=1).min()\n",
        "    \n",
        "    # 批量生成交互特征（类别内和类别间）\n",
        "    # 类别内交互：同一类别特征之间的交互\n",
        "    interaction_features = []\n",
        "    \n",
        "    # M (动量) 类别内交互\n",
        "    m_features = [col for col in ['M1', 'M2', 'M3', 'M4', 'M17'] if col in df_out.columns]\n",
        "    if len(m_features) >= 2:\n",
        "        for i, feat1 in enumerate(m_features[:3]):  # 只选择前3个进行交互\n",
        "            for feat2 in m_features[i+1:min(i+3, len(m_features))]:\n",
        "                if feat1 in df_out.columns and feat2 in df_out.columns:\n",
        "                    features_dict[f'feat_{feat1}_x_{feat2}'] = df_out[feat1] * df_out[feat2]\n",
        "                    features_dict[f'feat_{feat1}_div_{feat2}'] = df_out[feat1] / (df_out[feat2] + epsilon)\n",
        "                    interaction_features.extend([f'feat_{feat1}_x_{feat2}', f'feat_{feat1}_div_{feat2}'])\n",
        "    \n",
        "    # S (情绪) 类别内交互\n",
        "    s_features = [col for col in ['S1', 'S2', 'S3', 'S5', 'S12'] if col in df_out.columns]\n",
        "    if len(s_features) >= 2:\n",
        "        for i, feat1 in enumerate(s_features[:3]):\n",
        "            for feat2 in s_features[i+1:min(i+3, len(s_features))]:\n",
        "                if feat1 in df_out.columns and feat2 in df_out.columns:\n",
        "                    features_dict[f'feat_{feat1}_x_{feat2}'] = df_out[feat1] * df_out[feat2]\n",
        "                    interaction_features.append(f'feat_{feat1}_x_{feat2}')\n",
        "    \n",
        "    # E (经济) 类别内交互\n",
        "    e_features = [col for col in ['E1', 'E2', 'E3', 'E11', 'E19'] if col in df_out.columns]\n",
        "    if len(e_features) >= 2:\n",
        "        for i, feat1 in enumerate(e_features[:3]):\n",
        "            for feat2 in e_features[i+1:min(i+3, len(e_features))]:\n",
        "                if feat1 in df_out.columns and feat2 in df_out.columns:\n",
        "                    features_dict[f'feat_{feat1}_x_{feat2}'] = df_out[feat1] * df_out[feat2]\n",
        "                    interaction_features.append(f'feat_{feat1}_x_{feat2}')\n",
        "    \n",
        "    # 类别间交互（特别是熊市相关）\n",
        "    # V (波动率) × I (流动性) - 熊市重要\n",
        "    v_features = [col for col in ['V1', 'V3', 'V5', 'V7'] if col in df_out.columns]\n",
        "    i_features = [col for col in ['I1', 'I2', 'I3'] if col in df_out.columns]\n",
        "    if len(v_features) > 0 and len(i_features) > 0:\n",
        "        v_feat = v_features[0]\n",
        "        i_feat = i_features[0]\n",
        "        if v_feat in df_out.columns and i_feat in df_out.columns:\n",
        "            features_dict['feat_V_x_I'] = df_out[v_feat] * df_out[i_feat]\n",
        "            features_dict['feat_V_div_I'] = df_out[v_feat] / (df_out[i_feat] + epsilon)\n",
        "            interaction_features.extend(['feat_V_x_I', 'feat_V_div_I'])\n",
        "    \n",
        "    # M (动量) × V (波动率) - 熊市动量失效\n",
        "    if len(m_features) > 0 and len(v_features) > 0:\n",
        "        m_feat = m_features[0]\n",
        "        v_feat = v_features[0]\n",
        "        if m_feat in df_out.columns and v_feat in df_out.columns:\n",
        "            features_dict['feat_M_div_V'] = df_out[m_feat] / (df_out[v_feat] + epsilon)\n",
        "            features_dict['feat_M_x_V'] = df_out[m_feat] * df_out[v_feat]\n",
        "            interaction_features.extend(['feat_M_div_V', 'feat_M_x_V'])\n",
        "    \n",
        "    # S (情绪) × V (波动率) - 恐慌情绪\n",
        "    if len(s_features) > 0 and len(v_features) > 0:\n",
        "        s_feat = s_features[0]\n",
        "        v_feat = v_features[0]\n",
        "        if s_feat in df_out.columns and v_feat in df_out.columns:\n",
        "            features_dict['feat_S_x_V'] = df_out[s_feat] * df_out[v_feat]\n",
        "            interaction_features.append('feat_S_x_V')\n",
        "    \n",
        "    # P (价格) × V (波动率) - 价格波动\n",
        "    p_features = [col for col in ['P1', 'P5', 'P7', 'P9'] if col in df_out.columns]\n",
        "    if len(p_features) > 0 and len(v_features) > 0:\n",
        "        p_feat = p_features[0]\n",
        "        v_feat = v_features[0]\n",
        "        if p_feat in df_out.columns and v_feat in df_out.columns:\n",
        "            features_dict['feat_P_x_V'] = df_out[p_feat] * df_out[v_feat]\n",
        "            interaction_features.append('feat_P_x_V')\n",
        "    \n",
        "    # 保留原有的重要交互特征\n",
        "    if 'I2' in df_out.columns and 'M17' in df_out.columns:\n",
        "        m17_thr = df_out[\"M17\"].median()\n",
        "        features_dict[\"feat_I2_poshinge_x_M17low\"] = np.maximum(df_out[\"I2\"], 0.0) * (df_out[\"M17\"] < m17_thr).astype(float)\n",
        "        interaction_features.append(\"feat_I2_poshinge_x_M17low\")\n",
        "    \n",
        "    if 'S12' in df_out.columns and 'V7' in df_out.columns:\n",
        "        features_dict[\"feat_S12_minus_V7\"] = df_out['S12'] - df_out['V7']\n",
        "        interaction_features.append(\"feat_S12_minus_V7\")\n",
        "    \n",
        "    if 'M4' in df_out.columns and 'P7' in df_out.columns:\n",
        "        features_dict[\"feat_M4_minus_P7\"] = df_out[\"M4\"] - df_out[\"P7\"]\n",
        "        interaction_features.append(\"feat_M4_minus_P7\")\n",
        "    \n",
        "    if 'M4' in df_out.columns and 'E19' in df_out.columns:\n",
        "        features_dict['feat_M4_x_E19'] = df_out['M4'] * df_out['E19']\n",
        "        interaction_features.append('feat_M4_x_E19')\n",
        "    \n",
        "    if 'P7' in df_out.columns and 'V7' in df_out.columns:\n",
        "        features_dict['feat_P7_x_V7'] = df_out['P7'] * df_out['V7']\n",
        "        interaction_features.append('feat_P7_x_V7')\n",
        "    \n",
        "    if 'S5' in df_out.columns and 'E11' in df_out.columns:\n",
        "        features_dict['feat_S5_x_E11'] = df_out['S5'] * df_out['E11']\n",
        "        interaction_features.append('feat_S5_x_E11')\n",
        "\n",
        "    # 环境感知特征 (Regime Awareness) - 增强版\n",
        "    regime_features = []\n",
        "    \n",
        "    # 1. 宏观趋势特征\n",
        "    if 'S5' in df_out.columns:\n",
        "        if 'S5_roll_mean_60' in features_dict:\n",
        "            features_dict['feat_Macro_Trend'] = df_out['S5'] - features_dict['S5_roll_mean_60']\n",
        "        else:\n",
        "            # 如果还没有生成，临时计算\n",
        "            s5_roll_mean = df_out['S5'].rolling(60, min_periods=1).mean()\n",
        "            features_dict['feat_Macro_Trend'] = df_out['S5'] - s5_roll_mean\n",
        "    elif 'M4' in df_out.columns:\n",
        "        if 'M4_roll_mean_60' in features_dict:\n",
        "            features_dict['feat_Macro_Trend'] = df_out['M4'] - features_dict['M4_roll_mean_60']\n",
        "        else:\n",
        "            features_dict['feat_Macro_Trend'] = df_out['M4']\n",
        "    else:\n",
        "        features_dict['feat_Macro_Trend'] = 0\n",
        "    regime_features.append('feat_Macro_Trend')\n",
        "    \n",
        "    # 2. 波动率状态特征\n",
        "    if 'S2_roll_std_60' in features_dict:\n",
        "        features_dict['feat_Vol_Regime'] = features_dict['S2_roll_std_60']\n",
        "    elif 'S2' in df_out.columns:\n",
        "        features_dict['feat_Vol_Regime'] = df_out['S2'].rolling(60, min_periods=1).std()\n",
        "    elif 'V7' in df_out.columns:\n",
        "        features_dict['feat_Vol_Regime'] = df_out['V7']\n",
        "    else:\n",
        "        features_dict['feat_Vol_Regime'] = 0\n",
        "    regime_features.append('feat_Vol_Regime')\n",
        "    \n",
        "    # 3. 动量 × 趋势\n",
        "    if 'forward_returns_lag' in df_out.columns:\n",
        "        if 'feat_Macro_Trend' in features_dict:\n",
        "            features_dict['feat_Mom_x_Trend'] = df_out['forward_returns_lag'] * features_dict['feat_Macro_Trend']\n",
        "        else:\n",
        "            features_dict['feat_Mom_x_Trend'] = df_out['forward_returns_lag']\n",
        "        regime_features.append('feat_Mom_x_Trend')\n",
        "    \n",
        "    # 4. 熊市指标：连续下跌天数（新增）\n",
        "    if 'forward_returns_lag' in df_out.columns:\n",
        "        returns_neg = (df_out['forward_returns_lag'] < 0).astype(int)\n",
        "        # 计算连续下跌天数\n",
        "        bear_days = returns_neg.groupby((returns_neg != returns_neg.shift()).cumsum()).cumsum()\n",
        "        features_dict['feat_Bear_Days'] = bear_days\n",
        "        regime_features.append('feat_Bear_Days')\n",
        "    \n",
        "    # 5. 波动率突破指标（新增，熊市重要）\n",
        "    if 'S2' in df_out.columns:\n",
        "        vol_current = df_out['S2'].rolling(5, min_periods=1).std()\n",
        "        vol_historical = df_out['S2'].rolling(60, min_periods=1).std()\n",
        "        features_dict['feat_Vol_Spike'] = vol_current / (vol_historical + epsilon)\n",
        "        regime_features.append('feat_Vol_Spike')\n",
        "    elif 'V7' in df_out.columns:\n",
        "        vol_current = df_out['V7'].rolling(5, min_periods=1).mean()\n",
        "        vol_historical = df_out['V7'].rolling(60, min_periods=1).mean()\n",
        "        features_dict['feat_Vol_Spike'] = vol_current / (vol_historical + epsilon)\n",
        "        regime_features.append('feat_Vol_Spike')\n",
        "    \n",
        "    # 6. 相对强度特征（新增）\n",
        "    if 'M4' in df_out.columns:\n",
        "        # 当前值 vs 历史分位数\n",
        "        def calc_percentile(series):\n",
        "            if len(series) < 60:\n",
        "                return np.nan\n",
        "            return (series.iloc[-1] > series.iloc[:-1]).sum() / len(series.iloc[:-1])\n",
        "        features_dict['feat_M4_percentile_60'] = df_out['M4'].rolling(60, min_periods=60).apply(calc_percentile, raw=False)\n",
        "        regime_features.append('feat_M4_percentile_60')\n",
        "    \n",
        "    # 7. RSI 类特征（新增，熊市超卖指标）\n",
        "    if 'forward_returns_lag' in df_out.columns:\n",
        "        returns = df_out['forward_returns_lag']\n",
        "        gains = returns.where(returns > 0, 0)\n",
        "        losses = -returns.where(returns < 0, 0)\n",
        "        avg_gain = gains.rolling(14, min_periods=1).mean()\n",
        "        avg_loss = losses.rolling(14, min_periods=1).mean()\n",
        "        rs = avg_gain / (avg_loss + epsilon)\n",
        "        features_dict['feat_RSI'] = 100 - (100 / (1 + rs))\n",
        "        regime_features.append('feat_RSI')\n",
        "    \n",
        "    NEW_REGIME_COLS = regime_features\n",
        "    \n",
        "    # 确保新特征的索引与原始 DataFrame 对齐（合并所有特征）\n",
        "    if features_dict:\n",
        "        features_df = pd.DataFrame(features_dict, index=df_out.index)\n",
        "        df_out = pd.concat([df_out, features_df], axis=1)\n",
        "    \n",
        "    # 删除废弃列\n",
        "    df_out.drop(columns = [col for col in COLS_TO_DROP if col in df_out.columns], inplace = True)\n",
        "    \n",
        "    # 统计信息（用于日志）- 在所有特征生成完成后计算\n",
        "    n_base_features = len([f for f in TOP_FEATURES if f in df_out.columns])\n",
        "    # 衍生特征：Lag 和 Rolling 特征\n",
        "    derived_patterns = [f'{f}_lag_' for f in TOP_FEATURES] + [f'{f}_roll_' for f in TOP_FEATURES]\n",
        "    n_derived_features = len([col for col in df_out.columns if any(col.startswith(p) for p in derived_patterns)])\n",
        "    # 交互特征：以 feat_ 开头但不是环境感知特征\n",
        "    n_interaction_features = len([col for col in df_out.columns if col.startswith('feat_') and col not in NEW_REGIME_COLS])\n",
        "    # 环境感知特征\n",
        "    n_regime_features = len([col for col in NEW_REGIME_COLS if col in df_out.columns])\n",
        "    \n",
        "    if (log):\n",
        "        total_features = len(df_out.columns)\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(\"特征工程统计\")\n",
        "        print(f\"{'='*70}\")\n",
        "        print(f\"基础特征数: {n_base_features} (从 {len(TOP_FEATURES)} 个候选特征中选择)\")\n",
        "        print(f\"衍生特征数: {n_derived_features} (Lag + Rolling 统计)\")\n",
        "        print(f\"交互特征数: {n_interaction_features} (类别内和类别间交互)\")\n",
        "        print(f\"环境感知特征数: {n_regime_features} (市场状态指标)\")\n",
        "        print(f\"总特征数: {total_features}\")\n",
        "        print(f\"特征类别覆盖: M(动量), S(情绪), E(经济), P(价格), V(波动率), I(流动性), D(方向)\")\n",
        "        print(f\"Feature X (Top 10): {df_out.columns.tolist()[:10]} ... and {total_features-10} more\")\n",
        "        print(f\"Feature Creator Output Shape: {df_out.shape}\")\n",
        "        if NEW_REGIME_COLS:\n",
        "            print(f\"环境感知特征: {[c for c in NEW_REGIME_COLS if c in df_out.columns]}\")\n",
        "        print(f\"{'='*70}\\n\")\n",
        "        log = False\n",
        "        \n",
        "    return df_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "准备数据...\n",
            "======================================================================\n",
            "X shape: (9021, 97)\n",
            "y shape: (9021,)\n",
            "\n",
            "特征选择器参数:\n",
            "  n_clusters: None\n",
            "  min_cluster_size: 3\n",
            "  max_clusters: 15\n",
            "  loss_threshold: None\n",
            "  top_k_per_group: 0.65\n",
            "  clustering_method: kmeans\n",
            "  n_repeats: 3\n",
            "  random_state: 42\n",
            "  verbose: 1\n",
            "\n",
            "======================================================================\n",
            "配置并行模型\n",
            "======================================================================\n",
            "机会模型（Regressor）: RMSE (MSE)\n",
            "方向模型（Classifier）: Logloss (Cross Entropy)\n",
            "\n",
            "✓ 并行集成 Pipeline 创建完成\n",
            "\n",
            "======================================================================\n",
            "开始两段滑动窗口测试\n",
            "======================================================================\n",
            "训练窗口: 5000\n",
            "测试窗口: 2000\n",
            "窗口模式: 滑动窗口\n",
            "允许做空: True\n",
            "================================================================================\n",
            "两段滑动窗口测试 (Move Forward Two-Way Test)\n",
            "================================================================================\n",
            "总样本数: 9021, 训练窗口: 5000, 测试窗口: 2000\n",
            "窗口模式: 滑动窗口\n",
            "预计轮数: ~2\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "第 1 轮\n",
            "================================================================================\n",
            "  训练段: [13:5013) = 5000 样本\n",
            "  测试段: [5013:7013) = 2000 样本\n",
            "\n",
            "  [步骤1/2] 并行训练模型...\n",
            "======================================================================\n",
            "ParallelEnsemblePipeline - 开始拟合（并行双模型）\n",
            "======================================================================\n",
            "样本数: 5000\n",
            "\n",
            "======================================================================\n",
            "[模型1/2] 训练机会模型（Regressor，MSE）\n",
            "======================================================================\n",
            "======================================================================\n",
            "OpportunityPipeline - 开始拟合（机会模型，MSE）\n",
            "======================================================================\n",
            "机会目标统计: min=0.0003, max=2.0000, mean=1.0660\n",
            "\n",
            "[步骤1/3] 特征工程...\n",
            "\n",
            "======================================================================\n",
            "特征工程统计\n",
            "======================================================================\n",
            "基础特征数: 29 (从 29 个候选特征中选择)\n",
            "衍生特征数: 397 (Lag + Rolling 统计)\n",
            "交互特征数: 36 (类别内和类别间交互)\n",
            "环境感知特征数: 7 (市场状态指标)\n",
            "总特征数: 535\n",
            "特征类别覆盖: M(动量), S(情绪), E(经济), P(价格), V(波动率), I(流动性), D(方向)\n",
            "Feature X (Top 10): ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'E1'] ... and 525 more\n",
            "Feature Creator Output Shape: (5000, 535)\n",
            "环境感知特征: ['feat_Macro_Trend', 'feat_Vol_Regime', 'feat_Mom_x_Trend', 'feat_Bear_Days', 'feat_Vol_Spike', 'feat_M4_percentile_60', 'feat_RSI']\n",
            "======================================================================\n",
            "\n",
            "特征工程后特征数量: 535\n",
            "\n",
            "[步骤2/3] 特征选择（使用 opportunity_loss_function）...\n",
            "  对因子进行聚类...\n",
            "  自动选择最优聚类数量...\n",
            "  聚类完成: 5 个有效组 (总因子数: 535)\n",
            "  使用损失函数计算因子组重要性（推理阶段置换，不重新训练）...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  测试各组: 100%|██████████| 5/5 [00:00<00:00,  8.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  组损失计算完成\n",
            "  Top 5 重要组 (损失增加越大越重要):\n",
            "    组 0 (395个因子): 损失增加 = 0.023031\n",
            "    组 24 (90个因子): 损失增加 = 0.003580\n",
            "    组 23 (3个因子): 损失增加 = 0.000042\n",
            "    组 13 (11个因子): 损失增加 = -0.001278\n",
            "    组 18 (14个因子): 损失增加 = -0.001385\n",
            "  从重要组中选择因子（使用全局模型特征重要性）...\n",
            "  筛选出 2 个重要组 (阈值: 0.000042)\n",
            "    组 0: 395 个有效因子 -> 选择 256 个 (top_k=0.65)\n",
            "    组 24: 90 个有效因子 -> 选择 58 个 (top_k=0.65)\n",
            "  组内因子选择统计:\n",
            "    重要组总因子数: 485\n",
            "    选择后因子数: 314\n",
            "    去重后因子数: 314\n",
            "    最终选择 314 个因子 (top_k_per_group=0.65)\n",
            "特征选择后特征数量: 314\n",
            "\n",
            "[步骤3/3] 训练机会模型（MSE）...\n",
            "OpportunityPipeline - 拟合完成\n",
            "\n",
            "======================================================================\n",
            "[模型2/2] 训练方向模型（Classifier，Logloss）\n",
            "======================================================================\n",
            "======================================================================\n",
            "DirectionClassifierPipeline - 开始拟合（方向分类，Logloss）\n",
            "======================================================================\n",
            "方向标签分布: 多(1)=2538 (50.76%), 空(0)=2462 (49.24%)\n",
            "\n",
            "[步骤1/3] 特征工程...\n",
            "特征工程后特征数量: 535\n",
            "\n",
            "[步骤2/3] 特征选择（使用 direction_loss_function）...\n",
            "  对因子进行聚类...\n",
            "  自动选择最优聚类数量...\n",
            "  聚类完成: 5 个有效组 (总因子数: 535)\n",
            "  使用损失函数计算因子组重要性（推理阶段置换，不重新训练）...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  测试各组: 100%|██████████| 5/5 [00:00<00:00,  7.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  组损失计算完成\n",
            "  Top 5 重要组 (损失增加越大越重要):\n",
            "    组 0 (395个因子): 损失增加 = 3.625794\n",
            "    组 24 (90个因子): 损失增加 = 1.379327\n",
            "    组 18 (14个因子): 损失增加 = 0.446120\n",
            "    组 13 (11个因子): 损失增加 = 0.171139\n",
            "    组 23 (3个因子): 损失增加 = 0.079079\n",
            "  从重要组中选择因子（使用全局模型特征重要性）...\n",
            "  筛选出 2 个重要组 (阈值: 0.446120)\n",
            "    组 0: 395 个有效因子 -> 选择 256 个 (top_k=0.65)\n",
            "    组 24: 90 个有效因子 -> 选择 58 个 (top_k=0.65)\n",
            "  组内因子选择统计:\n",
            "    重要组总因子数: 485\n",
            "    选择后因子数: 314\n",
            "    去重后因子数: 314\n",
            "    最终选择 314 个因子 (top_k_per_group=0.65)\n",
            "特征选择后特征数量: 314\n",
            "\n",
            "[步骤3/3] 训练方向分类模型（Logloss）...\n",
            "DirectionClassifierPipeline - 拟合完成\n",
            "\n",
            "======================================================================\n",
            "ParallelEnsemblePipeline - 拟合完成\n",
            "======================================================================\n",
            "\n",
            "  [步骤2/2] 在测试段上预测...\n",
            "\n",
            "  预测均值: 0.7174, 真实均值: 0.0002\n",
            "\n",
            "================================================================================\n",
            "第 2 轮\n",
            "================================================================================\n",
            "  训练段: [2013:7013) = 5000 样本\n",
            "  测试段: [7013:9013) = 2000 样本\n",
            "\n",
            "  [步骤1/2] 并行训练模型...\n",
            "======================================================================\n",
            "ParallelEnsemblePipeline - 开始拟合（并行双模型）\n",
            "======================================================================\n",
            "样本数: 5000\n",
            "\n",
            "======================================================================\n",
            "[模型1/2] 训练机会模型（Regressor，MSE）\n",
            "======================================================================\n",
            "======================================================================\n",
            "OpportunityPipeline - 开始拟合（机会模型，MSE）\n",
            "======================================================================\n",
            "机会目标统计: min=0.0009, max=2.0000, mean=1.0508\n",
            "\n",
            "[步骤1/3] 特征工程...\n",
            "特征工程后特征数量: 535\n",
            "\n",
            "[步骤2/3] 特征选择（使用 opportunity_loss_function）...\n",
            "  对因子进行聚类...\n",
            "  自动选择最优聚类数量...\n",
            "  聚类完成: 4 个有效组 (总因子数: 535)\n",
            "  使用损失函数计算因子组重要性（推理阶段置换，不重新训练）...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  测试各组: 100%|██████████| 4/4 [00:00<00:00,  9.74it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  组损失计算完成\n",
            "  Top 5 重要组 (损失增加越大越重要):\n",
            "    组 0 (323个因子): 损失增加 = 0.045179\n",
            "    组 16 (12个因子): 损失增加 = -0.000379\n",
            "    组 20 (130个因子): 损失增加 = -0.000530\n",
            "    组 24 (47个因子): 损失增加 = -0.002420\n",
            "  从重要组中选择因子（使用全局模型特征重要性）...\n",
            "  筛选出 2 个重要组 (阈值: -0.000454)\n",
            "    组 0: 323 个有效因子 -> 选择 209 个 (top_k=0.65)\n",
            "    组 16: 12 个有效因子 -> 选择 7 个 (top_k=0.65)\n",
            "  组内因子选择统计:\n",
            "    重要组总因子数: 335\n",
            "    选择后因子数: 216\n",
            "    去重后因子数: 216\n",
            "    最终选择 216 个因子 (top_k_per_group=0.65)\n",
            "特征选择后特征数量: 216\n",
            "\n",
            "[步骤3/3] 训练机会模型（MSE）...\n",
            "OpportunityPipeline - 拟合完成\n",
            "\n",
            "======================================================================\n",
            "[模型2/2] 训练方向模型（Classifier，Logloss）\n",
            "======================================================================\n",
            "======================================================================\n",
            "DirectionClassifierPipeline - 开始拟合（方向分类，Logloss）\n",
            "======================================================================\n",
            "方向标签分布: 多(1)=2586 (51.72%), 空(0)=2414 (48.28%)\n",
            "\n",
            "[步骤1/3] 特征工程...\n",
            "特征工程后特征数量: 535\n",
            "\n",
            "[步骤2/3] 特征选择（使用 direction_loss_function）...\n",
            "  对因子进行聚类...\n",
            "  自动选择最优聚类数量...\n",
            "  聚类完成: 4 个有效组 (总因子数: 535)\n",
            "  使用损失函数计算因子组重要性（推理阶段置换，不重新训练）...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  测试各组: 100%|██████████| 4/4 [00:00<00:00,  8.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  组损失计算完成\n",
            "  Top 5 重要组 (损失增加越大越重要):\n",
            "    组 0 (323个因子): 损失增加 = 3.209955\n",
            "    组 20 (130个因子): 损失增加 = 1.644813\n",
            "    组 24 (47个因子): 损失增加 = 0.617838\n",
            "    组 16 (12个因子): 损失增加 = 0.152585\n",
            "  从重要组中选择因子（使用全局模型特征重要性）...\n",
            "  筛选出 2 个重要组 (阈值: 1.131326)\n",
            "    组 0: 323 个有效因子 -> 选择 209 个 (top_k=0.65)\n",
            "    组 20: 130 个有效因子 -> 选择 84 个 (top_k=0.65)\n",
            "  组内因子选择统计:\n",
            "    重要组总因子数: 453\n",
            "    选择后因子数: 293\n",
            "    去重后因子数: 293\n",
            "    最终选择 293 个因子 (top_k_per_group=0.65)\n",
            "特征选择后特征数量: 293\n",
            "\n",
            "[步骤3/3] 训练方向分类模型（Logloss）...\n",
            "DirectionClassifierPipeline - 拟合完成\n",
            "\n",
            "======================================================================\n",
            "ParallelEnsemblePipeline - 拟合完成\n",
            "======================================================================\n",
            "\n",
            "  [步骤2/2] 在测试段上预测...\n",
            "\n",
            "  预测均值: 0.9890, 真实均值: 0.0002\n",
            "\n",
            "================================================================================\n",
            "两段滑动窗口测试完成！\n",
            "================================================================================\n",
            "总轮数: 2, 有效预测数: 4000\n",
            "整体IC: 0.0081\n",
            "整体夏普率: 0.3434\n",
            "================================================================================\n",
            "\n",
            "======================================================================\n",
            "结果分析\n",
            "======================================================================\n",
            "有效预测数: 4000\n",
            "整体 IC: 0.0081\n",
            "整体夏普率: 0.3434\n",
            "\n",
            "仓位统计:\n",
            "  min: -1.1851\n",
            "  max: 1.2643\n",
            "  mean: 0.8532\n",
            "\n",
            "======================================================================\n",
            "方向模型评估\n",
            "======================================================================\n",
            "总体方向准确率: 0.5255\n",
            "F1 分数: 0.6674 (多头为正类)\n",
            "F1 分数 (宏平均): 0.4200\n",
            "Precision: 0.5301, Recall: 0.9007\n",
            "  做多准确率: 0.9007 (真实多时预测多)\n",
            "  做空准确率: 0.1050 (真实空时预测空)\n",
            "  真实多头样本数: 2114\n",
            "  真实空头样本数: 1886\n",
            "\n",
            "预测概率分析:\n",
            "  预测正确时平均概率: 0.5047\n",
            "  预测错误时平均概率: 0.5044\n",
            "\n",
            "======================================================================\n",
            "机会模型评估\n",
            "======================================================================\n",
            "R² 分数: 0.0513\n",
            "RMSE: 0.5956\n",
            "MAE: 0.5215\n",
            "相关系数: 0.2978\n",
            "\n",
            "真实机会统计: min=0.0003, max=2.0000, mean=1.0228\n",
            "预测机会统计: min=0.8870, max=1.2643, mean=1.0630\n",
            "\n",
            "======================================================================\n",
            "按时间段分析\n",
            "======================================================================\n",
            "\n",
            "第 1 轮 (样本数: 2000):\n",
            "  IC: 0.0331, 夏普率: 0.6508\n",
            "  方向准确率: 0.5305\n",
            "  机会 R²: 0.0298, 机会相关系数: 0.2852\n",
            "\n",
            "第 2 轮 (样本数: 2000):\n",
            "  IC: -0.0308, 夏普率: 0.0927\n",
            "  方向准确率: 0.5205\n",
            "  机会 R²: 0.0714, 机会相关系数: 0.3256\n",
            "\n",
            "\n",
            "======================================================================\n",
            "数据流诊断: 并行模型预测\n",
            "======================================================================\n",
            "\n",
            "1. 长度检查:\n",
            "   y_pred长度: 4000\n",
            "   y_true长度: 4000\n",
            "   ✓ 长度一致\n",
            "   ✓ 长度足够\n",
            "\n",
            "2. NaN检查:\n",
            "   y_pred NaN数量: 0\n",
            "   y_true NaN数量: 0\n",
            "   ✓ y_pred无NaN\n",
            "   ✓ y_true无NaN\n",
            "\n",
            "3. Inf检查:\n",
            "   y_pred Inf数量: 0\n",
            "   y_true Inf数量: 0\n",
            "   ✓ 无Inf值\n",
            "\n",
            "4. 方差检查:\n",
            "   y_pred方差: 0.4069675563\n",
            "   y_true方差: 0.0001009008\n",
            "   ✓ y_pred有方差\n",
            "   ✓ y_true有方差\n",
            "\n",
            "5. 统计信息:\n",
            "   y_pred: min=-1.185134, max=1.264332, mean=0.853188, std=0.637940, median=1.052261\n",
            "   y_true: min=-0.040582, max=0.040551, mean=0.000223, std=0.010045, median=0.000357\n",
            "\n",
            "6. 市场收益率检查:\n",
            "   市场收益率长度: 4000\n",
            "   NaN数量: 0\n",
            "   ✓ 市场收益率无NaN\n",
            "   市场状态分布:\n",
            "     牛市 (> 2.00%): 97 个样本\n",
            "     熊市 (< -2.00%): 140 个样本\n",
            "     混合市场: 3763 个样本\n",
            "     牛市数据质量: y_pred NaN=0, y_true NaN=0, y_pred方差=0.156181, y_true方差=0.000043\n",
            "     熊市数据质量: y_pred NaN=0, y_true NaN=0, y_pred方差=0.168780, y_true方差=0.000044\n",
            "\n",
            "======================================================================\n",
            "✓ 数据质量检查通过，可以计算IC\n",
            "======================================================================\n",
            "\n",
            "\n",
            "======================================================================\n",
            "按市场状态分组 IC 测试（改进版 - 含数据验证）\n",
            "======================================================================\n",
            "======================================================================\n",
            "数据流诊断: 整体数据\n",
            "======================================================================\n",
            "\n",
            "1. 长度检查:\n",
            "   y_pred长度: 4000\n",
            "   y_true长度: 4000\n",
            "   ✓ 长度一致\n",
            "   ✓ 长度足够\n",
            "\n",
            "2. NaN检查:\n",
            "   y_pred NaN数量: 0\n",
            "   y_true NaN数量: 0\n",
            "   ✓ y_pred无NaN\n",
            "   ✓ y_true无NaN\n",
            "\n",
            "3. Inf检查:\n",
            "   y_pred Inf数量: 0\n",
            "   y_true Inf数量: 0\n",
            "   ✓ 无Inf值\n",
            "\n",
            "4. 方差检查:\n",
            "   y_pred方差: 0.4069675563\n",
            "   y_true方差: 0.0001009008\n",
            "   ✓ y_pred有方差\n",
            "   ✓ y_true有方差\n",
            "\n",
            "5. 统计信息:\n",
            "   y_pred: min=-1.185134, max=1.264332, mean=0.853188, std=0.637940, median=1.052261\n",
            "   y_true: min=-0.040582, max=0.040551, mean=0.000223, std=0.010045, median=0.000357\n",
            "\n",
            "6. 市场收益率检查:\n",
            "   市场收益率长度: 4000\n",
            "   NaN数量: 0\n",
            "   ✓ 市场收益率无NaN\n",
            "   市场状态分布:\n",
            "     牛市 (> 2.00%): 97 个样本\n",
            "     熊市 (< -2.00%): 140 个样本\n",
            "     混合市场: 3763 个样本\n",
            "     牛市数据质量: y_pred NaN=0, y_true NaN=0, y_pred方差=0.156181, y_true方差=0.000043\n",
            "     熊市数据质量: y_pred NaN=0, y_true NaN=0, y_pred方差=0.168780, y_true方差=0.000044\n",
            "\n",
            "======================================================================\n",
            "✓ 数据质量检查通过，可以计算IC\n",
            "======================================================================\n",
            "\n",
            "市场状态划分标准:\n",
            "  牛市: 市场收益率 > 2.00%\n",
            "  熊市: 市场收益率 < -2.00%\n",
            "  混合市场: -2.00% <= 市场收益率 <= 2.00%\n",
            "\n",
            "======================================================================\n",
            "1. 牛市 IC 测试\n",
            "======================================================================\n",
            "样本数: 97\n",
            "整体 IC (Pearson):  0.0148\n",
            "整体 IC (Spearman): 0.1645\n",
            "IC t-statistic: 0.1441\n",
            "IC p-value: 0.885751\n",
            "  ✗ IC 不显著\n",
            "\n",
            "滚动IC分析（30天窗口）:\n",
            "  滚动IC均值: 0.0275\n",
            "  滚动IC标准差: 0.1213\n",
            "  滚动IC > 0 的比例: 55.22%\n",
            "  IC比率 (IC_IR): 0.2266\n",
            "\n",
            "======================================================================\n",
            "2. 熊市 IC 测试\n",
            "======================================================================\n",
            "样本数: 140\n",
            "整体 IC (Pearson):  -0.1258\n",
            "整体 IC (Spearman): -0.2910\n",
            "IC t-statistic: -1.4898\n",
            "IC p-value: 0.138566\n",
            "  ✗ IC 不显著\n",
            "\n",
            "滚动IC分析（30天窗口）:\n",
            "  滚动IC均值: -0.1921\n",
            "  滚动IC标准差: 0.1284\n",
            "  滚动IC > 0 的比例: 6.36%\n",
            "  IC比率 (IC_IR): -1.4968\n",
            "\n",
            "\n",
            "======================================================================\n",
            "生成拟合结果图\n",
            "======================================================================\n",
            "\n",
            "图表已保存到: fitting_results.png\n",
            "\n",
            "✓ 并行模型测试完成\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# 主执行代码 - 使用并行模型（带特征选择）\n",
        "# ============================================================\n",
        "\n",
        "# 重置日志标志\n",
        "log = True\n",
        "\n",
        "# 准备数据\n",
        "print(\"=\"*70)\n",
        "print(\"准备数据...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "X_full = X.copy()\n",
        "y_full = y.copy()\n",
        "\n",
        "# Delta 清洗：对 P/E/I 类特征进行分数差分处理\n",
        "X_full = delta_clean(X_full, d=0.4, verbose=True)\n",
        "\n",
        "print(f\"\\nX shape: {X_full.shape}\")\n",
        "print(f\"y shape: {y_full.shape}\")\n",
        "\n",
        "# 准备市场数据\n",
        "market_excess = y_full\n",
        "rf = X_full['risk_free_rate_lag'].fillna(0) if 'risk_free_rate_lag' in X_full.columns else pd.Series(0, index=X_full.index)\n",
        "\n",
        "# ============================================================\n",
        "# 配置特征选择器参数（两个模型共用配置，但各自独立选择）\n",
        "# ============================================================\n",
        "\n",
        "parallel_feature_selector_params = {\n",
        "    'n_clusters': None,\n",
        "    'min_cluster_size': 3,\n",
        "    'max_clusters': 15,\n",
        "    'loss_threshold': None,\n",
        "    'top_k_per_group': 0.65,\n",
        "    'clustering_method': 'kmeans',\n",
        "    'n_repeats': 3,\n",
        "    'random_state': 42,\n",
        "    'verbose': 1\n",
        "}\n",
        "\n",
        "print(\"\\n特征选择器参数:\")\n",
        "for k, v in parallel_feature_selector_params.items():\n",
        "    print(f\"  {k}: {v}\")\n",
        "\n",
        "# ============================================================\n",
        "# 配置并行模型\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"配置并行模型\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 机会模型配置（Regressor）\n",
        "opportunity_model = CatBoostRegressor(\n",
        "    iterations=800,\n",
        "    learning_rate=0.004,\n",
        "    depth=6,\n",
        "    min_data_in_leaf=20,\n",
        "    l2_leaf_reg=7.0,\n",
        "    random_strength=5.5,\n",
        "    colsample_bylevel=0.78,\n",
        "    early_stopping_rounds=50,\n",
        "    bootstrap_type='Bernoulli',\n",
        "    subsample=0.85,\n",
        "    loss_function='RMSE',\n",
        "    verbose=0,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "# 方向模型配置（Classifier）\n",
        "direction_model = CatBoostClassifier(\n",
        "    iterations=800,\n",
        "    learning_rate=0.004,\n",
        "    depth=6,\n",
        "    min_data_in_leaf=20,\n",
        "    l2_leaf_reg=7.0,\n",
        "    random_strength=5.5,\n",
        "    colsample_bylevel=0.78,\n",
        "    early_stopping_rounds=50,\n",
        "    bootstrap_type='Bernoulli',\n",
        "    subsample=0.85,\n",
        "    loss_function='Logloss',\n",
        "    verbose=0,\n",
        "    random_seed=42\n",
        ")\n",
        "\n",
        "print(\"机会模型（Regressor）: RMSE (MSE)\")\n",
        "print(\"方向模型（Classifier）: Logloss (Cross Entropy)\")\n",
        "\n",
        "# ============================================================\n",
        "# 创建并行集成 Pipeline\n",
        "# ============================================================\n",
        "\n",
        "pipeline = ParallelEnsemblePipeline(\n",
        "    opportunity_model=opportunity_model,\n",
        "    direction_model=direction_model,\n",
        "    feature_creator=create_drop_features,\n",
        "    feature_selector_params=parallel_feature_selector_params,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n✓ 并行集成 Pipeline 创建完成\")\n",
        "\n",
        "# ============================================================\n",
        "# 运行两段滑动窗口测试\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"开始两段滑动窗口测试\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 测试参数\n",
        "TRAIN_SIZE = 5000\n",
        "TEST_SIZE = 2000\n",
        "START_INDEX = 13\n",
        "EXPANDING_WINDOW = False\n",
        "ALLOW_SHORT = True\n",
        "\n",
        "print(f\"训练窗口: {TRAIN_SIZE}\")\n",
        "print(f\"测试窗口: {TEST_SIZE}\")\n",
        "print(f\"窗口模式: {'扩展窗口' if EXPANDING_WINDOW else '滑动窗口'}\")\n",
        "print(f\"允许做空: {ALLOW_SHORT}\")\n",
        "\n",
        "# 运行测试\n",
        "test_result = move_forward_two_way_test(\n",
        "    X_full, y_full,\n",
        "    pipeline=pipeline,\n",
        "    train_size=TRAIN_SIZE,\n",
        "    test_size=TEST_SIZE,\n",
        "    start_index=START_INDEX,\n",
        "    expanding_window=EXPANDING_WINDOW,\n",
        "    market_excess=market_excess,\n",
        "    rf=rf,\n",
        "    allow_short=ALLOW_SHORT,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# 提取结果\n",
        "oof_predictions = test_result['predictions']\n",
        "oof_direction = test_result['direction']\n",
        "oof_opportunity = test_result['opportunity']\n",
        "oof_direction_proba = test_result['direction_proba']\n",
        "round_stats = test_result['round_stats']\n",
        "\n",
        "# ============================================================\n",
        "# 结果分析\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"结果分析\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "valid_predictions = oof_predictions.dropna()\n",
        "print(f\"有效预测数: {len(valid_predictions)}\")\n",
        "\n",
        "if len(valid_predictions) > 0:\n",
        "    results_df = train_df.loc[valid_predictions.index].copy()\n",
        "    results_df['prediction'] = valid_predictions\n",
        "    results_df['positions'] = valid_predictions\n",
        "    \n",
        "    y_test_all = y_full.loc[valid_predictions.index]\n",
        "    ic = np.corrcoef(valid_predictions.values, y_test_all.values)[0, 1]\n",
        "    print(f\"整体 IC: {ic:.4f}\")\n",
        "    \n",
        "    me_test = market_excess.loc[valid_predictions.index]\n",
        "    rf_test = rf.loc[valid_predictions.index]\n",
        "    sharpe = ad_sharpe_ratio_scorer(valid_predictions.values, me_test, rf_test)\n",
        "    print(f\"整体夏普率: {sharpe:.4f}\")\n",
        "    \n",
        "    print(f\"\\n仓位统计:\")\n",
        "    print(f\"  min: {valid_predictions.min():.4f}\")\n",
        "    print(f\"  max: {valid_predictions.max():.4f}\")\n",
        "    print(f\"  mean: {valid_predictions.mean():.4f}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # 方向模型评估 - 准确率\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"方向模型评估\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    valid_idx = valid_predictions.index\n",
        "    valid_direction = oof_direction.loc[valid_idx].dropna()\n",
        "    valid_direction_proba = oof_direction_proba.loc[valid_idx].dropna()\n",
        "    \n",
        "    # 真实方向：y > 0 为多，y <= 0 为空\n",
        "    true_direction = (y_full.loc[valid_direction.index] > 0).astype(int) * 2 - 1  # 转换为 ±1\n",
        "    pred_direction = valid_direction.values\n",
        "    \n",
        "    # 准确率和 F1 分数计算\n",
        "    from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "    \n",
        "    # 转换为 0/1 标签用于 sklearn 计算\n",
        "    true_labels = (true_direction.values + 1) // 2  # -1→0, 1→1\n",
        "    pred_labels = (pred_direction + 1) // 2  # -1→0, 1→1\n",
        "    \n",
        "    accuracy = accuracy_score(true_labels, pred_labels)\n",
        "    f1 = f1_score(true_labels, pred_labels, average='binary')  # 以多头(1)为正类\n",
        "    f1_macro = f1_score(true_labels, pred_labels, average='macro')  # 宏平均\n",
        "    precision = precision_score(true_labels, pred_labels, average='binary')\n",
        "    recall = recall_score(true_labels, pred_labels, average='binary')\n",
        "    \n",
        "    # 多空分别准确率\n",
        "    long_mask = true_direction.values == 1\n",
        "    short_mask = true_direction.values == -1\n",
        "    \n",
        "    long_accuracy = (pred_direction[long_mask] == 1).sum() / long_mask.sum() if long_mask.sum() > 0 else 0\n",
        "    short_accuracy = (pred_direction[short_mask] == -1).sum() / short_mask.sum() if short_mask.sum() > 0 else 0\n",
        "    \n",
        "    print(f\"总体方向准确率: {accuracy:.4f}\")\n",
        "    print(f\"F1 分数: {f1:.4f} (多头为正类)\")\n",
        "    print(f\"F1 分数 (宏平均): {f1_macro:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
        "    print(f\"  做多准确率: {long_accuracy:.4f} (真实多时预测多)\")\n",
        "    print(f\"  做空准确率: {short_accuracy:.4f} (真实空时预测空)\")\n",
        "    print(f\"  真实多头样本数: {long_mask.sum()}\")\n",
        "    print(f\"  真实空头样本数: {short_mask.sum()}\")\n",
        "    \n",
        "    # 平均预测概率\n",
        "    avg_proba_when_correct = valid_direction_proba.values[(pred_direction == true_direction.values)].mean()\n",
        "    avg_proba_when_wrong = valid_direction_proba.values[(pred_direction != true_direction.values)].mean()\n",
        "    print(f\"\\n预测概率分析:\")\n",
        "    print(f\"  预测正确时平均概率: {avg_proba_when_correct:.4f}\")\n",
        "    print(f\"  预测错误时平均概率: {avg_proba_when_wrong:.4f}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # 机会模型评估 - R²\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"机会模型评估\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    valid_opportunity = oof_opportunity.loc[valid_idx].dropna()\n",
        "    \n",
        "    # 真实机会：|generate_hft_positions(...)|\n",
        "    from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "    \n",
        "    # 需要计算真实的机会目标（使用与训练时相同的参数）\n",
        "    y_test_opp = y_full.loc[valid_opportunity.index]\n",
        "    forward_returns_lag_test = X_full.loc[valid_opportunity.index, 'forward_returns_lag'] if 'forward_returns_lag' in X_full.columns else pd.Series(0, index=valid_opportunity.index)\n",
        "    \n",
        "    # 使用与训练时相同的参数：span_N=60, sensitivity_k=1, allow_short=True\n",
        "    true_opportunity = np.abs(generate_hft_positions(\n",
        "        y_test_opp, forward_returns_lag_test, \n",
        "        span_N=60, sensitivity_k=1, allow_short=True\n",
        "    ).values)\n",
        "    pred_opportunity = np.abs(valid_opportunity.values)  # 确保是绝对值\n",
        "    \n",
        "    r2 = r2_score(true_opportunity, pred_opportunity)\n",
        "    rmse = np.sqrt(mean_squared_error(true_opportunity, pred_opportunity))\n",
        "    mae = mean_absolute_error(true_opportunity, pred_opportunity)\n",
        "    corr = np.corrcoef(true_opportunity, pred_opportunity)[0, 1]\n",
        "    \n",
        "    print(f\"R² 分数: {r2:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"相关系数: {corr:.4f}\")\n",
        "    print(f\"\\n真实机会统计: min={true_opportunity.min():.4f}, max={true_opportunity.max():.4f}, mean={true_opportunity.mean():.4f}\")\n",
        "    print(f\"预测机会统计: min={pred_opportunity.min():.4f}, max={pred_opportunity.max():.4f}, mean={pred_opportunity.mean():.4f}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # 按时间段分析\n",
        "    # ============================================================\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"按时间段分析\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    for stat in round_stats:\n",
        "        round_num = stat['round']\n",
        "        test_indices = stat['test_indices']\n",
        "        \n",
        "        # 获取该轮的数据\n",
        "        round_idx = [i for i in test_indices if i in valid_predictions.index]\n",
        "        if len(round_idx) == 0:\n",
        "            continue\n",
        "            \n",
        "        round_pred = valid_predictions.loc[round_idx]\n",
        "        round_true = y_full.loc[round_idx]\n",
        "        round_direction_pred = oof_direction.loc[round_idx]\n",
        "        round_direction_true = (round_true > 0).astype(int) * 2 - 1\n",
        "        round_opportunity_pred = oof_opportunity.loc[round_idx]\n",
        "        \n",
        "        frl_round = X_full.loc[round_idx, 'forward_returns_lag'] if 'forward_returns_lag' in X_full.columns else pd.Series(0, index=round_idx)\n",
        "        round_opportunity_true = np.abs(generate_hft_positions(\n",
        "            round_true, frl_round, span_N=60, sensitivity_k=1, allow_short=True\n",
        "        ).values)\n",
        "        \n",
        "        # 计算指标\n",
        "        round_ic = np.corrcoef(round_pred.values, round_true.values)[0, 1]\n",
        "        round_dir_acc = (round_direction_pred.values == round_direction_true.values).mean()\n",
        "        round_opp_r2 = r2_score(round_opportunity_true, round_opportunity_pred.values) if len(round_idx) > 1 else np.nan\n",
        "        round_opp_corr = np.corrcoef(round_opportunity_true, round_opportunity_pred.values)[0, 1]\n",
        "        \n",
        "        me_round = market_excess.loc[round_idx]\n",
        "        rf_round = rf.loc[round_idx]\n",
        "        round_sharpe = ad_sharpe_ratio_scorer(round_pred.values, me_round, rf_round)\n",
        "        \n",
        "        print(f\"\\n第 {round_num} 轮 (样本数: {len(round_idx)}):\")\n",
        "        print(f\"  IC: {round_ic:.4f}, 夏普率: {round_sharpe:.4f}\")\n",
        "        print(f\"  方向准确率: {round_dir_acc:.4f}\")\n",
        "        print(f\"  机会 R²: {round_opp_r2:.4f}, 机会相关系数: {round_opp_corr:.4f}\")\n",
        "    \n",
        "    # ============================================================\n",
        "    # 数据流诊断\n",
        "    # ============================================================\n",
        "    print(\"\\n\")\n",
        "    diagnosis = diagnose_ic_data_flow(\n",
        "        y_pred=valid_predictions.values,\n",
        "        y_true=y_test_all.values,\n",
        "        market_returns=me_test.values,\n",
        "        name=\"并行模型预测\"\n",
        "    )\n",
        "    \n",
        "    # ============================================================\n",
        "    # 市场状态分组 IC 测试\n",
        "    # ============================================================\n",
        "    print(\"\\n\")\n",
        "    ic_results = ic_test_by_market_regime_improved(\n",
        "        y_pred=valid_predictions.values,\n",
        "        y_true=y_test_all.values,\n",
        "        market_returns=me_test.values,\n",
        "        threshold_bull=0.02,\n",
        "        threshold_bear=-0.02,\n",
        "        window_size=30\n",
        "    )\n",
        "    \n",
        "    # ============================================================\n",
        "    # 生成拟合结果图\n",
        "    # ============================================================\n",
        "    print(\"\\n\")\n",
        "    plot_fitting_results(\n",
        "        results_df=results_df,\n",
        "        oof_predictions=oof_predictions,\n",
        "        ic_results=ic_results,\n",
        "        save_path='fitting_results.png'\n",
        "    )\n",
        "\n",
        "print(\"\\n✓ 并行模型测试完成\")"
      ]
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "databundleVersionId": 14348714,
          "sourceId": 111543,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31192,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "dsml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
